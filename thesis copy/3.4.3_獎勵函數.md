# 3.4.3 獎勵函數 (Reward Function)

獎勵函數是連接智能體動作與學習目標的橋樑，它通過一個純量回饋信號來告知智能體其決策的優劣。在本研究中，為了探索不同學習信號對模型性能的影響，我們設計並實現了兩種截然不同的獎勵模式，分別對應不同的實驗組別。這兩種模式的設計旨在平衡即時回饋的指導性與最終目標的全局性。

### 1. 步階獎勵 (Step Reward)

步階獎勵模式旨在為智能體提供一個密集的、即時的回饋信號。在每個決策間隔（10 ticks）結束時，系統會根據這段時間內交叉路口的局部觀測指標，計算一個綜合獎勵值。這種高頻率的回饋有助於智能體快速學習到基礎的交通控制啟發式規則。

步階獎勵 $R_\text{step}$ 由以下四個加權分量構成：

*   **流量獎勵 (Flow Reward)**: 正向獎勵。獎勵智能體在決策間隔內成功引導通過交叉路口的機器人數量。
    *   $$
        R_\text{flow} = w_\text{flow} \times (N_\text{horizontal} + N_\text{vertical})
        $$
    *   其中 $N$ 是通過的機器人數量，$w_\text{flow}$ 是權重。

*   **能源懲罰 (Energy Penalty)**: 負向獎勵（懲罰）。懲罰因等待和啟停造成的額外能源消耗。
    *   $$
        R_\text{energy} = w_\text{energy} \times E_\text{consumed}
        $$
    *   其中 $E_\text{consumed}$ 是估算的能源消耗，$w_\text{energy}$ 是權重。

*   **等待時間懲罰 (Waiting Time Penalty)**: 負向獎勵。懲罰所有仍在佇列中等待的機器人的累計等待時間。
    *   $$
        R_\text{wait} = w_\text{wait} \times T_\text{cumulative\_wait}
        $$
    *   其中 $T_\text{cumulative\_wait}$ 是累計等待時間，$w_\text{wait}$ 是權重。

*   **相位切換懲罰 (Switching Penalty)**: 負向獎勵。對每一次號誌相位的切換施加一個固定的懲罰，以避免過於頻繁、無效的相位切換，鼓勵控制器維持暢通的交通流。
    *   $$
        R_\text{switch} = w_\text{switch}
        $$
        （僅在發生切換時觸發）

**關鍵路口加權**: 為了讓智能體優先學習如何管理靠近揀貨站等瓶頸區域的交叉路口，對於這些被標記為「關鍵」的交叉路口，其計算出的 $R_\text{step}$ 會被乘以一個大於 1 的放大係數 $w_\text{critical}$。

### 2. 全域獎勵 (Global Reward)

與步階獎勵不同，全域獎勵模式提供一個稀疏的、延遲的回饋信號。在這種模式下，智能體在整個評估回合（evaluation episode，例如 3000 或 8000 ticks）中不會收到任何獎勵信號。只有在回合結束時，系統才會根據整個倉儲的最終宏觀性能指標，計算一個單一的獎勵值。

這種模式的目的是迫使智能體學習一系列能夠對系統長期、全局目標產生積極影響的複雜行為，而非僅僅優化局部指標。

全域獎勵 $R_\text{global}$ 由以下三個核心指標的加權組合而成：

*   **訂單完成效率 (Order Completion Score)**: 主要的正向獎勵。基於在規定時間內完成的訂單數量。
    *   $$
        S_\text{order} = w_\text{order} \times N_\text{completed\_orders}
        $$

*   **總體時間成本 (Total Time Cost)**: 負向獎勵。懲罰所有訂單從生成到完成所花費的總時間。
    *   $$
        C_\text{time} = w_\text{time} \times T_\text{total\_order\_time}
        $$

*   **總體能源成本 (Total Energy Cost)**: 負向獎勵。懲罰整個系統在運行期間的總估算能耗。
    *   $$
        C_\text{energy} = w_\text{energy\_global} \times E_\text{total}
        $$

*   **回堵懲罰 (Spillback Penalty)**: 一個巨大的負向懲罰。如果在評估期間發生了導致系統死鎖的嚴重回堵，則施加此懲罰，以確保智能體學會避免災難性策略。
    *   $$
        P_\text{spillback} = w_\text{spillback}
        $$
        （僅在發生嚴重回堵時觸發）

最終的全域獎勵為：
$$
R_\text{global} = S_\text{order} - C_\text{time} - C_\text{energy} - P_\text{spillback}
$$