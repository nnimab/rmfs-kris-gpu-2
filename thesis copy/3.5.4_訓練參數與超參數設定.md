# 3.4.4 訓練參數與超參數設定

為了確保實驗的可複現性與結果的有效性，本節將詳細列出在訓練 `DQN` 與 `NERL` 控制器時所使用的全部關鍵參數與超參數。這些參數的設定綜合考量了演算法的穩定性、收斂速度以及計算資源的限制，並在初步實驗中進行了調校。

### 1. 通用參數

以下為 `DQN` 與 `NERL` 兩種控制器共用的參數：

| 參數名稱 | 程式碼變數 | 預設值 | 說明 |
| :--- | :--- | :--- | :--- |
| 最小綠燈時間 | `min_green_time` | 1 | 為避免訊號頻繁切換的最小持續時間 (tick) |
| 方向偏好因子 | `bias_factor` | 1.5 | 給予水平方向的額外優先級權重 |
| 狀態空間維度 | `state_size` | 17 | 輸入神經網路的狀態向量維度 |
| 動作空間維度 | `action_size` | 3 | 神經網路輸出的可能動作數量 (保持/水平/垂直) |
| 最大等待時間閾值 | `max_wait_threshold` | 50 | 觸發防鎖死機制的機器人最大等待時間 (tick) |

### 2. DQN 特有超參數

以下為 `DQNController` 在訓練過程中使用的主要超參數：

| 參數名稱 | 程式碼變數 | 預設值 | 說明 |
| :--- | :--- | :--- | :--- |
| 學習率 | `learning_rate` | 5e-4 | Adam 優化器的學習率 |
| 折扣因子 (Gamma) | `gamma` | 0.95 | 未來獎勵的折扣係數，值越接近1代表越重視長期回報 |
| 初始探索率 (Epsilon) | `epsilon` | 1.0 | 訓練初期隨機選擇動作的機率 |
| 最小探索率 | `epsilon_min` | 0.01 | 探索率衰減的下限 |
| 探索率衰減率 | `epsilon_decay` | 0.999 | 每個訓練步驟後，探索率乘以的衰減係數 |
| 經驗回放記憶體大小 | `memory_size` | 100,000 | 存儲經驗元組的最大數量 |
| 批次大小 | `batch_size` | 8,192 | 每次從記憶體中抽樣進行訓練的樣本數量 |
| 目標網路更新頻率 | (硬編碼) | 1,000 ticks | 每隔多少時間步將策略網路的權重複製到目標網路 |

### 3. NERL 特有超參數

以下為 `NEController` 在演化過程中使用的主要超參數：

| 參數名稱 | 程式碼變數 | 預設值 | 說明 |
| :--- | :--- | :--- | :--- |
| 族群大小 | `population_size` | 20 | 每一代中包含的個體（神經網路）數量 |
| 精英保留比例 | `elite_ratio` | 0.2 | 每一代中適應度最高的個體被直接保留到下一代的比例 |
| 錦標賽選擇大小 | `tournament_size` | 4 | 在錦標賽選擇中，每次隨機挑選進行比較的個體數量 |
| 交叉率 | `crossover_rate` | 0.8 | 兩個父代進行交叉操作產生子代的機率 |
| 變異率 | `mutation_rate` | 0.2 | 個體基因（網路權重）發生變異的基礎機率 |
| 變異強度 | `mutation_strength` | 0.15 | 高斯變異的標準差，控制變異的幅度大小 |
| 評估間隔 | `evolution_interval`| 1,000 ticks | 每個個體進行評估的持續時間 (tick) |

這些參數共同定義了兩種 DRL 方法的學習行為。在第 4 章的實驗評估中，我們將基於這些設定來對控制器進行訓練和比較。 