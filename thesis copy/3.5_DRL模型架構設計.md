# 3.4 深度強化學習方法

為了應對第 3.1 節所定義的動態交通控制挑戰，並超越第 3.3 節基線控制器在適應性上的限制，本研究導入了深度強化學習（Deep Reinforcement Learning, DRL）作為核心解決方案。DRL 結合了深度學習在高維數據中提取特徵的強大能力與強化學習在序列決策問題中通過試錯進行學習的優勢，使其特別適合解決如倉儲交通控制這類具有複雜、非線性狀態空間的難題。

與依賴固定規則的基線控制器不同，DRL 智慧體能夠從與環境的互動中自主學習控制策略，發現人類設計師難以預見的有效模式。它們的目標是最大化一個長期累積的獎勵訊號，從而學習到能夠平衡即時需求與長遠系統效率的複雜行為。

本研究將實現並深入比較兩種代表性的 DRL 方法，它們在學習機制上存在本質差異，分別代表了基於價值函數梯度下降和基於族群演化優化的兩大技術路線：

1.  **深度 Q 網路 (Deep Q-Network, DQN)**：一種基於價值的（Value-based）典型 DRL 演算法，它使用神經網路來近似最優的動作價值函數（Action-Value Function, Q-function），並通過時間差分學習（Temporal Difference Learning）進行更新。

2.  **神經演化強化學習 (Neuroevolution Reinforcement Learning, NERL)**：一種結合了神經網路與演化演算法（Evolutionary Algorithms）的黑箱優化方法。它不依賴梯度下降，而是將神經網路的權重視為基因，通過選擇、交叉和變異等演化操作，在一個族群中直接對策略進行搜索和優化。

接下來的章節將分別對這兩種方法的架構設計、獎勵函數以及訓練配置進行詳細闡述。 