# RMFS控制器優化開發計畫書

**版本:** 1.1  
**日期:** 2025年7月8日  
**更新:** 新增階段1.5算法優化，基於深度分析發現的公平性問題

## 1. 總體目標

在18天內，完成一個基於神經進化強化學習(NERL)的RMFS交通控制器的高品質研究，並完成論文與口試簡報的撰寫。

核心策略是**「訓練與評估分離」**，使用單一專案庫搭配兩種不同的啟動模式，以確保研究的效率、可重現性與嚴謹性。

## 2. 專案結構規劃

我們將在現有專案的基礎上，新增兩個核心啟動腳本，以管理不同的工作流程：

- **`train.py`**: **訓練啟動器**。此腳本專門負責訓練學習型代理（NERL、DQN），它將以高效的**無頭模式(Headless Mode)**在背景運行，其最終產出是訓練好的模型檔案（例如 `models/nerl_best.pth`）。

- **`evaluate.py`**: **評估啟動器**。此腳本專門負責公平地比較所有控制器（包括傳統方法和訓練好的模型）的最終效能。它可以選擇以**視覺化模式(GUI Mode)**運行，以便觀察與展示。其最終產出是論文所需的核心數據報告（例如 `results/final_comparison.csv`）。

這種結構可以確保訓練和評估的邏輯分離，同時共享所有核心程式碼，保證了研究的一致性。

## 3. 開發階段與任務分解

### **階段一：核心演算法修正與訓練 (預計 5-6 天)**

**目標：** 修正NERL演算法的評估流程，並在無頭模式下高效地訓練出穩定且高效的NERL與DQN模型。

**任務詳情：**

1.  **創建 `train.py` 腳本 (1 天):**
    *   建立 `train.py` 檔案。
    *   整合 `pyNetLogo` 函式庫，並設定以**無頭模式**啟動 `rmfs.nlogo` 的功能。
    *   編寫命令行參數解析，允許使用者選擇要訓練的代理（`--agent nerl` 或 `--agent dqn`）。

2.  **實現NERL正確的訓練迴圈 (2-3 天):**
    *   在 `train.py` 中，實現以「世代(Generation)」為單位的主迴圈。
    *   在世代迴圈內部，實現以「個體(Individual)」為單位的評估迴圈。
    *   **關鍵邏輯：** 在評估每個個體前，必須呼叫NetLogo的`kill_workspace()`和`load_model()`來**重置模擬環境**，確保評估的公平性。
    *   為每個個體運行一段固定時長的模擬（例如2000 ticks），並計算其最終的**適應度分數(Fitness Score)**。
    *   將整代所有個體的適應度分數列表傳遞給NERL控制器，以執行進化操作。

3.  **執行訓練並產出模型 (2 天):**
    *   運行 `python train.py --agent nerl` 來訓練NERL模型。
    *   根據需要，對DQN也進行類似的訓練流程。
    *   監控訓練過程中的適應度變化曲線，確保演算法正在穩定收斂。
    *   最終產出 `models/nerl_best.pth` 和 `models/dqn_best.pth`。

### **階段1.5：算法優化與公平性修復 (預計 3-4 天)**

**目標：** 修復在階段一測試中發現的三個關鍵問題，確保NERL和DQN控制器的公平對比基礎。

**重要發現：** 通過深度分析發現NERL和DQN控制器存在嚴重的公平性問題，需要進行算法優化才能進行有意義的對比研究。

**任務詳情：**

1.  **解決資訊孤島問題 (1 天):**
    *   **問題描述：** 每個路口獨立決策，無法看到相鄰路口狀況，導致區域性交通協調不佳
    *   **解決方案：** 擴展狀態空間，加入相鄰路口資訊
    *   **技術實現：** 
        - 修改 `get_state()` 方法，從8維擴展到16維狀態空間
        - 新增相鄰路口機器人數量、等待時間等資訊
        - 更新神經網路架構以支援新的狀態空間

2.  **修復硬編碼正規化問題 (1 天):**
    *   **問題描述：** 狀態特徵使用固定正規化參數，無法適應極端交通狀況
    *   **解決方案：** 實現自適應正規化機制
    *   **技術實現：**
        - 創建 `AdaptiveNormalizer` 類
        - 使用運行時統計數據動態調整正規化參數
        - 替換硬編碼的最大值（10個機器人、50 ticks等待時間等）

3.  **統一獎勵機制與雙重對比框架 (1-2 天):**
    *   **問題描述：** NERL使用全局適應度，DQN使用即時獎勵，對比不公平
    *   **解決方案：** 設計統一的多目標獎勵函數和雙重對比框架
    *   **技術實現：**
        - 設計改進的多維度獎勵函數（完成率+能源效率+公平性+穩定性）
        - 修改 `train.py` 支援 `--reward_type global/step` 參數
        - 實現四種對比模式：
          - DQN(即時獎勵) vs NERL(即時獎勵) - 比較學習算法
          - DQN(全局獎勵) vs NERL(全局獎勵) - 比較學習算法
          - 分析獎勵機制本身對性能的影響

4.  **重新訓練優化後的模型:**
    *   使用改進的狀態空間和獎勵機制重新訓練NERL
    *   訓練統一獎勵機制下的DQN模型
    *   產出四個模型變體：nerl_step.pth, nerl_global.pth, dqn_step.pth, dqn_global.pth

### **階段二：效能評估與數據分析 (預計 2-3 天)**

**目標：** 在完全相同的基準下，對所有優化後的控制器進行公平的效能比較，並產出論文所需的核心數據與圖表。

**任務詳情：**

1.  **創建統一評估框架 `evaluate.py` (半天):**
    *   建立 `evaluate.py` 檔案，支援載入階段1.5中產出的四個模型變體
    *   設定統一的評估參數：相同隨機數種子、相同訂單列表、相同模擬時長
    *   支援六種控制器對比：Time-based, Queue-based, DQN-step, DQN-global, NERL-step, NERL-global

2.  **執行多維度對比實驗 (1.5 天):**
    *   **算法對比：** 在相同獎勵機制下比較 DQN vs NERL 的學習能力
    *   **獎勵機制對比：** 在相同算法下比較 即時獎勵 vs 全局獎勵 的效果
    *   **傳統vs智能：** 比較傳統控制器 vs 智能控制器的整體性能
    *   運行較長的模擬時長（例如20000 ticks）確保統計顯著性

3.  **數據分析與多維度視覺化 (1 天):**
    *   產出 `algorithm_comparison.csv` (算法對比)、`reward_comparison.csv` (獎勵對比)、`overall_comparison.csv` (整體對比)
    *   生成論文所需的多維度圖表：
        - 學習曲線對比圖
        - 性能指標雷達圖  
        - 獎勵機制影響分析圖
        - 公平性指標對比圖

### **階段三：論文撰寫與簡報製作 (預計 7-8 天)**

**目標：** 完成論文初稿、最終稿以及口試簡報。

**任務詳情：**

1.  **撰寫核心章節 (3-4 天):**
    *   **研究方法：** 詳細描述優化後的NERL/DQN架構、統一獎勵機制、雙重對比框架
    *   **結果與討論：** 
        - 算法優化前後的性能對比
        - 多維度控制器對比分析（6種控制器）
        - 獎勵機制對學習效果的影響分析
        - 資訊孤島問題的解決效果驗證

2.  **完成論文全稿 (2-3 天):**
    *   根據您的研究計畫書，快速完成緒論、文獻回顧、結論等章節。
    *   整理參考文獻、摘要、圖表目錄，並進行通篇校對與潤飾。

3.  **製作口試簡報 (1-2 天):**
    *   基於論文的圖表和結論，製作一份清晰、有說服力的口試簡報。
    *   準備口試問答環節可能被問到的問題。

## 4. 當前進度狀態 (2025年7月8日)

### **✅ 已完成項目**
- **階段一 (基礎訓練框架)：** 100% 完成
  - `train.py` 腳本已實現並成功運行
  - NERL 訓練迴圈正常工作，能夠進行世代進化
  - 基礎模型已產出：`models/nerl_traffic.pth`
  - 訓練過程驗證：2世代訓練顯示適應度從-0.44提升到-0.33

### **🔍 關鍵發現與問題識別**
通過深度分析，發現了三個影響公平對比的關鍵問題：
1. **資訊孤島問題：** 每個路口獨立決策，缺乏相鄰路口資訊
2. **硬編碼正規化：** 使用固定參數，無法適應極端交通狀況  
3. **獎勵機制不統一：** NERL使用全局適應度，DQN使用即時獎勵

### **📋 更新後的時程安排**
- **階段一：** ✅ 已完成 (實際用時: 1天)
- **階段1.5：** 🔧 算法優化階段 (預計3-4天) 
- **階段二：** 📊 公平對比評估 (預計2-3天)
- **階段三：** 📝 論文撰寫 (保持7-8天)
- **總計：** 13-16天 (仍在18天目標內)

    📊 階段1.5成功完成，現狀分析

    ✅ 已完成的三大核心改進：
    1. 16維狀態空間 - 解決資訊孤島問題，包含相鄰路口信息
    2. 自適應正規化機制 - AdaptiveNormalizer 和 TrafficStateNormalizer 
    已實現
    3. 統一獎勵系統 - UnifiedRewardSystem 支持step/global兩種模式

    🔧 現有訓練基礎設施：
    - train.py 支持NERL訓練，包含reward_mode參數
    - 階段1.5的三大改進已整合到控制器中

### **其他維護與修復**

- **2025-07-15:**
  - **修復：** 調整了 `lib/generator/warehouse_generator.py` 中的訂單生成參數 (`order_period_time` 和 `order_start_arrival_time`)，以解決因訂單生成時間跨度過長而導致的模擬初期無訂單、系統死鎖的問題。此修改確保了在延長的訓練週期內，訂單能夠從一開始就持續生成。