
---

### **RMFS神經演化控制器之混合式獎勵系統設計詳解 (論文專用)**

#### **1. 設計哲學與核心目標**

本研究旨在為基於神經演化（NERL）的倉儲機器人交通控制器設計一套優化能源效率的獎勵系統。系統的核心目標是引導智能體學習到一種交通調度策略，該策略能夠在**最大化訂單吞吐量**的同時，**最小化系統整體的能源與時間成本**。

為應對複雜工業環境中強化學習面臨的**稀疏獎勵 (Sparsity)**、**信用分配 (Credit Assignment)** 和**獎勵駭客 (Reward Hacking)** 等挑戰，本系統採用了一種創新的**混合式獎勵範式**。該範式結合了評估最終績效的**宏觀全局獎勵（Global Reward）**與引導過程決策的**微觀即時獎勵（Step Reward）**。此設計遵循以下核心原則：

1.  **目標導向 (Goal-Oriented):** 全局獎勵函數直接數學化本研究的核心KPI——**能源效率**，定義為單位總成本（能源與時間）所完成的訂單數量。這確保了演化方向與最終研究目標的強一致性。
2.  **魯棒的過程引導 (Robust Process Guidance):** 引入與交通控制任務強相關的**里程碑事件獎勵 (Milestone Reward)**，以解決因訂單完成週期長而導致的獎勵稀疏問題。該設計能夠應對系統內在的**動態路徑重規劃**特性，保證獎勵信號的有效性。
3.  **分層激勵 (Hierarchical Incentive):** 即時獎勵不僅僅是簡單的行為獎勵，而是引入了**基於任務優先級的成本函數**。它引導智能體在微觀層面學會權衡，優先服務於高價值任務（如運送貨物），從而使其局部最優決策與全局目標保持一致。
4.  **成本的顯式化 (Explicit Cost Modeling):** 針對模擬環境中**怠速能耗為零**的特性，本系統創造性地將**時間等待成本**作為主要的即時懲罰項，有效避免了智能體因模型限制而學會無效的「靜止等待」策略。

---

#### **2. 全局獎勵 (`Global Reward`)：策略的最終適應度函數**

全局獎勵 \(R_{global}\) 在每一次完整的評估週期（Episode）結束時計算一次。它作為NERL算法的**適應度分數（Fitness Score）**，直接決定了個體策略在演化過程中的生存和傳播機率。

##### **2.1. 全局獎勵公式**

\[
R_{global} = \underbrace{\left( \frac{N_{orders\_completed} \times W_{completion}}{E_{total} + C_{time\_total} + \epsilon} \right)}_{\text{核心能源效率分數}} + \underbrace{R_{milestone}}_{\text{總里程碑獎勵}}
\]

##### **2.2. 全局獎勵參數詳解**

| 參數 | 符號 | 說明與計算方法 |
| :--- | :--- | :--- |
| **完成訂單數** | \(N_{orders\_completed}\) | **來源:** `warehouse.order_manager.finished_orders` 列表的長度。<br>**計算:** 在一個Episode內，系統判定為已完成的訂單總數。訂單完成的節點為其包含的最後一件商品被揀選的時刻。該指標代表了策略的**核心產出**。 |
| **完成獎勵權重** | \(W_{completion}\) | **來源:** 預設超參數（例如 `+100.0`）。<br>**計算:** 一個正向常數，用於調節訂單完成量在總獎勵中的重要性。其作用是將離散的訂單計數放大到與連續的成本項（能源、時間）在數值上可比較的尺度。 |
| **總能源消耗** | \(E_{total}\) | **來源:** `warehouse.total_energy` 屬性。<br>**計算:** 在一個Episode內，所有機器人消耗的能量總和（單位：焦耳）。該值由模擬器基於考慮了速度、加速度和負載的物理模型實時累加。 |
| **總時間成本** | \(C_{time\_total}\) | **來源:** `eval_ticks`（外部Python Tick總數）與預設權重。<br>**計算:** \(C_{time\_total} = T_{python\_ticks} \times W_{time}\)。其中 \(T_{python\_ticks}\) 是Episode的總模擬步數。此成本項懲罰效率低下的策略，因為任何無效行為都會延長總步數，從而增加總成本。 |
| **時間成本權重** | \(W_{time}\) | **來源:** 預設超參數（例如 `0.1`）。<br>**計算:** 用於將無單位的模擬步數轉化為與能源成本量級相當的成本值。 |
| **平滑項** | \(\epsilon\) | **來源:** 一個極小的常數（例如 `1e-6`）。<br>**計算:** 用於防止在極端情況下（總成本為零）出現分母為零的計算錯誤。 |
| **總里程碑獎勵** | \(R_{milestone}\) | **來源:** 在Episode運行過程中累加。<br>**計算:** 所有觸發的里程碑事件獎勵的總和。詳見下節。 |

##### **2.3. 里程碑獎勵 (`Milestone Reward`) 設計**

為了解決在訓練初期智能體難以完成完整訂單而導致的獎勵極度稀疏問題，我們設計了與交通控制任務強相關的里程碑獎勵。

1.  **成功護送至揀貨區獎勵 (`+10.0`)**
    *   **觸發邏輯:** 當一個**攜帶著貨架**的機器人（狀態為 `delivering_pod`）通過了其路徑上**事實上的最後一個**受控路口時觸發。
    *   **魯棒性設計:** 為了應對**動態路徑重規劃**，該判斷並非基於靜態的預設計劃。而是在機器人每次離開路口時，動態地進行一次**前瞻性路徑檢查**：從其當前位置到最終目的地（揀貨台）的新路徑中，是否還包含任何受控路口。若不包含，則認定里程碑達成。此設計確保了獎勵的準確性。

2.  **成功釋放揀貨區交通獎勵 (`+5.0`)**
    *   **觸發邏輯:** 當一個**已完成揀貨、空載**的機器人（狀態為 `returning_pod`）通過了其返程路徑上的**第一個**受控路口時觸發。
    *   **實現機制:** 在機器人狀態剛轉變為 `returning_pod` 時，其初始返程路徑是確定的。系統此時記錄下該路徑的第一個路口。當機器人後續實際通過此路口時，即觸發獎勵。此設計鼓勵智能體及時疏散揀貨區周邊的交通，提高系統流動性。

---

#### **3. 即時獎勵 (`Step Reward`)：微觀決策的成本引導函數**

即時獎勵 \(R_{step}\) 在模擬的**每一個時間步（tick）**為**每一個路口**的決策提供即時反饋。它被設計成一個純粹的**成本函數**，理論最高分為0，實際運行中恆為負值。其目的是為需要高頻反饋的算法（如DQN）提供穩定的訓練梯度，或在NERL中作為輔助目標。

##### **3.1. 即時獎勵公式**

\[
R_{step} = \text{clip} \left( (\underbrace{R_{pass}}_{\text{通過獎勵}} - \underbrace{C_{wait\_time}}_{\text{時間等待成本}} - \underbrace{C_{switch}}_{\text{信號切換成本}}), -1.0, 1.0 \right)
\]
其中 `clip` 函數將最終得分裁剪至`[-1.0, 1.0]`區間，以增強訓練的穩定性。

##### **3.2. 即時獎勵參數詳解**

| 參數 | 符號 | 說明與計算方法 |
| :--- | :--- | :--- |
| **通過獎勵** | \(R_{pass}\) | **來源:** 在路口即時統計的通過機器人列表。<br>**計算:** 根據通過的機器人**任務優先級**給予不同權重的獎勵並加總。這是對智能體有效疏導交通行為的正向激勵。 |
| **時間等待成本** | \(C_{wait\_time}\) | **來源:** 在路口即時統計的等待機器人列表。<br>**計算:** 根據等待的機器人**任務優先級**施加不同權重的懲罰並加總。這是對造成擁堵和時間浪費行為的直接懲罰。 |
| **信號切換成本** | \(C_{switch}\) | **來源:** 控制器自身動作記錄。<br>**計算:** 如果控制器在當前tick改變了路口的交通方向，則產生一個固定的成本（例如`0.1`），否則為0。此成本旨在抑制過於頻繁的信號切換，鼓勵形成穩定的交通流。 |

##### **3.3. 任務優先級設計**

這是 `Step Reward` 的核心智慧。我們將機器人的任務狀態映射為三個不同的優先級，使得成本函數能夠體現業務邏輯。

| 優先級 | 對應任務狀態 (`robot.current_state`) | 業務邏輯 | \(R_{pass}\)權重 | \(C_{wait\_time}\)權重 |
| :--- | :--- | :--- | :--- | :--- |
| **高 (High)** | `delivering_pod` (送貨中) | 價值最高，直接影響訂單完成。應最快通過。 | `+1.0` | `0.05` |
| **中 (Medium)** | `returning_pod` (返程中) | 騰出空間，為後續高價值任務做準備。 | `+0.7` | `0.02` |
| **低 (Low)** | `taking_pod` (去取貨) | 任務的起始階段，時間敏感性最低。 | `+0.5` | `0.01` |

**計算示例:** 假設一個路口，一個`delivering_pod`狀態的機器人通過，同時一個`taking_pod`狀態的機器人在等待。該tick的獎勵貢獻為：`(+1.0) - (0.01) = +0.99`（未計切換成本）。這清晰地量化了智能體決策的優劣，引導其做出符合全局利益的選擇。