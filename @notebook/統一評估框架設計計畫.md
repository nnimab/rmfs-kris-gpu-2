    🎯 設計目標

    建立一個統一的評估框架，能夠公平地比較6種控制器的性能，並生成論文所
    需的核心數據。

    📋 核心功能設計

    1. 統一評估框架架構

    - 基於現有基礎設施：利用現有的netlogo.py、PerformanceReportGenerato
    r、train.py的訓練基礎
    - 獨立評估模式：與訓練完全分離，確保評估的公正性
    - 統一參數控制：固定隨機種子、評估時長、倉庫配置

    2. 六控制器支援

    - 傳統控制器：Time-based, Queue-based（現有實現）
    - AI控制器：DQN-step, DQN-global, NERL-step, 
    NERL-global（需要訓練4個模型變體）
    - 模型載入機制：自動載入對應的.pth檔案

    3. 三種對比維度

    - 算法對比：相同獎勵機制下的DQN vs NERL學習能力
    - 獎勵機制對比：相同算法下的step vs global效果
    - 傳統vs智能：全面性能對比

    4. 數據輸出格式

    - algorithm_comparison.csv：算法對比數據
    - reward_comparison.csv：獎勵機制對比數據  
    - overall_comparison.csv：整體性能對比數據
    - 實時監控：評估進度和中間結果

    🔧 技術實現方案

    1. 評估流程設計

    1. 載入配置（固定種子、評估時長20000 ticks）
    2. 依序評估每個控制器
    3. 收集性能指標（能源、完成率、等待時間、公平性等）
    4. 生成對比分析報告
    5. 輸出CSV數據和視覺化圖表

    2. 關鍵技術挑戰

    - 模型切換機制：確保評估時正確載入對應的AI模型
    - 狀態重置：每次評估前完全重置倉庫狀態
    - 性能指標統一：使用階段1.5的統一獎勵系統作為評估基準
    - 公平性保證：相同的訂單序列、相同的初始條件

    3. 優先實施順序

    1. 基礎框架：建立evaluate.py的核心結構
    2. 控制器集成：實現6種控制器的統一載入
    3. 數據收集：整合PerformanceReportGenerator
    4. 對比分析：實現三種對比維度的數據分析
    5. 視覺化輸出：生成論文所需的圖表

    💡 主要優勢

    - 建立在現有基礎上：最大化利用階段1.5的成果
    - 跳過訓練階段：先建立框架，後補充模型
    - 論文導向：直接產出論文所需的核心數據
    - 可擴展性：未來可輕鬆添加新的控制器或指標