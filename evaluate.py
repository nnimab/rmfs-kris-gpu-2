#!/usr/bin/env python3
"""
RMFS æ§åˆ¶å™¨è©•ä¼°è…³æœ¬ï¼ˆç°¡åŒ–ç‰ˆï¼‰
ç”¨æ–¼åŸ·è¡Œå–®æ¬¡è©•ä¼°é‹è¡Œä¸¦ä¿å­˜åŸå§‹æ•¸æ“š
"""

import argparse
import os
import json
import time
import signal
import sys
import numpy as np
import pandas as pd
import pickle
from datetime import datetime
from pathlib import Path
from concurrent.futures import ProcessPoolExecutor, as_completed
from multiprocessing import cpu_count

# å…¨åŸŸè®Šæ•¸ç”¨æ–¼è¿½è¹¤ä¸­æ–·
interrupted = False

def signal_handler(sig, frame):
    """è™•ç†ä¸­æ–·ä¿¡è™Ÿ"""
    global interrupted
    interrupted = True
    print("\n\nâš ï¸  æ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿï¼Œæ­£åœ¨å®‰å…¨åœæ­¢è©•ä¼°...")
    print("è«‹ç¨å€™ï¼Œæ­£åœ¨ä¿å­˜å·²å®Œæˆçš„çµæœ...")
    # ä¸ç«‹å³é€€å‡ºï¼Œè®“ç¨‹åºæœ‰æ©Ÿæœƒä¿å­˜çµæœ

# è¨­ç½®ä¿¡è™Ÿè™•ç†å™¨
signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

# å°å…¥å¿…è¦çš„æ¨¡çµ„
import netlogo
from ai.controllers.dqn_controller import DQNController
from ai.controllers.nerl_controller import NEController
from ai.controllers.queue_based_controller import QueueBasedController
from ai.controllers.time_based_controller import TimeBasedController
from lib.logger import get_logger

class ControllerEvaluator:
    def __init__(self, evaluation_ticks=5000, num_runs=3, output_dir=None):
        self.evaluation_ticks = evaluation_ticks
        self.num_runs = num_runs
        
        # å¦‚æœæ²’æœ‰æŒ‡å®šè¼¸å‡ºç›®éŒ„ï¼Œå‰µå»ºå¸¶æ™‚é–“æˆ³çš„å­ç›®éŒ„
        if output_dir is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            base_dir = Path("result/evaluations")
            base_dir.mkdir(parents=True, exist_ok=True)
            self.output_dir = base_dir / f"EVAL_{timestamp}_{evaluation_ticks}ticks"
        else:
            # å¦‚æœæŒ‡å®šäº†è¼¸å‡ºç›®éŒ„ï¼Œç›´æ¥ä½¿ç”¨
            self.output_dir = Path(output_dir)
        
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # è¨­ç½®æ—¥èªŒ
        log_file = self.output_dir / "evaluation.log"
        self.logger = get_logger(log_file_path=str(log_file))
        
        # å­˜å„²çµæœ
        self.results = {}
        
    def load_trained_models(self):
        """è¼‰å…¥æ‰€æœ‰è¨“ç·´å¥½çš„æ¨¡å‹"""
        models = {}
        training_runs_dir = Path("models/training_runs")
        
        if not training_runs_dir.exists():
            self.logger.warning(f"æ‰¾ä¸åˆ°è¨“ç·´æ¨¡å‹ç›®éŒ„: {training_runs_dir}")
            return models
            
        # æƒææ‰€æœ‰è¨“ç·´é‹è¡Œç›®éŒ„
        for run_dir in training_runs_dir.iterdir():
            if not run_dir.is_dir():
                continue
                
            # å°‹æ‰¾æœ€ä½³æ¨¡å‹
            best_model_path = run_dir / "best_model.pth"
            metadata_path = run_dir / "metadata.json"
            
            if best_model_path.exists() and metadata_path.exists():
                try:
                    with open(metadata_path, 'r') as f:
                        metadata = json.load(f)
                    
                    agent_type = metadata.get('agent_type', 'unknown')
                    reward_mode = metadata.get('reward_mode', 'unknown')
                    
                    # å‰µå»ºæ¨¡å‹æ¨™è­˜ç¬¦
                    model_key = f"{agent_type}_{reward_mode}"
                    
                    models[model_key] = {
                        'type': agent_type,
                        'reward_mode': reward_mode,
                        'model_path': str(best_model_path),
                        'metadata': metadata
                    }
                    
                    self.logger.info(f"è¼‰å…¥æ¨¡å‹: {model_key} from {best_model_path}")
                    
                except Exception as e:
                    self.logger.error(f"è¼‰å…¥æ¨¡å‹å¤±æ•— {run_dir}: {e}")
                    
        return models
        
    def evaluate_controller(self, controller_name, controller_config, run_id=0):
        """è©•ä¼°å–®å€‹æ§åˆ¶å™¨"""
        self.logger.info(f"é–‹å§‹è©•ä¼° {controller_name} (é‹è¡Œ {run_id + 1}/{self.num_runs})")
        
        # æª¢æŸ¥æ˜¯å¦è¢«ä¸­æ–·
        if interrupted:
            self.logger.warning("è©•ä¼°è¢«ä¸­æ–·")
            return None
        
        start_time = time.time()
        
        try:
            # åˆå§‹åŒ–NetLogoæ¨¡å‹
            # ä½¿ç”¨ netlogo.py çš„ setup å‡½æ•¸
            netlogo.setup()
            
            # è¼‰å…¥ warehouse ç‹€æ…‹
            import pickle
            # ç¢ºä¿ states è³‡æ–™å¤¾å­˜åœ¨
            state_dir = 'states'
            if not os.path.exists(state_dir):
                os.makedirs(state_dir)
            
            sim_id = os.environ.get('SIMULATION_ID', '')
            if sim_id:
                state_file = os.path.join(state_dir, f'netlogo_{sim_id}.state')
            else:
                state_file = os.path.join(state_dir, 'netlogo.state')
            with open(state_file, 'rb') as file:
                warehouse = pickle.load(file)
            
            # å‰µå»ºæ§åˆ¶å™¨å¯¦ä¾‹
            controller_type = controller_config['type']
            controller = None
            
            if controller_type == 'dqn':
                controller = DQNController(
                    model_path=controller_config['model_path'],
                    reward_mode=controller_config['reward_mode']
                )
            elif controller_type == 'nerl':
                controller = NEController(
                    model_path=controller_config['model_path'],
                    reward_mode=controller_config['reward_mode']
                )
            elif controller_type == 'queue_based':
                controller = QueueBasedController()
            elif controller_type == 'time_based':
                controller = TimeBasedController()
            elif controller_type == 'none':
                # ç„¡æ§åˆ¶å™¨æ¨¡å¼
                controller = None
                self.logger.info("ä½¿ç”¨ç„¡æ§åˆ¶å™¨æ¨¡å¼ - ä¸é€²è¡Œäº¤é€šæ§åˆ¶")
            else:
                raise ValueError(f"æœªçŸ¥çš„æ§åˆ¶å™¨é¡å‹: {controller_type}")
            
            # è¨­ç½®æ§åˆ¶å™¨ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
            if controller is not None:
                # ä½¿ç”¨ warehouse çš„ set_traffic_controller æ–¹æ³•
                warehouse.set_traffic_controller(controller_type, model_path=controller_config.get('model_path'))
            else:
                # ç„¡æ§åˆ¶å™¨æ¨¡å¼ - ä¸å•Ÿç”¨äº¤é€šæ§åˆ¶
                warehouse.update_intersection_using_RL = False
                warehouse.current_controller = "none"
            
            # é‹è¡Œè©•ä¼°
            self.logger.info(f"é–‹å§‹é‹è¡Œ {self.evaluation_ticks} ticks...")
            
            # åˆå§‹åŒ–çµ±è¨ˆè®Šæ•¸
            metrics = {
                'completed_orders': 0,
                'total_orders': 0,
                'total_wait_time': 0,
                'total_robots': 0,
                'total_robot_active_time': 0,
                'total_energy_consumed': 0,
                'signal_switch_count': 0,
                'avg_traffic_rate': 0.0,
                'time_per_tick': []
            }
            
            # ä¸»è©•ä¼°å¾ªç’°
            tick_interval = 100  # æ¯100å€‹tickè¨˜éŒ„ä¸€æ¬¡
            save_interval = 5000  # æ¯5000å€‹tickä¿å­˜ä¸€æ¬¡ç‹€æ…‹ï¼ˆæ¸›å°‘I/Oï¼‰
            
            for tick in range(self.evaluation_ticks):
                # æª¢æŸ¥æ˜¯å¦è¢«ä¸­æ–·
                if interrupted:
                    self.logger.warning(f"åœ¨ tick {tick} è¢«ä¸­æ–·")
                    break
                    
                tick_start = time.time()
                
                # åŸ·è¡Œä¸€å€‹tick
                warehouse.tick()
                
                # åªåœ¨ç‰¹å®šé–“éš”æˆ–æœ€å¾Œä¸€å€‹tickæ™‚ä¿å­˜ç‹€æ…‹
                if tick % save_interval == 0 or tick == self.evaluation_ticks - 1:
                    with open(state_file, 'wb') as file:
                        pickle.dump(warehouse, file)
                    self.logger.debug(f"ä¿å­˜ç‹€æ…‹åœ¨ tick {tick}")
                
                # è¨˜éŒ„æ™‚é–“
                tick_time = time.time() - tick_start
                metrics['time_per_tick'].append(tick_time)
                
                # å®šæœŸæ”¶é›†çµ±è¨ˆ
                if tick % tick_interval == 0 or tick == self.evaluation_ticks - 1:
                    # æ”¶é›†ç•¶å‰çµ±è¨ˆ
                    current_completed = len(warehouse.order_manager.finished_orders)
                    current_total = len(warehouse.order_manager.orders)
                    
                    # æ›´æ–°ç´¯è¨ˆçµ±è¨ˆ
                    metrics['completed_orders'] = current_completed
                    metrics['total_orders'] = current_total
                    
                    # æ”¶é›†æ©Ÿå™¨äººçµ±è¨ˆ
                    total_robots = len(warehouse.robot_manager.robots)
                    working_robots = len([r for r in warehouse.robot_manager.robots if hasattr(r, 'route_stop_points') and len(r.route_stop_points) > 0])
                    
                    metrics['total_robots'] = total_robots
                    
                    # æ”¶é›†ç­‰å¾…æ™‚é–“
                    # è¨ˆç®—æ‰€æœ‰æ©Ÿå™¨äººåœ¨æ‰€æœ‰è·¯å£çš„ç¸½ç­‰å¾…æ™‚é–“
                    total_wait = 0
                    for robot in warehouse.robot_manager.robots:
                        if hasattr(robot, 'intersection_wait_time'):
                            total_wait += sum(robot.intersection_wait_time.values())
                    # ç´¯åŠ ç¸½ç­‰å¾…æ™‚é–“ï¼Œè€Œä¸æ˜¯å¹³å‡å€¼
                    metrics['total_wait_time'] = total_wait
                    
                    # æ”¶é›†èƒ½æºæ¶ˆè€—
                    # ä½¿ç”¨ warehouse.total_energy è€Œéæ©Ÿå™¨äººç´¯è¨ˆï¼Œä»¥ä¿æŒèˆ‡æ­·å²è©•ä¼°çš„ä¸€è‡´æ€§
                    metrics['total_energy_consumed'] = warehouse.total_energy
                    
                    # æ”¶é›†äº¤é€šæ§åˆ¶çµ±è¨ˆï¼ˆå¦‚æœæ§åˆ¶å™¨æ”¯æ´ï¼‰
                    if controller is not None:
                        if hasattr(controller, 'get_signal_switch_count'):
                            metrics['signal_switch_count'] = controller.get_signal_switch_count()
                        
                        if hasattr(controller, 'getAverageTrafficRate'):
                            metrics['avg_traffic_rate'] = controller.getAverageTrafficRate()
                    
                    # è¨˜éŒ„é€²åº¦
                    if tick % 1000 == 0:
                        self.logger.info(f"  é€²åº¦: {tick}/{self.evaluation_ticks} ticks, "
                                       f"å®Œæˆè¨‚å–®: {current_completed}/{current_total}")
            
            # è¨ˆç®—æœ€çµ‚çµ±è¨ˆ
            execution_time = time.time() - start_time
            final_tick = warehouse._tick
            
            # è¨ˆç®—è¡ç”ŸæŒ‡æ¨™
            completion_rate = metrics['completed_orders'] / metrics['total_orders'] if metrics['total_orders'] > 0 else 0
            # å¹³å‡ç­‰å¾…æ™‚é–“ = ç¸½ç­‰å¾…æ™‚é–“ / (æ©Ÿå™¨äººæ•¸ * tickæ•¸)
            avg_wait_time = metrics['total_wait_time'] / (metrics['total_robots'] * final_tick) if (metrics['total_robots'] > 0 and final_tick > 0) else 0
            robot_utilization = metrics['total_robot_active_time'] / (final_tick * metrics['total_robots']) if metrics['total_robots'] > 0 else 0
            energy_per_order = metrics['total_energy_consumed'] / metrics['completed_orders'] if metrics['completed_orders'] > 0 else 0
            
            # çµ„è£çµæœ
            result = {
                'controller_name': controller_name,
                'controller_type': controller_type,
                'run_id': run_id,
                'evaluation_ticks': self.evaluation_ticks,
                'warehouse_final_tick': final_tick,
                'completed_orders': metrics['completed_orders'],
                'total_orders': metrics['total_orders'],
                'completion_rate': completion_rate,
                'avg_wait_time': avg_wait_time,
                'robot_utilization': robot_utilization,
                'total_energy': metrics['total_energy_consumed'],
                'energy_per_order': energy_per_order,
                'signal_switch_count': metrics['signal_switch_count'],
                'avg_traffic_rate': metrics['avg_traffic_rate'],
                'execution_time': execution_time,
                'avg_tick_time': np.mean(metrics['time_per_tick']) if metrics['time_per_tick'] else 0,
                'timestamp': datetime.now().isoformat()
            }
            
            # æ·»åŠ å…ƒæ•¸æ“š
            if 'metadata' in controller_config:
                result['controller_metadata'] = controller_config['metadata']
            
            self.logger.info(f"è©•ä¼°å®Œæˆ: {controller_name} (é‹è¡Œ {run_id + 1}), "
                           f"å®Œæˆç‡: {completion_rate*100:.1f}%, "
                           f"åŸ·è¡Œæ™‚é–“: {execution_time:.1f}ç§’")
            
            # æ¸…ç†
            # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
            if os.path.exists(state_file):
                os.remove(state_file)
            
            return result
            
        except Exception as e:
            self.logger.error(f"è©•ä¼° {controller_name} å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def parse_controller_specs(self, controller_specs):
        """è§£ææ§åˆ¶å™¨è¦æ ¼"""
        controllers = {}
        
        for spec in controller_specs:
            if ':' in spec:
                # AIæ¨¡å‹æ ¼å¼: type:path/to/model.pth
                parts = spec.split(':', 1)
                controller_type = parts[0]
                model_path = parts[1]
                
                # å¾è·¯å¾‘ä¸­æå–åç¨±
                model_name = Path(model_path).stem
                controller_name = f"{controller_type}_{model_name}"
                
                # å˜—è©¦æ¨æ–·reward_mode
                if 'step' in model_name:
                    reward_mode = 'step'
                elif 'global' in model_name:
                    reward_mode = 'global'
                else:
                    reward_mode = 'unknown'
                
                controllers[controller_name] = {
                    'type': controller_type,
                    'reward_mode': reward_mode,
                    'model_path': model_path,
                    'metadata': {
                        'controller_type': controller_type,
                        'model_path': model_path
                    }
                }
            else:
                # å‚³çµ±æ§åˆ¶å™¨
                controllers[spec] = {
                    'type': spec,
                    'reward_mode': None,
                    'model_path': None,
                    'metadata': {'controller_type': spec}
                }
        
        return controllers
    
    def run_evaluation(self, controller_specs=None, parallel=False):
        """é‹è¡Œå®Œæ•´è©•ä¼°"""
        self.logger.info("é–‹å§‹æ§åˆ¶å™¨æ€§èƒ½è©•ä¼°")
        
        if controller_specs is not None:
            # ä½¿ç”¨æŒ‡å®šçš„æ§åˆ¶å™¨
            controllers_to_evaluate = self.parse_controller_specs(controller_specs)
            self.logger.info(f"è§£ææ§åˆ¶å™¨è¦æ ¼: {controller_specs}")
            self.logger.info(f"è§£æçµæœ: {list(controllers_to_evaluate.keys())}")
        else:
            # ç„¡æ§åˆ¶å™¨æ¨¡å¼
            self.logger.info("æœªæŒ‡å®šæ§åˆ¶å™¨ï¼Œå°‡åŸ·è¡Œç„¡æ§åˆ¶å™¨è©•ä¼°æ¨¡å¼")
            controllers_to_evaluate = {
                'no_controller': {
                    'type': 'none',
                    'reward_mode': None,
                    'model_path': None,
                    'metadata': {'controller_type': 'none'}
                }
            }
        
        self.logger.info(f"å°‡è©•ä¼° {len(controllers_to_evaluate)} å€‹æ§åˆ¶å™¨")
        
        # è©•ä¼°æ¯å€‹æ§åˆ¶å™¨
        all_results = []
        
        if parallel:
            # ä½µè¡Œè©•ä¼°
            self.logger.info(f"ä½¿ç”¨ä½µè¡Œæ¨¡å¼è©•ä¼° (æœ€å¤š {cpu_count()} é€²ç¨‹)")
            self._run_parallel_evaluation(controllers_to_evaluate, all_results)
        else:
            # ä¸²è¡Œè©•ä¼°
            for controller_name, controller_config in controllers_to_evaluate.items():
                # æª¢æŸ¥æ˜¯å¦è¢«ä¸­æ–·
                if interrupted:
                    self.logger.warning("è©•ä¼°è¢«ç”¨æˆ¶ä¸­æ–·")
                    break
                    
                self.logger.info(f"é–‹å§‹è©•ä¼°æ§åˆ¶å™¨: {controller_name}")
                
                controller_results = []
                for run_id in range(self.num_runs):
                    # æª¢æŸ¥æ˜¯å¦è¢«ä¸­æ–·
                    if interrupted:
                        self.logger.warning(f"åœ¨é‹è¡Œ {run_id + 1}/{self.num_runs} æ™‚è¢«ä¸­æ–·")
                        break
                        
                    result = self.evaluate_controller(controller_name, controller_config, run_id)
                    if result:
                        controller_results.append(result)
                        all_results.append(result)
                
                # ä¿å­˜è©²æ§åˆ¶å™¨çš„çµæœ
                if controller_results:
                    self.results[controller_name] = {
                        'individual_runs': controller_results,
                        'config': controller_config
                    }
        
        # ä¿å­˜çµæœï¼ˆå³ä½¿è¢«ä¸­æ–·ä¹Ÿè¦ä¿å­˜ï¼‰
        if all_results:
            self.save_results(all_results)
            
            if not interrupted:
                self.logger.info("è©•ä¼°å®Œæˆ")
            else:
                self.logger.warning("è©•ä¼°è¢«ä¸­æ–·ï¼Œå·²ä¿å­˜éƒ¨åˆ†çµæœ")
                # ä¿å­˜ä¸­æ–·ç‹€æ…‹
                interrupt_file = self.output_dir / "evaluation_interrupted.txt"
                with open(interrupt_file, 'w', encoding='utf-8') as f:
                    f.write(f"è©•ä¼°åœ¨ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} è¢«ä¸­æ–·\n")
                    f.write(f"å·²å®Œæˆçš„è©•ä¼°: {len(all_results)} å€‹é‹è¡Œ\n")
                    f.write(f"å·²è©•ä¼°çš„æ§åˆ¶å™¨: {', '.join(self.results.keys())}\n")
        else:
            self.logger.error("æ²’æœ‰å®Œæˆä»»ä½•è©•ä¼°")
            
        return self.results
    
    def _run_parallel_evaluation(self, controllers_to_evaluate, all_results):
        """ä½µè¡Œé‹è¡Œè©•ä¼°"""
        max_workers = min(cpu_count(), len(controllers_to_evaluate) * self.num_runs)
        
        self.logger.info(f"ä½µè¡Œè©•ä¼°é…ç½®: {len(controllers_to_evaluate)} å€‹æ§åˆ¶å™¨, "
                        f"æ¯å€‹é‹è¡Œ {self.num_runs} æ¬¡, ä½¿ç”¨ {max_workers} å€‹é€²ç¨‹")
        
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰è©•ä¼°ä»»å‹™
            future_to_task = {}
            
            for controller_name, controller_config in controllers_to_evaluate.items():
                for run_id in range(self.num_runs):
                    future = executor.submit(
                        self.evaluate_controller,
                        controller_name,
                        controller_config,
                        run_id
                    )
                    future_to_task[future] = (controller_name, run_id)
            
            # æ”¶é›†çµæœ
            controller_results_map = {name: [] for name in controllers_to_evaluate}
            
            for future in as_completed(future_to_task):
                # æª¢æŸ¥æ˜¯å¦è¢«ä¸­æ–·
                if interrupted:
                    self.logger.warning("ä½µè¡Œè©•ä¼°è¢«ç”¨æˆ¶ä¸­æ–·")
                    # å–æ¶ˆæ‰€æœ‰å¾…è™•ç†çš„ä»»å‹™
                    for f in future_to_task:
                        f.cancel()
                    break
                    
                controller_name, run_id = future_to_task[future]
                try:
                    result = future.result()
                    if result:
                        controller_results_map[controller_name].append(result)
                        all_results.append(result)
                        self.logger.info(f"å®Œæˆ: {controller_name} (é‹è¡Œ {run_id + 1})")
                except Exception as e:
                    self.logger.error(f"è©•ä¼°å¤±æ•— {controller_name} (é‹è¡Œ {run_id + 1}): {e}")
            
            # æ•´ç†çµæœ
            for controller_name, controller_results in controller_results_map.items():
                if controller_results:
                    self.results[controller_name] = {
                        'individual_runs': controller_results,
                        'config': controllers_to_evaluate[controller_name]
                    }
    
    def save_results(self, all_results):
        """ä¿å­˜è©•ä¼°çµæœ"""
        timestamp = datetime.now().strftime("%Y-%m-%d_%H%M%S")
        
        # ä¿å­˜è©³ç´°çµæœ
        results_file = self.output_dir / "evaluation_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump({
                'evaluation_config': {
                    'evaluation_ticks': self.evaluation_ticks,
                    'num_runs': self.num_runs,
                    'timestamp': timestamp
                },
                'results': all_results,
                'controllers_evaluated': list(self.results.keys())
            }, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜CSVæ ¼å¼
        df = pd.DataFrame(all_results)
        csv_file = self.output_dir / "evaluation_results.csv"
        df.to_csv(csv_file, index=False, encoding='utf-8')
        
        # ä¿å­˜è©•ä¼°æ‘˜è¦
        summary_file = self.output_dir / "evaluation_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump({
                'timestamp': timestamp,
                'evaluation_ticks': self.evaluation_ticks,
                'num_runs': self.num_runs,
                'controllers_evaluated': list(self.results.keys()),
                'total_runs_completed': len(all_results),
                'interrupted': interrupted
            }, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"çµæœå·²ä¿å­˜: {results_file}, {csv_file}")

def main():
    parser = argparse.ArgumentParser(description="RMFSæ§åˆ¶å™¨æ€§èƒ½è©•ä¼°")
    parser.add_argument('--eval_ticks', type=int, default=5000,
                       help='è©•ä¼°æ™‚é•· (ticks)')
    parser.add_argument('--num_runs', type=int, default=3,
                       help='æ¯å€‹æ§åˆ¶å™¨çš„é‡è¤‡é‹è¡Œæ¬¡æ•¸')
    parser.add_argument('--output_dir', default='evaluation_results',
                       help='çµæœè¼¸å‡ºç›®éŒ„')
    parser.add_argument('--controllers', nargs='+', 
                       help='æŒ‡å®šè¦è©•ä¼°çš„æ§åˆ¶å™¨åˆ—è¡¨ (ä¾‹å¦‚: time_based queue_based dqn:path/to/model.pth)')
    parser.add_argument('--parallel', action='store_true',
                       help='å•Ÿç”¨ä½µè¡Œè©•ä¼°æ¨¡å¼')
    parser.add_argument('--seed', type=int, default=42,
                       help='éš¨æ©Ÿç¨®å­')
    
    args = parser.parse_args()
    
    # è¨­ç½®éš¨æ©Ÿç¨®å­
    np.random.seed(args.seed)
    
    evaluator = ControllerEvaluator(
        evaluation_ticks=args.eval_ticks,
        num_runs=args.num_runs,
        output_dir=args.output_dir
    )
    
    results = evaluator.run_evaluation(
        controller_specs=args.controllers,
        parallel=args.parallel
    )
    
    print("\n" + "="*50)
    print("ğŸ“Š è©•ä¼°å®Œæˆï¼")
    print(f"çµæœä¿å­˜åœ¨: {evaluator.output_dir}")
    print("="*50)
    
    return results

if __name__ == "__main__":
    main()