#!/usr/bin/env python3
"""
RMFS æ§åˆ¶å™¨è©•ä¼°è…³æœ¬
ç”¨æ–¼æ¯”è¼ƒä¸åŒæ§åˆ¶å™¨çš„æ€§èƒ½
"""

import argparse
import os
import json
import time
import numpy as np
import pandas as pd
from datetime import datetime
from pathlib import Path
import matplotlib.pyplot as plt

# å°å…¥å¿…è¦çš„æ¨¡çµ„
import netlogo
from ai.controllers.dqn_controller import DQNController
from ai.controllers.nerl_controller import NEController
from ai.controllers.queue_based_controller import QueueBasedController
from ai.controllers.time_based_controller import TimeBasedController
from lib.logger import get_logger

class ControllerEvaluator:
    def __init__(self, evaluation_ticks=5000, num_runs=3, output_dir="evaluation_results"):
        self.evaluation_ticks = evaluation_ticks
        self.num_runs = num_runs
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # è¨­ç½®æ—¥èªŒ
        log_file = self.output_dir / "evaluation.log"
        self.logger = get_logger(log_file_path=str(log_file))
        
        # å­˜å„²çµæœ
        self.results = {}
        
    def load_trained_models(self):
        """è¼‰å…¥æ‰€æœ‰è¨“ç·´å¥½çš„æ¨¡å‹"""
        models = {}
        training_runs_dir = Path("models/training_runs")
        
        if not training_runs_dir.exists():
            self.logger.warning("æœªæ‰¾åˆ°è¨“ç·´çµæœç›®éŒ„")
            return models
            
        for run_dir in training_runs_dir.iterdir():
            if not run_dir.is_dir():
                continue
                
            metadata_file = run_dir / "metadata.json"
            if not metadata_file.exists():
                continue
                
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                controller_type = metadata.get('controller_type')
                reward_mode = metadata.get('reward_mode')
                
                # å°‹æ‰¾æœ€ä½³æ¨¡å‹æ–‡ä»¶
                model_file = None
                if controller_type == 'nerl':
                    model_file = run_dir / "best_model.pth"
                elif controller_type == 'dqn':
                    # å°‹æ‰¾finalæ¨¡å‹
                    for file in run_dir.glob("*.pth"):
                        if "final" in file.name:
                            model_file = file
                            break
                
                if model_file and model_file.exists():
                    model_name = f"{controller_type}_{reward_mode}"
                    models[model_name] = {
                        'type': controller_type,
                        'reward_mode': reward_mode,
                        'model_path': str(model_file),
                        'metadata': metadata,
                        'run_dir': run_dir
                    }
                    self.logger.info(f"æ‰¾åˆ°æ¨¡å‹: {model_name}")
                    
            except Exception as e:
                self.logger.warning(f"è¼‰å…¥æ¨¡å‹æ™‚å‡ºéŒ¯ {run_dir}: {e}")
                
        return models
    
    def evaluate_controller(self, controller_name, controller_config, run_id=0):
        """è©•ä¼°å–®å€‹æ§åˆ¶å™¨"""
        self.logger.info(f"é–‹å§‹è©•ä¼° {controller_name} (é‹è¡Œ {run_id + 1}/{self.num_runs})")
        
        try:
            # å‰µå»ºå€‰åº«ç’°å¢ƒ
            if controller_config['type'] in ['nerl', 'dqn']:
                # AIæ§åˆ¶å™¨
                controller_kwargs = {
                    'reward_mode': controller_config['reward_mode'],
                    'log_file_path': None  # è©•ä¼°æ™‚ä¸éœ€è¦è©³ç´°æ—¥èªŒ
                }
                warehouse = netlogo.training_setup(
                    controller_type=controller_config['type'], 
                    controller_kwargs=controller_kwargs
                )
                
                # è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹
                if controller_config['type'] == 'nerl':
                    controller = warehouse.intersection_manager.controllers.get('nerl')
                    if controller and controller.best_individual:
                        # è¼‰å…¥æœ€ä½³å€‹é«”
                        import torch
                        state_dict = torch.load(controller_config['model_path'], map_location='cpu')
                        controller.best_individual.load_state_dict(state_dict)
                        controller.set_active_individual(controller.best_individual)
                        controller.set_training_mode(False)
                        
                elif controller_config['type'] == 'dqn':
                    controller = warehouse.intersection_manager.controllers.get('dqn')
                    if controller:
                        controller.load_model(controller_config['model_path'])
                        controller.set_training_mode(False)
                        
            else:
                # å‚³çµ±æ§åˆ¶å™¨
                controller_kwargs = {}
                if controller_config['type'] == 'queue_based':
                    warehouse = netlogo.training_setup(controller_type="queue_based", controller_kwargs=controller_kwargs)
                elif controller_config['type'] == 'time_based':
                    warehouse = netlogo.training_setup(controller_type="time_based", controller_kwargs=controller_kwargs)
            
            if not warehouse:
                self.logger.error(f"ç„¡æ³•å‰µå»ºå€‰åº«ç’°å¢ƒ: {controller_name}")
                return None
                
            # è¨˜éŒ„é–‹å§‹æ™‚é–“
            start_time = time.time()
            
            # é‹è¡Œè©•ä¼°
            for tick in range(self.evaluation_ticks):
                if tick % 1000 == 0:
                    self.logger.info(f"{controller_name} è©•ä¼°é€²åº¦: {tick}/{self.evaluation_ticks}")
                
                warehouse, status = netlogo.training_tick(warehouse)
                
                if status != "OK":
                    self.logger.warning(f"{controller_name} æ¨¡æ“¬ç‹€æ…‹: {status}")
                    if "critical" in status.lower():
                        break
            
            # è¨ˆç®—åŸ·è¡Œæ™‚é–“
            execution_time = time.time() - start_time
            
            # æ”¶é›†çµæœ
            completed_orders = len([j for j in warehouse.job_manager.jobs if j.is_finished])
            total_orders = len(warehouse.job_manager.jobs)
            completion_rate = completed_orders / total_orders if total_orders > 0 else 0
            
            # è¨ˆç®—ç­‰å¾…æ™‚é–“çµ±è¨ˆ
            wait_times = []
            total_robots_waiting = 0
            max_wait_time = 0
            
            for intersection in warehouse.intersection_manager.intersections:
                for robot in list(intersection.horizontal_robots.values()) + list(intersection.vertical_robots.values()):
                    total_robots_waiting += 1
                    if hasattr(robot, 'current_intersection_start_time') and robot.current_intersection_start_time:
                        wait_time = warehouse._tick - robot.current_intersection_start_time
                        wait_times.append(wait_time)
                        max_wait_time = max(max_wait_time, wait_time)
            
            avg_wait_time = np.mean(wait_times) if wait_times else 0
            
            # è¨ˆç®—èƒ½æºæ•ˆç‡
            total_energy = getattr(warehouse, 'total_energy', 0)
            energy_per_order = total_energy / completed_orders if completed_orders > 0 else 0
            
            # è¨ˆç®—æ©Ÿå™¨äººåˆ©ç”¨ç‡ï¼ˆåŸºæ–¼æ™‚é–“çš„çœŸæ­£åˆ©ç”¨ç‡ï¼‰
            total_robots = 0
            total_utilization = 0.0
            current_tick = warehouse.current_tick
            
            for obj in warehouse.getMovableObjects():
                if obj.object_type == "robot":
                    total_robots += 1
                    
                    # è¨ˆç®—ç´¯ç©æ´»å‹•æ™‚é–“
                    accumulated_active_time = obj.total_active_time
                    
                    # å¦‚æœæ©Ÿå™¨äººç›®å‰è™•æ–¼éé–’ç½®ç‹€æ…‹ï¼ŒåŠ ä¸Šå¾ä¸Šæ¬¡ç‹€æ…‹è®ŠåŒ–åˆ°ç¾åœ¨çš„æ™‚é–“
                    if obj.current_state != 'idle' and obj.last_state_change_time > 0:
                        accumulated_active_time += current_tick - obj.last_state_change_time
                    
                    # è¨ˆç®—åˆ©ç”¨ç‡ï¼ˆé¿å…é™¤é›¶ï¼‰
                    if current_tick > 0:
                        robot_utilization_individual = accumulated_active_time / current_tick
                        total_utilization += min(robot_utilization_individual, 1.0)  # ç¢ºä¿ä¸è¶…é100%
            
            robot_utilization = total_utilization / total_robots if total_robots > 0 else 0.0
            
            # è¨ˆç®—äº¤å‰å£æ“å µ
            intersection_congestion = []
            for intersection in warehouse.intersection_manager.intersections:
                h_robots = len(intersection.horizontal_robots)
                v_robots = len(intersection.vertical_robots)
                total_at_intersection = h_robots + v_robots
                if total_at_intersection > 0:
                    intersection_congestion.append(total_at_intersection)
            
            avg_congestion = np.mean(intersection_congestion) if intersection_congestion else 0
            max_congestion = max(intersection_congestion) if intersection_congestion else 0
            
            result = {
                'controller_name': controller_name,
                'run_id': run_id,
                'execution_time': execution_time,
                'evaluation_ticks': self.evaluation_ticks,
                
                # æ ¸å¿ƒæ€§èƒ½æŒ‡æ¨™
                'completed_orders': completed_orders,
                'total_orders': total_orders,
                'completion_rate': completion_rate,
                
                # æ™‚é–“æ•ˆç‡æŒ‡æ¨™
                'avg_wait_time': avg_wait_time,
                'max_wait_time': max_wait_time,
                'total_robots_waiting': total_robots_waiting,
                
                # èƒ½æºæ•ˆç‡æŒ‡æ¨™
                'total_energy': total_energy,
                'energy_per_order': energy_per_order,
                
                # ç³»çµ±æ•ˆç‡æŒ‡æ¨™
                'robot_utilization': robot_utilization,
                'avg_intersection_congestion': avg_congestion,
                'max_intersection_congestion': max_congestion,
                
                # å…¶ä»–æŒ‡æ¨™
                'stop_and_go_events': getattr(warehouse, 'stop_and_go', 0),
                'warehouse_final_tick': warehouse._tick
            }
            
            self.logger.info(f"{controller_name} é‹è¡Œ {run_id + 1} å®Œæˆ: "
                           f"å®Œæˆç‡ {completion_rate*100:.1f}%, "
                           f"å¹³å‡ç­‰å¾… {avg_wait_time:.1f} ticks")
            
            return result
            
        except Exception as e:
            self.logger.error(f"è©•ä¼° {controller_name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            return None
    
    def run_evaluation(self):
        """é‹è¡Œå®Œæ•´è©•ä¼°"""
        self.logger.info("é–‹å§‹æ§åˆ¶å™¨æ€§èƒ½è©•ä¼°")
        
        # è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹
        trained_models = self.load_trained_models()
        
        # æ·»åŠ å‚³çµ±æ§åˆ¶å™¨
        controllers_to_evaluate = {
            **trained_models,
            'queue_based': {
                'type': 'queue_based',
                'reward_mode': None,
                'model_path': None,
                'metadata': {'controller_type': 'queue_based'}
            },
            'time_based': {
                'type': 'time_based', 
                'reward_mode': None,
                'model_path': None,
                'metadata': {'controller_type': 'time_based'}
            }
        }
        
        self.logger.info(f"å°‡è©•ä¼° {len(controllers_to_evaluate)} å€‹æ§åˆ¶å™¨")
        
        # è©•ä¼°æ¯å€‹æ§åˆ¶å™¨
        all_results = []
        
        for controller_name, controller_config in controllers_to_evaluate.items():
            self.logger.info(f"é–‹å§‹è©•ä¼°æ§åˆ¶å™¨: {controller_name}")
            
            controller_results = []
            for run_id in range(self.num_runs):
                result = self.evaluate_controller(controller_name, controller_config, run_id)
                if result:
                    controller_results.append(result)
                    all_results.append(result)
            
            # è¨ˆç®—è©²æ§åˆ¶å™¨çš„å¹³å‡æ€§èƒ½
            if controller_results:
                avg_result = self.calculate_average_performance(controller_results)
                self.results[controller_name] = {
                    'individual_runs': controller_results,
                    'average_performance': avg_result,
                    'config': controller_config
                }
        
        # ä¿å­˜çµæœ
        self.save_results(all_results)
        
        # ç”Ÿæˆæ¯”è¼ƒåœ–è¡¨
        self.generate_comparison_charts()
        
        # ç”Ÿæˆå ±å‘Š
        self.generate_evaluation_report()
        
        self.logger.info("è©•ä¼°å®Œæˆ")
        return self.results
    
    def calculate_average_performance(self, results):
        """è¨ˆç®—å¹³å‡æ€§èƒ½"""
        if not results:
            return None
            
        avg_result = {}
        numeric_fields = [
            'execution_time', 'completed_orders', 'completion_rate',
            'avg_wait_time', 'max_wait_time', 'total_energy', 'energy_per_order',
            'robot_utilization', 'avg_intersection_congestion', 'max_intersection_congestion'
        ]
        
        for field in numeric_fields:
            values = [r[field] for r in results if field in r and r[field] is not None]
            if values:
                avg_result[field] = np.mean(values)
                avg_result[f'{field}_std'] = np.std(values)
            else:
                avg_result[field] = 0
                avg_result[f'{field}_std'] = 0
        
        avg_result['num_runs'] = len(results)
        return avg_result
    
    def save_results(self, all_results):
        """ä¿å­˜è©•ä¼°çµæœ"""
        timestamp = datetime.now().strftime("%Y-%m-%d_%H%M%S")
        
        # ä¿å­˜è©³ç´°çµæœ
        results_file = self.output_dir / f"evaluation_results_{timestamp}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump({
                'evaluation_config': {
                    'evaluation_ticks': self.evaluation_ticks,
                    'num_runs': self.num_runs,
                    'timestamp': timestamp
                },
                'results': self.results
            }, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜CSVæ ¼å¼
        df = pd.DataFrame(all_results)
        csv_file = self.output_dir / f"evaluation_results_{timestamp}.csv"
        df.to_csv(csv_file, index=False, encoding='utf-8')
        
        self.logger.info(f"çµæœå·²ä¿å­˜: {results_file}, {csv_file}")
    
    def generate_comparison_charts(self):
        """ç”Ÿæˆæ¯”è¼ƒåœ–è¡¨"""
        if not self.results:
            return
            
        # æº–å‚™æ•¸æ“š
        controllers = []
        completion_rates = []
        avg_wait_times = []
        energy_per_orders = []
        robot_utilizations = []
        
        for name, data in self.results.items():
            avg_perf = data['average_performance']
            controllers.append(name)
            completion_rates.append(avg_perf['completion_rate'] * 100)
            avg_wait_times.append(avg_perf['avg_wait_time'])
            energy_per_orders.append(avg_perf['energy_per_order'])
            robot_utilizations.append(avg_perf['robot_utilization'] * 100)
        
        # å‰µå»ºæ¯”è¼ƒåœ–è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('æ§åˆ¶å™¨æ€§èƒ½è©•ä¼°æ¯”è¼ƒ', fontsize=16, fontweight='bold')
        
        # è¨­ç½®é¡è‰²
        colors = plt.cm.Set3(np.linspace(0, 1, len(controllers)))
        
        # å®Œæˆç‡æ¯”è¼ƒ
        bars1 = axes[0,0].bar(controllers, completion_rates, color=colors)
        axes[0,0].set_title('è¨‚å–®å®Œæˆç‡æ¯”è¼ƒ', fontweight='bold')
        axes[0,0].set_ylabel('å®Œæˆç‡ (%)')
        axes[0,0].tick_params(axis='x', rotation=45)
        
        # æ·»åŠ æ•¸å€¼æ¨™ç±¤
        for bar, rate in zip(bars1, completion_rates):
            height = bar.get_height()
            axes[0,0].text(bar.get_x() + bar.get_width()/2., height + 0.5,
                          f'{rate:.1f}%', ha='center', va='bottom', fontweight='bold')
        
        # å¹³å‡ç­‰å¾…æ™‚é–“æ¯”è¼ƒ
        bars2 = axes[0,1].bar(controllers, avg_wait_times, color=colors)
        axes[0,1].set_title('å¹³å‡ç­‰å¾…æ™‚é–“æ¯”è¼ƒ', fontweight='bold')
        axes[0,1].set_ylabel('ç­‰å¾…æ™‚é–“ (ticks)')
        axes[0,1].tick_params(axis='x', rotation=45)
        
        for bar, wait in zip(bars2, avg_wait_times):
            height = bar.get_height()
            axes[0,1].text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                          f'{wait:.1f}', ha='center', va='bottom', fontweight='bold')
        
        # èƒ½æºæ•ˆç‡æ¯”è¼ƒ
        bars3 = axes[1,0].bar(controllers, energy_per_orders, color=colors)
        axes[1,0].set_title('èƒ½æºæ•ˆç‡æ¯”è¼ƒ', fontweight='bold')
        axes[1,0].set_ylabel('æ¯è¨‚å–®èƒ½æºæ¶ˆè€—')
        axes[1,0].tick_params(axis='x', rotation=45)
        
        for bar, energy in zip(bars3, energy_per_orders):
            height = bar.get_height()
            if height > 0:
                axes[1,0].text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                              f'{energy:.0f}', ha='center', va='bottom', fontweight='bold')
        
        # æ©Ÿå™¨äººåˆ©ç”¨ç‡æ¯”è¼ƒ
        bars4 = axes[1,1].bar(controllers, robot_utilizations, color=colors)
        axes[1,1].set_title('æ©Ÿå™¨äººåˆ©ç”¨ç‡æ¯”è¼ƒ', fontweight='bold')
        axes[1,1].set_ylabel('åˆ©ç”¨ç‡ (%)')
        axes[1,1].tick_params(axis='x', rotation=45)
        
        for bar, util in zip(bars4, robot_utilizations):
            height = bar.get_height()
            axes[1,1].text(bar.get_x() + bar.get_width()/2., height + 1,
                          f'{util:.1f}%', ha='center', va='bottom', fontweight='bold')
        
        plt.tight_layout()
        
        # ä¿å­˜åœ–è¡¨
        timestamp = datetime.now().strftime("%Y-%m-%d_%H%M%S")
        chart_file = self.output_dir / f"evaluation_comparison_{timestamp}.png"
        plt.savefig(chart_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        self.logger.info(f"æ¯”è¼ƒåœ–è¡¨å·²ä¿å­˜: {chart_file}")
    
    def generate_evaluation_report(self):
        """ç”Ÿæˆè©•ä¼°å ±å‘Š"""
        timestamp = datetime.now().strftime("%Y-%m-%d_%H%M%S")
        report_file = self.output_dir / f"evaluation_report_{timestamp}.txt"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("="*60 + "\n")
            f.write("RMFS æ§åˆ¶å™¨æ€§èƒ½è©•ä¼°å ±å‘Š\n")
            f.write("="*60 + "\n")
            f.write(f"è©•ä¼°æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"è©•ä¼°æ™‚é•·: {self.evaluation_ticks} ticks\n")
            f.write(f"é‡è¤‡é‹è¡Œ: {self.num_runs} æ¬¡\n\n")
            
            # æ€§èƒ½æ’å
            if self.results:
                # æŒ‰å®Œæˆç‡æ’åº
                sorted_by_completion = sorted(
                    self.results.items(),
                    key=lambda x: x[1]['average_performance']['completion_rate'],
                    reverse=True
                )
                
                f.write("1. è¨‚å–®å®Œæˆç‡æ’å\n")
                f.write("-" * 30 + "\n")
                for i, (name, data) in enumerate(sorted_by_completion, 1):
                    avg_perf = data['average_performance']
                    f.write(f"{i}. {name}: {avg_perf['completion_rate']*100:.1f}% "
                           f"(Â±{avg_perf['completion_rate_std']*100:.1f}%)\n")
                
                # æŒ‰ç­‰å¾…æ™‚é–“æ’åºï¼ˆè¶Šä½è¶Šå¥½ï¼‰
                sorted_by_wait = sorted(
                    self.results.items(),
                    key=lambda x: x[1]['average_performance']['avg_wait_time']
                )
                
                f.write("\n2. å¹³å‡ç­‰å¾…æ™‚é–“æ’å (è¶Šä½è¶Šå¥½)\n")
                f.write("-" * 30 + "\n")
                for i, (name, data) in enumerate(sorted_by_wait, 1):
                    avg_perf = data['average_performance']
                    f.write(f"{i}. {name}: {avg_perf['avg_wait_time']:.1f} ticks "
                           f"(Â±{avg_perf['avg_wait_time_std']:.1f})\n")
                
                # è©³ç´°æ€§èƒ½åˆ†æ
                f.write("\n3. è©³ç´°æ€§èƒ½åˆ†æ\n")
                f.write("-" * 30 + "\n")
                
                for name, data in self.results.items():
                    avg_perf = data['average_performance']
                    f.write(f"\n{name}:\n")
                    f.write(f"  å®Œæˆç‡: {avg_perf['completion_rate']*100:.1f}% (Â±{avg_perf['completion_rate_std']*100:.1f}%)\n")
                    f.write(f"  å¹³å‡ç­‰å¾…æ™‚é–“: {avg_perf['avg_wait_time']:.1f} ticks (Â±{avg_perf['avg_wait_time_std']:.1f})\n")
                    f.write(f"  æ©Ÿå™¨äººåˆ©ç”¨ç‡: {avg_perf['robot_utilization']*100:.1f}% (Â±{avg_perf['robot_utilization_std']*100:.1f}%)\n")
                    f.write(f"  å¹³å‡åŸ·è¡Œæ™‚é–“: {avg_perf['execution_time']:.1f} ç§’ (Â±{avg_perf['execution_time_std']:.1f})\n")
                    
                    if avg_perf['energy_per_order'] > 0:
                        f.write(f"  æ¯è¨‚å–®èƒ½æºæ¶ˆè€—: {avg_perf['energy_per_order']:.0f} (Â±{avg_perf['energy_per_order_std']:.0f})\n")
                
                # çµè«–å’Œå»ºè­°
                f.write("\n4. çµè«–å’Œå»ºè­°\n")
                f.write("-" * 30 + "\n")
                
                best_controller = sorted_by_completion[0]
                f.write(f"â€¢ æœ€ä½³æ•´é«”æ€§èƒ½: {best_controller[0]} "
                       f"(å®Œæˆç‡ {best_controller[1]['average_performance']['completion_rate']*100:.1f}%)\n")
                
                fastest_controller = sorted_by_wait[0]
                f.write(f"â€¢ æœ€ä½ç­‰å¾…æ™‚é–“: {fastest_controller[0]} "
                       f"(å¹³å‡ç­‰å¾… {fastest_controller[1]['average_performance']['avg_wait_time']:.1f} ticks)\n")
                
                f.write("\nå»ºè­°:\n")
                f.write("â€¢ å¦‚æœè¿½æ±‚æœ€é«˜å®Œæˆç‡ï¼Œå»ºè­°ä½¿ç”¨ " + best_controller[0] + "\n")
                f.write("â€¢ å¦‚æœè¿½æ±‚æœ€ä½å»¶é²ï¼Œå»ºè­°ä½¿ç”¨ " + fastest_controller[0] + "\n")
                f.write("â€¢ è€ƒæ…®çµåˆå¤šç¨®æ§åˆ¶ç­–ç•¥çš„æ··åˆæ–¹æ³•\n")
        
        self.logger.info(f"è©•ä¼°å ±å‘Šå·²ä¿å­˜: {report_file}")

def main():
    parser = argparse.ArgumentParser(description="RMFSæ§åˆ¶å™¨æ€§èƒ½è©•ä¼°")
    parser.add_argument('--eval_ticks', type=int, default=5000,
                       help='è©•ä¼°æ™‚é•· (ticks)')
    parser.add_argument('--num_runs', type=int, default=3,
                       help='æ¯å€‹æ§åˆ¶å™¨çš„é‡è¤‡é‹è¡Œæ¬¡æ•¸')
    parser.add_argument('--output_dir', default='evaluation_results',
                       help='çµæœè¼¸å‡ºç›®éŒ„')
    
    args = parser.parse_args()
    
    evaluator = ControllerEvaluator(
        evaluation_ticks=args.eval_ticks,
        num_runs=args.num_runs,
        output_dir=args.output_dir
    )
    
    results = evaluator.run_evaluation()
    
    print("\n" + "="*50)
    print("ğŸ“Š è©•ä¼°å®Œæˆï¼")
    print(f"çµæœä¿å­˜åœ¨: {evaluator.output_dir}")
    print("="*50)
    
    return results

if __name__ == "__main__":
    main()