#!/usr/bin/env python3
"""
RMFS æ§åˆ¶å™¨è©•ä¼°è…³æœ¬
ç”¨æ–¼æ¯”è¼ƒä¸åŒæ§åˆ¶å™¨çš„æ€§èƒ½
"""

import argparse
import os
import json
import time
import signal
import sys
import numpy as np
import pandas as pd
from datetime import datetime
from pathlib import Path
import matplotlib.pyplot as plt
from concurrent.futures import ProcessPoolExecutor, as_completed
from multiprocessing import cpu_count

# è¨­ç½® matplotlib å­—é«”
try:
    from fix_matplotlib_font import setup_matplotlib_font, get_chart_labels
    chinese_support = setup_matplotlib_font()
except:
    # å¦‚æœç„¡æ³•è¼‰å…¥ä¿®å¾©æ¨¡çµ„ï¼Œä½¿ç”¨åŸºæœ¬è¨­ç½®
    import matplotlib
    matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans']
    matplotlib.rcParams['axes.unicode_minus'] = False
    chinese_support = False
    
    def get_chart_labels(chinese_support=True):
        if chinese_support:
            return {
                'title': 'æ§åˆ¶å™¨æ€§èƒ½è©•ä¼°æ¯”è¼ƒ',
                'completion_rate': 'è¨‚å–®å®Œæˆç‡æ¯”è¼ƒ',
                'wait_time': 'å¹³å‡ç­‰å¾…æ™‚é–“æ¯”è¼ƒ',
                'energy': 'èƒ½æºæ•ˆç‡æ¯”è¼ƒ',
                'utilization': 'æ©Ÿå™¨äººåˆ©ç”¨ç‡æ¯”è¼ƒ',
                'completion_rate_y': 'å®Œæˆç‡ (%)',
                'wait_time_y': 'ç­‰å¾…æ™‚é–“ (ticks)',
                'energy_y': 'æ¯è¨‚å–®èƒ½æºæ¶ˆè€—',
                'utilization_y': 'åˆ©ç”¨ç‡ (%)'
            }
        else:
            return {
                'title': 'Controller Performance Comparison',
                'completion_rate': 'Order Completion Rate',
                'wait_time': 'Average Wait Time',
                'energy': 'Energy Efficiency',
                'utilization': 'Robot Utilization',
                'completion_rate_y': 'Completion Rate (%)',
                'wait_time_y': 'Wait Time (ticks)',
                'energy_y': 'Energy per Order',
                'utilization_y': 'Utilization (%)'
            }

# å…¨åŸŸè®Šæ•¸ç”¨æ–¼è¿½è¹¤ä¸­æ–·
interrupted = False

def signal_handler(sig, frame):
    """è™•ç†ä¸­æ–·ä¿¡è™Ÿ"""
    global interrupted
    interrupted = True
    print("\n\nâš ï¸  æ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿï¼Œæ­£åœ¨å®‰å…¨åœæ­¢è©•ä¼°...")
    print("è«‹ç¨å€™ï¼Œæ­£åœ¨ä¿å­˜å·²å®Œæˆçš„çµæœ...")
    # ä¸ç«‹å³é€€å‡ºï¼Œè®“ç¨‹åºæœ‰æ©Ÿæœƒä¿å­˜çµæœ

# è¨­ç½®ä¿¡è™Ÿè™•ç†å™¨
signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

# å°å…¥å¿…è¦çš„æ¨¡çµ„
import netlogo
from ai.controllers.dqn_controller import DQNController
from ai.controllers.nerl_controller import NEController
from ai.controllers.queue_based_controller import QueueBasedController
from ai.controllers.time_based_controller import TimeBasedController
from lib.logger import get_logger

class ControllerEvaluator:
    def __init__(self, evaluation_ticks=5000, num_runs=3, output_dir="evaluation_results"):
        self.evaluation_ticks = evaluation_ticks
        self.num_runs = num_runs
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # è¨­ç½®æ—¥èªŒ
        log_file = self.output_dir / "evaluation.log"
        self.logger = get_logger(log_file_path=str(log_file))
        
        # å­˜å„²çµæœ
        self.results = {}
        
    def load_trained_models(self):
        """è¼‰å…¥æ‰€æœ‰è¨“ç·´å¥½çš„æ¨¡å‹"""
        models = {}
        training_runs_dir = Path("models/training_runs")
        
        if not training_runs_dir.exists():
            self.logger.warning("æœªæ‰¾åˆ°è¨“ç·´çµæœç›®éŒ„")
            return models
            
        for run_dir in training_runs_dir.iterdir():
            if not run_dir.is_dir():
                continue
                
            metadata_file = run_dir / "metadata.json"
            if not metadata_file.exists():
                continue
                
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                controller_type = metadata.get('controller_type')
                reward_mode = metadata.get('reward_mode')
                
                # å°‹æ‰¾æœ€ä½³æ¨¡å‹æ–‡ä»¶
                model_file = None
                if controller_type == 'nerl':
                    model_file = run_dir / "best_model.pth"
                elif controller_type == 'dqn':
                    # å°‹æ‰¾finalæ¨¡å‹
                    for file in run_dir.glob("*.pth"):
                        if "final" in file.name:
                            model_file = file
                            break
                
                if model_file and model_file.exists():
                    model_name = f"{controller_type}_{reward_mode}"
                    models[model_name] = {
                        'type': controller_type,
                        'reward_mode': reward_mode,
                        'model_path': str(model_file),
                        'metadata': metadata,
                        'run_dir': run_dir
                    }
                    self.logger.info(f"æ‰¾åˆ°æ¨¡å‹: {model_name}")
                    
            except Exception as e:
                self.logger.warning(f"è¼‰å…¥æ¨¡å‹æ™‚å‡ºéŒ¯ {run_dir}: {e}")
                
        return models
    
    def evaluate_controller(self, controller_name, controller_config, run_id=0):
        """è©•ä¼°å–®å€‹æ§åˆ¶å™¨"""
        self.logger.info(f"é–‹å§‹è©•ä¼° {controller_name} (é‹è¡Œ {run_id + 1}/{self.num_runs})")
        
        try:
            # å‰µå»ºå€‰åº«ç’°å¢ƒ
            if controller_config['type'] in ['nerl', 'dqn']:
                # AIæ§åˆ¶å™¨
                controller_kwargs = {
                    'reward_mode': controller_config['reward_mode'],
                    'log_file_path': None  # è©•ä¼°æ™‚ä¸éœ€è¦è©³ç´°æ—¥èªŒ
                }
                warehouse = netlogo.training_setup(
                    controller_type=controller_config['type'], 
                    controller_kwargs=controller_kwargs
                )
                
                # è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹
                if controller_config['type'] == 'nerl':
                    controller = warehouse.intersection_manager.controllers.get('nerl')
                    if controller and controller.best_individual:
                        # è¼‰å…¥æœ€ä½³å€‹é«”
                        import torch
                        state_dict = torch.load(controller_config['model_path'], map_location='cpu')
                        controller.best_individual.load_state_dict(state_dict)
                        controller.set_active_individual(controller.best_individual)
                        controller.set_training_mode(False)
                        
                elif controller_config['type'] == 'dqn':
                    controller = warehouse.intersection_manager.controllers.get('dqn')
                    if controller:
                        controller.load_model(controller_config['model_path'])
                        controller.set_training_mode(False)
                        
            else:
                # å‚³çµ±æ§åˆ¶å™¨
                controller_kwargs = {}
                if controller_config['type'] == 'queue_based':
                    warehouse = netlogo.training_setup(controller_type="queue_based", controller_kwargs=controller_kwargs)
                elif controller_config['type'] == 'time_based':
                    warehouse = netlogo.training_setup(controller_type="time_based", controller_kwargs=controller_kwargs)
            
            if not warehouse:
                self.logger.error(f"ç„¡æ³•å‰µå»ºå€‰åº«ç’°å¢ƒ: {controller_name}")
                return None
                
            # è¨˜éŒ„é–‹å§‹æ™‚é–“
            start_time = time.time()
            
            # é‹è¡Œè©•ä¼°
            for tick in range(self.evaluation_ticks):
                if tick % 1000 == 0:
                    self.logger.info(f"{controller_name} è©•ä¼°é€²åº¦: {tick}/{self.evaluation_ticks}")
                
                warehouse, status = netlogo.training_tick(warehouse)
                
                if status != "OK":
                    self.logger.warning(f"{controller_name} æ¨¡æ“¬ç‹€æ…‹: {status}")
                    if "critical" in status.lower():
                        break
            
            # è¨ˆç®—åŸ·è¡Œæ™‚é–“
            execution_time = time.time() - start_time
            
            # æ”¶é›†çµæœ
            completed_orders = len([j for j in warehouse.job_manager.jobs if j.is_finished])
            total_orders = len(warehouse.job_manager.jobs)
            completion_rate = completed_orders / total_orders if total_orders > 0 else 0
            
            # è¨ˆç®—ç­‰å¾…æ™‚é–“çµ±è¨ˆ
            wait_times = []
            total_robots_waiting = 0
            max_wait_time = 0
            
            for intersection in warehouse.intersection_manager.intersections:
                for robot in list(intersection.horizontal_robots.values()) + list(intersection.vertical_robots.values()):
                    total_robots_waiting += 1
                    if hasattr(robot, 'current_intersection_start_time') and robot.current_intersection_start_time:
                        wait_time = warehouse._tick - robot.current_intersection_start_time
                        wait_times.append(wait_time)
                        max_wait_time = max(max_wait_time, wait_time)
            
            avg_wait_time = np.mean(wait_times) if wait_times else 0
            
            # è¨ˆç®—èƒ½æºæ•ˆç‡
            total_energy = getattr(warehouse, 'total_energy', 0)
            energy_per_order = total_energy / completed_orders if completed_orders > 0 else 0
            
            # è¨ˆç®—æ©Ÿå™¨äººåˆ©ç”¨ç‡ï¼ˆåŸºæ–¼æ™‚é–“çš„çœŸæ­£åˆ©ç”¨ç‡ï¼‰
            total_robots = 0
            total_utilization = 0.0
            current_tick = warehouse._tick
            
            for obj in warehouse.getMovableObjects():
                if obj.object_type == "robot":
                    total_robots += 1
                    
                    # è¨ˆç®—ç´¯ç©æ´»å‹•æ™‚é–“
                    accumulated_active_time = obj.total_active_time
                    
                    # å¦‚æœæ©Ÿå™¨äººç›®å‰è™•æ–¼éé–’ç½®ç‹€æ…‹ï¼ŒåŠ ä¸Šå¾ä¸Šæ¬¡ç‹€æ…‹è®ŠåŒ–åˆ°ç¾åœ¨çš„æ™‚é–“
                    if obj.current_state != 'idle' and obj.last_state_change_time > 0:
                        accumulated_active_time += current_tick - obj.last_state_change_time
                    
                    # è¨ˆç®—åˆ©ç”¨ç‡ï¼ˆé¿å…é™¤é›¶ï¼‰
                    if current_tick > 0:
                        robot_utilization_individual = accumulated_active_time / current_tick
                        total_utilization += min(robot_utilization_individual, 1.0)  # ç¢ºä¿ä¸è¶…é100%
            
            robot_utilization = total_utilization / total_robots if total_robots > 0 else 0.0
            
            # è¨ˆç®—äº¤å‰å£æ“å µ
            intersection_congestion = []
            for intersection in warehouse.intersection_manager.intersections:
                h_robots = len(intersection.horizontal_robots)
                v_robots = len(intersection.vertical_robots)
                total_at_intersection = h_robots + v_robots
                if total_at_intersection > 0:
                    intersection_congestion.append(total_at_intersection)
            
            avg_congestion = np.mean(intersection_congestion) if intersection_congestion else 0
            max_congestion = max(intersection_congestion) if intersection_congestion else 0
            
            # è¨ˆç®—ä¿¡è™Ÿåˆ‡æ›ç¸½æ•¸
            total_signal_switches = 0
            for intersection in warehouse.intersection_manager.intersections:
                if hasattr(intersection, 'signal_switch_count'):
                    total_signal_switches += intersection.signal_switch_count
            
            # è¨ˆç®—å¹³å‡äº¤é€šæµé‡ç‡
            traffic_rates = []
            for intersection in warehouse.intersection_manager.intersections:
                if hasattr(intersection, 'getAverageTrafficRate'):
                    rate = intersection.getAverageTrafficRate(warehouse._tick)
                    if rate > 0:
                        traffic_rates.append(rate)
            avg_traffic_rate = np.mean(traffic_rates) if traffic_rates else 0
            
            result = {
                'controller_name': controller_name,
                'run_id': run_id,
                'execution_time': execution_time,
                'evaluation_ticks': self.evaluation_ticks,
                
                # æ ¸å¿ƒæ€§èƒ½æŒ‡æ¨™
                'completed_orders': completed_orders,
                'total_orders': total_orders,
                'completion_rate': completion_rate,
                
                # æ™‚é–“æ•ˆç‡æŒ‡æ¨™
                'avg_wait_time': avg_wait_time,
                'max_wait_time': max_wait_time,
                'total_robots_waiting': total_robots_waiting,
                
                # èƒ½æºæ•ˆç‡æŒ‡æ¨™
                'total_energy': total_energy,
                'energy_per_order': energy_per_order,
                
                # ç³»çµ±æ•ˆç‡æŒ‡æ¨™
                'robot_utilization': robot_utilization,
                'avg_intersection_congestion': avg_congestion,
                'max_intersection_congestion': max_congestion,
                
                # äº¤é€šæ§åˆ¶æŒ‡æ¨™
                'signal_switch_count': total_signal_switches,
                'avg_traffic_rate': avg_traffic_rate,
                
                # å…¶ä»–æŒ‡æ¨™
                'stop_and_go_events': getattr(warehouse, 'stop_and_go', 0),
                'warehouse_final_tick': warehouse._tick
            }
            
            self.logger.info(f"{controller_name} é‹è¡Œ {run_id + 1} å®Œæˆ: "
                           f"å®Œæˆç‡ {completion_rate*100:.1f}%, "
                           f"å¹³å‡ç­‰å¾… {avg_wait_time:.1f} ticks")
            
            return result
            
        except Exception as e:
            self.logger.error(f"è©•ä¼° {controller_name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            return None
    
    def parse_controller_specs(self, controller_specs):
        """è§£ææ§åˆ¶å™¨è¦æ ¼åˆ—è¡¨"""
        controllers = {}
        
        for spec in controller_specs:
            if ':' in spec:
                # AIæ¨¡å‹æ ¼å¼: type:path
                controller_type, model_path = spec.split(':', 1)
                
                # å¾è·¯å¾‘æ¨æ–· reward_mode
                reward_mode = 'step' if 'step' in model_path else 'global'
                
                controller_name = f"{controller_type}_{reward_mode}_custom"
                controllers[controller_name] = {
                    'type': controller_type,
                    'reward_mode': reward_mode,
                    'model_path': model_path,
                    'metadata': {
                        'controller_type': controller_type,
                        'source': 'custom_path'
                    }
                }
            else:
                # å‚³çµ±æ§åˆ¶å™¨
                if spec in ['time_based', 'queue_based']:
                    controllers[spec] = {
                        'type': spec,
                        'reward_mode': None,
                        'model_path': None,
                        'metadata': {'controller_type': spec}
                    }
                else:
                    self.logger.warning(f"æœªçŸ¥çš„æ§åˆ¶å™¨é¡å‹: {spec}")
        
        return controllers
    
    def run_evaluation(self, controller_specs=None, parallel=False):
        """é‹è¡Œå®Œæ•´è©•ä¼°"""
        self.logger.info("é–‹å§‹æ§åˆ¶å™¨æ€§èƒ½è©•ä¼°")
        
        if controller_specs:
            # ä½¿ç”¨æŒ‡å®šçš„æ§åˆ¶å™¨
            controllers_to_evaluate = self.parse_controller_specs(controller_specs)
            self.logger.info(f"è§£ææ§åˆ¶å™¨è¦æ ¼: {controller_specs}")
            self.logger.info(f"è§£æçµæœ: {list(controllers_to_evaluate.keys())}")
        else:
            # è¼‰å…¥æ‰€æœ‰å¯ç”¨çš„æ§åˆ¶å™¨
            trained_models = self.load_trained_models()
            
            # æ·»åŠ å‚³çµ±æ§åˆ¶å™¨
            controllers_to_evaluate = {
                **trained_models,
                'queue_based': {
                    'type': 'queue_based',
                    'reward_mode': None,
                    'model_path': None,
                    'metadata': {'controller_type': 'queue_based'}
                },
                'time_based': {
                    'type': 'time_based', 
                    'reward_mode': None,
                    'model_path': None,
                    'metadata': {'controller_type': 'time_based'}
                }
            }
        
        self.logger.info(f"å°‡è©•ä¼° {len(controllers_to_evaluate)} å€‹æ§åˆ¶å™¨")
        
        # è©•ä¼°æ¯å€‹æ§åˆ¶å™¨
        all_results = []
        
        if parallel:
            # ä½µè¡Œè©•ä¼°
            self.logger.info(f"ä½¿ç”¨ä½µè¡Œæ¨¡å¼è©•ä¼° (æœ€å¤š {cpu_count()} é€²ç¨‹)")
            self._run_parallel_evaluation(controllers_to_evaluate, all_results)
        else:
            # ä¸²è¡Œè©•ä¼°
            for controller_name, controller_config in controllers_to_evaluate.items():
                # æª¢æŸ¥æ˜¯å¦è¢«ä¸­æ–·
                if interrupted:
                    self.logger.warning("è©•ä¼°è¢«ç”¨æˆ¶ä¸­æ–·")
                    break
                    
                self.logger.info(f"é–‹å§‹è©•ä¼°æ§åˆ¶å™¨: {controller_name}")
                
                controller_results = []
                for run_id in range(self.num_runs):
                    # æª¢æŸ¥æ˜¯å¦è¢«ä¸­æ–·
                    if interrupted:
                        self.logger.warning(f"åœ¨é‹è¡Œ {run_id + 1}/{self.num_runs} æ™‚è¢«ä¸­æ–·")
                        break
                        
                    result = self.evaluate_controller(controller_name, controller_config, run_id)
                    if result:
                        controller_results.append(result)
                        all_results.append(result)
                
                # è¨ˆç®—è©²æ§åˆ¶å™¨çš„å¹³å‡æ€§èƒ½
                if controller_results:
                    avg_result = self.calculate_average_performance(controller_results)
                    self.results[controller_name] = {
                        'individual_runs': controller_results,
                        'average_performance': avg_result,
                        'config': controller_config
                    }
        
        # ä¿å­˜çµæœï¼ˆå³ä½¿è¢«ä¸­æ–·ä¹Ÿè¦ä¿å­˜ï¼‰
        if all_results:
            self.save_results(all_results)
            
            # åªæœ‰åœ¨æ²’è¢«ä¸­æ–·çš„æƒ…æ³ä¸‹æ‰ç”Ÿæˆå®Œæ•´å ±å‘Š
            if not interrupted:
                # ç”Ÿæˆæ¯”è¼ƒåœ–è¡¨
                self.generate_comparison_charts()
                
                # ç”Ÿæˆå ±å‘Š
                self.generate_evaluation_report()
                
                self.logger.info("è©•ä¼°å®Œæˆ")
            else:
                self.logger.warning("è©•ä¼°è¢«ä¸­æ–·ï¼Œå·²ä¿å­˜éƒ¨åˆ†çµæœ")
                # ä¿å­˜ä¸­æ–·ç‹€æ…‹
                interrupt_file = self.output_dir / "evaluation_interrupted.txt"
                with open(interrupt_file, 'w', encoding='utf-8') as f:
                    f.write(f"è©•ä¼°åœ¨ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} è¢«ä¸­æ–·\n")
                    f.write(f"å·²å®Œæˆçš„è©•ä¼°: {len(all_results)} å€‹é‹è¡Œ\n")
                    f.write(f"å·²è©•ä¼°çš„æ§åˆ¶å™¨: {', '.join(self.results.keys())}\n")
        else:
            self.logger.error("æ²’æœ‰å®Œæˆä»»ä½•è©•ä¼°")
            
        return self.results
    
    def calculate_average_performance(self, results):
        """è¨ˆç®—å¹³å‡æ€§èƒ½"""
        if not results:
            return None
            
        avg_result = {}
        numeric_fields = [
            'execution_time', 'completed_orders', 'completion_rate',
            'avg_wait_time', 'max_wait_time', 'total_energy', 'energy_per_order',
            'robot_utilization', 'avg_intersection_congestion', 'max_intersection_congestion'
        ]
        
        for field in numeric_fields:
            values = [r[field] for r in results if field in r and r[field] is not None]
            if values:
                avg_result[field] = np.mean(values)
                avg_result[f'{field}_std'] = np.std(values)
            else:
                avg_result[field] = 0
                avg_result[f'{field}_std'] = 0
        
        avg_result['num_runs'] = len(results)
        return avg_result
    
    def _run_parallel_evaluation(self, controllers_to_evaluate, all_results):
        """ä½µè¡Œé‹è¡Œè©•ä¼°"""
        max_workers = min(cpu_count(), len(controllers_to_evaluate) * self.num_runs)
        
        self.logger.info(f"ä½µè¡Œè©•ä¼°é…ç½®: {len(controllers_to_evaluate)} å€‹æ§åˆ¶å™¨, "
                        f"æ¯å€‹é‹è¡Œ {self.num_runs} æ¬¡, ä½¿ç”¨ {max_workers} å€‹é€²ç¨‹")
        
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰è©•ä¼°ä»»å‹™
            future_to_task = {}
            
            for controller_name, controller_config in controllers_to_evaluate.items():
                for run_id in range(self.num_runs):
                    future = executor.submit(
                        self.evaluate_controller,
                        controller_name,
                        controller_config,
                        run_id
                    )
                    future_to_task[future] = (controller_name, run_id)
            
            # æ”¶é›†çµæœ
            controller_results_map = {name: [] for name in controllers_to_evaluate}
            
            for future in as_completed(future_to_task):
                # æª¢æŸ¥æ˜¯å¦è¢«ä¸­æ–·
                if interrupted:
                    self.logger.warning("ä½µè¡Œè©•ä¼°è¢«ç”¨æˆ¶ä¸­æ–·")
                    # å–æ¶ˆæ‰€æœ‰å¾…è™•ç†çš„ä»»å‹™
                    for f in future_to_task:
                        f.cancel()
                    break
                    
                controller_name, run_id = future_to_task[future]
                try:
                    result = future.result()
                    if result:
                        controller_results_map[controller_name].append(result)
                        all_results.append(result)
                        self.logger.info(f"å®Œæˆ: {controller_name} (é‹è¡Œ {run_id + 1}/{self.num_runs})")
                except Exception as exc:
                    self.logger.error(f"è©•ä¼°å¤±æ•— {controller_name} (é‹è¡Œ {run_id + 1}): {exc}")
            
            # è¨ˆç®—å¹³å‡æ€§èƒ½
            for controller_name, controller_results in controller_results_map.items():
                if controller_results:
                    avg_result = self.calculate_average_performance(controller_results)
                    self.results[controller_name] = {
                        'individual_runs': controller_results,
                        'average_performance': avg_result,
                        'config': controllers_to_evaluate[controller_name]
                    }
    
    def save_results(self, all_results):
        """ä¿å­˜è©•ä¼°çµæœ"""
        timestamp = datetime.now().strftime("%Y-%m-%d_%H%M%S")
        
        # ä¿å­˜è©³ç´°çµæœ
        results_file = self.output_dir / f"evaluation_results_{timestamp}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump({
                'evaluation_config': {
                    'evaluation_ticks': self.evaluation_ticks,
                    'num_runs': self.num_runs,
                    'timestamp': timestamp
                },
                'results': self.results
            }, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜CSVæ ¼å¼
        df = pd.DataFrame(all_results)
        csv_file = self.output_dir / f"evaluation_results_{timestamp}.csv"
        df.to_csv(csv_file, index=False, encoding='utf-8')
        
        # ä¿å­˜è©•ä¼°æ‘˜è¦ï¼ˆä¾›è©•ä¼°ç®¡ç†å™¨ä½¿ç”¨ï¼‰
        summary_file = self.output_dir / "evaluation_summary.json"
        summary_data = {
            'timestamp': timestamp,
            'evaluation_ticks': self.evaluation_ticks,
            'num_runs': self.num_runs,
            'controllers_evaluated': list(self.results.keys()),
            'best_completion_rate': None,
            'best_controller': None
        }
        
        if self.results:
            best_controller = max(
                self.results.items(),
                key=lambda x: x[1]['average_performance']['completion_rate']
            )
            summary_data['best_controller'] = best_controller[0]
            summary_data['best_completion_rate'] = best_controller[1]['average_performance']['completion_rate']
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"çµæœå·²ä¿å­˜: {results_file}, {csv_file}")
    
    def generate_comparison_charts(self):
        """ç”Ÿæˆæ¯”è¼ƒåœ–è¡¨"""
        if not self.results:
            return
            
        # æº–å‚™æ•¸æ“š
        controllers = []
        completion_rates = []
        avg_wait_times = []
        energy_per_orders = []
        robot_utilizations = []
        
        for name, data in self.results.items():
            avg_perf = data['average_performance']
            controllers.append(name)
            completion_rates.append(avg_perf['completion_rate'] * 100)
            avg_wait_times.append(avg_perf['avg_wait_time'])
            energy_per_orders.append(avg_perf['energy_per_order'])
            robot_utilizations.append(avg_perf['robot_utilization'] * 100)
        
        # ç²å–åœ–è¡¨æ¨™ç±¤
        labels = get_chart_labels(chinese_support)
        
        # å‰µå»ºæ¯”è¼ƒåœ–è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle(labels['title'], fontsize=16, fontweight='bold')
        
        # è¨­ç½®é¡è‰²
        colors = plt.cm.Set3(np.linspace(0, 1, len(controllers)))
        
        # å®Œæˆç‡æ¯”è¼ƒ
        bars1 = axes[0,0].bar(controllers, completion_rates, color=colors)
        axes[0,0].set_title(labels['completion_rate'], fontweight='bold')
        axes[0,0].set_ylabel(labels['completion_rate_y'])
        axes[0,0].tick_params(axis='x', rotation=45)
        
        # æ·»åŠ æ•¸å€¼æ¨™ç±¤
        for bar, rate in zip(bars1, completion_rates):
            height = bar.get_height()
            axes[0,0].text(bar.get_x() + bar.get_width()/2., height + 0.5,
                          f'{rate:.1f}%', ha='center', va='bottom', fontweight='bold')
        
        # å¹³å‡ç­‰å¾…æ™‚é–“æ¯”è¼ƒ
        bars2 = axes[0,1].bar(controllers, avg_wait_times, color=colors)
        axes[0,1].set_title(labels['wait_time'], fontweight='bold')
        axes[0,1].set_ylabel(labels['wait_time_y'])
        axes[0,1].tick_params(axis='x', rotation=45)
        
        for bar, wait in zip(bars2, avg_wait_times):
            height = bar.get_height()
            axes[0,1].text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                          f'{wait:.1f}', ha='center', va='bottom', fontweight='bold')
        
        # èƒ½æºæ•ˆç‡æ¯”è¼ƒ
        bars3 = axes[1,0].bar(controllers, energy_per_orders, color=colors)
        axes[1,0].set_title(labels['energy'], fontweight='bold')
        axes[1,0].set_ylabel(labels['energy_y'])
        axes[1,0].tick_params(axis='x', rotation=45)
        
        for bar, energy in zip(bars3, energy_per_orders):
            height = bar.get_height()
            if height > 0:
                axes[1,0].text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                              f'{energy:.0f}', ha='center', va='bottom', fontweight='bold')
        
        # æ©Ÿå™¨äººåˆ©ç”¨ç‡æ¯”è¼ƒ
        bars4 = axes[1,1].bar(controllers, robot_utilizations, color=colors)
        axes[1,1].set_title(labels['utilization'], fontweight='bold')
        axes[1,1].set_ylabel(labels['utilization_y'])
        axes[1,1].tick_params(axis='x', rotation=45)
        
        for bar, util in zip(bars4, robot_utilizations):
            height = bar.get_height()
            axes[1,1].text(bar.get_x() + bar.get_width()/2., height + 1,
                          f'{util:.1f}%', ha='center', va='bottom', fontweight='bold')
        
        plt.tight_layout()
        
        # ä¿å­˜åœ–è¡¨
        timestamp = datetime.now().strftime("%Y-%m-%d_%H%M%S")
        chart_file = self.output_dir / f"evaluation_comparison_{timestamp}.png"
        plt.savefig(chart_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        self.logger.info(f"æ¯”è¼ƒåœ–è¡¨å·²ä¿å­˜: {chart_file}")
    
    def generate_evaluation_report(self):
        """ç”Ÿæˆè©•ä¼°å ±å‘Š"""
        timestamp = datetime.now().strftime("%Y-%m-%d_%H%M%S")
        report_file = self.output_dir / f"evaluation_report_{timestamp}.txt"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("="*60 + "\n")
            f.write("RMFS æ§åˆ¶å™¨æ€§èƒ½è©•ä¼°å ±å‘Š\n")
            f.write("="*60 + "\n")
            f.write(f"è©•ä¼°æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"è©•ä¼°æ™‚é•·: {self.evaluation_ticks} ticks\n")
            f.write(f"é‡è¤‡é‹è¡Œ: {self.num_runs} æ¬¡\n\n")
            
            # æ€§èƒ½æ’å
            if self.results:
                # æŒ‰å®Œæˆç‡æ’åº
                sorted_by_completion = sorted(
                    self.results.items(),
                    key=lambda x: x[1]['average_performance']['completion_rate'],
                    reverse=True
                )
                
                f.write("1. è¨‚å–®å®Œæˆç‡æ’å\n")
                f.write("-" * 30 + "\n")
                for i, (name, data) in enumerate(sorted_by_completion, 1):
                    avg_perf = data['average_performance']
                    f.write(f"{i}. {name}: {avg_perf['completion_rate']*100:.1f}% "
                           f"(Â±{avg_perf['completion_rate_std']*100:.1f}%)\n")
                
                # æŒ‰ç­‰å¾…æ™‚é–“æ’åºï¼ˆè¶Šä½è¶Šå¥½ï¼‰
                sorted_by_wait = sorted(
                    self.results.items(),
                    key=lambda x: x[1]['average_performance']['avg_wait_time']
                )
                
                f.write("\n2. å¹³å‡ç­‰å¾…æ™‚é–“æ’å (è¶Šä½è¶Šå¥½)\n")
                f.write("-" * 30 + "\n")
                for i, (name, data) in enumerate(sorted_by_wait, 1):
                    avg_perf = data['average_performance']
                    f.write(f"{i}. {name}: {avg_perf['avg_wait_time']:.1f} ticks "
                           f"(Â±{avg_perf['avg_wait_time_std']:.1f})\n")
                
                # è©³ç´°æ€§èƒ½åˆ†æ
                f.write("\n3. è©³ç´°æ€§èƒ½åˆ†æ\n")
                f.write("-" * 30 + "\n")
                
                for name, data in self.results.items():
                    avg_perf = data['average_performance']
                    f.write(f"\n{name}:\n")
                    f.write(f"  å®Œæˆç‡: {avg_perf['completion_rate']*100:.1f}% (Â±{avg_perf['completion_rate_std']*100:.1f}%)\n")
                    f.write(f"  å¹³å‡ç­‰å¾…æ™‚é–“: {avg_perf['avg_wait_time']:.1f} ticks (Â±{avg_perf['avg_wait_time_std']:.1f})\n")
                    f.write(f"  æ©Ÿå™¨äººåˆ©ç”¨ç‡: {avg_perf['robot_utilization']*100:.1f}% (Â±{avg_perf['robot_utilization_std']*100:.1f}%)\n")
                    f.write(f"  å¹³å‡åŸ·è¡Œæ™‚é–“: {avg_perf['execution_time']:.1f} ç§’ (Â±{avg_perf['execution_time_std']:.1f})\n")
                    
                    if avg_perf['energy_per_order'] > 0:
                        f.write(f"  æ¯è¨‚å–®èƒ½æºæ¶ˆè€—: {avg_perf['energy_per_order']:.0f} (Â±{avg_perf['energy_per_order_std']:.0f})\n")
                
                # çµè«–å’Œå»ºè­°
                f.write("\n4. çµè«–å’Œå»ºè­°\n")
                f.write("-" * 30 + "\n")
                
                best_controller = sorted_by_completion[0]
                f.write(f"â€¢ æœ€ä½³æ•´é«”æ€§èƒ½: {best_controller[0]} "
                       f"(å®Œæˆç‡ {best_controller[1]['average_performance']['completion_rate']*100:.1f}%)\n")
                
                fastest_controller = sorted_by_wait[0]
                f.write(f"â€¢ æœ€ä½ç­‰å¾…æ™‚é–“: {fastest_controller[0]} "
                       f"(å¹³å‡ç­‰å¾… {fastest_controller[1]['average_performance']['avg_wait_time']:.1f} ticks)\n")
                
                f.write("\nå»ºè­°:\n")
                f.write("â€¢ å¦‚æœè¿½æ±‚æœ€é«˜å®Œæˆç‡ï¼Œå»ºè­°ä½¿ç”¨ " + best_controller[0] + "\n")
                f.write("â€¢ å¦‚æœè¿½æ±‚æœ€ä½å»¶é²ï¼Œå»ºè­°ä½¿ç”¨ " + fastest_controller[0] + "\n")
                f.write("â€¢ è€ƒæ…®çµåˆå¤šç¨®æ§åˆ¶ç­–ç•¥çš„æ··åˆæ–¹æ³•\n")
        
        self.logger.info(f"è©•ä¼°å ±å‘Šå·²ä¿å­˜: {report_file}")
        
        # ä¿å­˜ç°¡åŒ–ç‰ˆæœ¬çš„æ‘˜è¦ï¼ˆä¾›è©•ä¼°ç®¡ç†å™¨é¡¯ç¤ºï¼‰
        summary_txt = self.output_dir / "evaluation_summary.txt"
        with open(summary_txt, 'w', encoding='utf-8') as f:
            f.write(f"è©•ä¼°æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"è©•ä¼°æ™‚é•·: {self.evaluation_ticks} ticks\n")
            f.write(f"é‡è¤‡æ¬¡æ•¸: {self.num_runs} æ¬¡\n")
            f.write(f"è©•ä¼°æ§åˆ¶å™¨æ•¸: {len(self.results)} å€‹\n\n")
            
            if self.results and sorted_by_completion:
                f.write("æœ€ä½³æ§åˆ¶å™¨æ’å:\n")
                for i, (name, data) in enumerate(sorted_by_completion[:3], 1):
                    avg_perf = data['average_performance']
                    f.write(f"{i}. {name}: å®Œæˆç‡ {avg_perf['completion_rate']*100:.1f}%\n")

def main():
    parser = argparse.ArgumentParser(description="RMFSæ§åˆ¶å™¨æ€§èƒ½è©•ä¼°")
    parser.add_argument('--eval_ticks', type=int, default=5000,
                       help='è©•ä¼°æ™‚é•· (ticks)')
    parser.add_argument('--num_runs', type=int, default=3,
                       help='æ¯å€‹æ§åˆ¶å™¨çš„é‡è¤‡é‹è¡Œæ¬¡æ•¸')
    parser.add_argument('--output_dir', default='evaluation_results',
                       help='çµæœè¼¸å‡ºç›®éŒ„')
    parser.add_argument('--controllers', nargs='+', 
                       help='æŒ‡å®šè¦è©•ä¼°çš„æ§åˆ¶å™¨åˆ—è¡¨ (ä¾‹å¦‚: time_based queue_based dqn:path/to/model.pth)')
    parser.add_argument('--parallel', action='store_true',
                       help='å•Ÿç”¨ä½µè¡Œè©•ä¼°æ¨¡å¼')
    
    args = parser.parse_args()
    
    evaluator = ControllerEvaluator(
        evaluation_ticks=args.eval_ticks,
        num_runs=args.num_runs,
        output_dir=args.output_dir
    )
    
    results = evaluator.run_evaluation(
        controller_specs=args.controllers,
        parallel=args.parallel
    )
    
    print("\n" + "="*50)
    print("ğŸ“Š è©•ä¼°å®Œæˆï¼")
    print(f"çµæœä¿å­˜åœ¨: {evaluator.output_dir}")
    print("="*50)
    
    return results

if __name__ == "__main__":
    main()