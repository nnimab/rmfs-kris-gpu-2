# 3.6.2 DRL 模型超參數設定

為確保本研究中 DRL 實驗的可複現性與結果的有效性，本節詳細列出在訓練 `DQN` 與 `NERL` 控制器時所使用的關鍵超參數。這些參數的設定基於初步的收斂性與穩定性實驗，並在正式訓練期間保持固定。

### 1. DQN 特有超參數

下表為 `DQN` 控制器（實驗組 3、4）在訓練過程中使用的主要超參數。

| 參數名稱 | 程式碼變數 | 值 | 說明 |
| :--- | :--- | :--- | :--- |
| 學習率 | `learning_rate` | 5e-4 | Adam 優化器的學習率。 |
| 折扣因子 (Gamma) | `gamma` | 0.99 | 未來獎勵的折扣係數，值越接近 1 代表越重視長期回報。 |
| 初始探索率 (Epsilon) | `epsilon` | 1.0 | 訓練初期完全隨機選擇動作的機率。 |
| 最小探索率 | `epsilon_min` | 0.01 | 探索率衰減的下限值。 |
| 探索率衰減率 | `epsilon_decay` | 0.9995 | 每個訓練步驟後，探索率乘以的指數衰減係數。 |
| 經驗回放記憶體大小 | `memory_size` | 100,000 | 存儲 $(s, a, r, s')$ 轉換樣本的最大數量。 |
| 批次大小 | `batch_size` | 1,024 | 每次從記憶體中抽樣進行網路更新的樣本數量。 |
| 目標網路更新頻率 | `target_update_freq` | 1,000 | 每隔多少個訓練**步驟**將策略網路的權重複製到目標網路。 |

### 2. NERL 特有超參數

下表為 `NERL` 控制器（實驗組 5-12）在演化過程中使用的主要超參數。其中，變異率與變異強度根據 **3.6.1 節** 中定義的**探索型 (A)** 與**利用型 (B)** 變體而有所不同。

| 參數名稱 | 程式碼變數 | 變體 A (探索型) | 變體 B (利用型) | 說明 |
| :--- | :--- | :--- | :--- | :--- |
| 族群大小 | `population_size` | 20 | 20 | 每一代中包含的個體（神經網路）數量。 |
| 精英保留比例 | `elite_ratio` | 0.1 | 0.1 | 每一代中適應度最高的個體被直接保留的比例。 |
| 錦標賽選擇大小 | `tournament_size` | 3 | 3 | 在錦標賽選擇中，每次隨機比較的個體數量。 |
| 變異率 | `mutation_rate` | **0.2** | **0.1** | 個體基因（網路權重）發生變異的基礎機率。 |
| 變異強度 | `mutation_strength` | **0.1** | **0.05** | 高斯變異的標準差，控制變異的幅度大小。 |
| 評估時長 | `eval_ticks` | 3000 / 8000 | 3000 / 8000 | 每個個體進行評估的持續時間 (ticks)。 |

這些超參數共同定義了兩種 DRL 方法的學習行為，是後續實驗分析與結果比較的重要基礎。 