# 3.4.4 獎勵函數設計

獎勵函數是強化學習中引導智能體行為的核心機制，它將系統的期望目標轉化為一個可觀測、可量化的純量回饋信號。本研究為探索不同時間尺度下的學習效果，設計了兩種截然不同的獎勵模式：「步階獎勵」與「全域獎勵」。

### 1. 步階獎勵 (Step Reward)

步階獎勵模式旨在為智能體提供一個密集的、即時的局部回饋信號。在每個決策間隔（$T_{\text{interval}} = 10$ ticks）結束時，系統會獨立評估每個交叉路口的局部性能，並計算一個綜合獎勵值。這種高頻率的回饋有助於智能體快速學習到基礎的交通控制啟發式規則，例如優先處理高優先級任務、減少等待車輛等。

對於單一交叉路口 $i$，其在一個決策間隔內的步階獎勵 $R_{\text{step}}(i)$ 由以下幾個加權分量構成：

$$
R_{\text{step}}(i) = (R_{\text{flow}} - C_{\text{wait}} - C_{\text{switch}}) \times w_{\text{critical}}(i)
$$

其中各分量的定義如下：

-   **流量獎勵 ($R_{\text{flow}}$)**: 根據成功通過路口的機器人數量及其任務優先級給予的正向獎勵。
    $$
    R_{\text{flow}} = \sum_{r \in P_i} w_{\text{pass}}(p(r))
    $$
    此處，$P_i$ 是該決策間隔內通過路口 $i$ 的機器人集合，$p(r)$ 是機器人 $r$ 的任務優先級（高、中、低），$w_{\text{pass}}$ 是對應優先級的獎勵權重。

-   **等待成本 ($C_{\text{wait}}$)**: 對於仍在路口佇列中等待的機器人，根據其累計等待時間和任務優先級施加的懲罰。
    $$
    C_{\text{wait}} = \sum_{r \in Q_i} w_{\text{wait}}(p(r)) \cdot t_{\text{wait}}(r)
    $$
    此處，$Q_i$ 是仍在路口 $i$ 等待的機器人集合，$t_{\text{wait}}(r)$ 是機器人 $r$ 的等待時間（單位：ticks），$w_{\text{wait}}$ 是對應優先級的懲罰權重。

-   **相位切換成本 ($C_{\text{switch}}$)**: 對每一次號誌相位的切換施加一個固定的懲罰，以避免過於頻繁、無效的切換，鼓勵控制器維持交通流的連續性。
    $$
    C_{\text{switch}} = w_{\text{switch}} \cdot \mathbb{I}(\text{switched})
    $$
    其中，$\mathbb{I}(\text{switched})$ 是指示函數，若發生相位切換則為1，否則為0。$w_{\text{switch}}$ 為固定的懲罰權重。

-   **關鍵路口加權 ($w_{\text{critical}}(i)$)**: 為使智能體優先學習管理靠近揀貨站等瓶頸區域的交通，對於被標記為「關鍵」的交叉路口 $i$，其獎勵值會被乘以一個大於1的放大係數。

### 2. 全域獎勵 (Global Reward)

全域獎勵模式提供一個稀疏的、延遲的回饋信號，旨在引導智能體學習對系統長期、宏觀目標有益的複雜策略。在該模式下，智能體在整個評估回合（$T_{\text{episode}}$）中不接收任何即時回饋，僅在回合結束時，根據系統的最終整體表現計算一個單一的獎勵值。

為避免不同量綱指標間的直接加減導致獎勵信號被單一指標主導，本研究設計了一個基於效率比率的全域獎勵函數。該函數將系統的「產出」作為分子，將系統的「成本」作為分母，其公式定義如下：

$$
R_{\text{global}} = \frac{N_{\text{completed}} \cdot w_{\text{completion}}}{\frac{E_{\text{total}}}{S_{\text{energy}}} + T_{\text{episode}} \cdot w_{\text{time}} + P_{\text{spillback}} + \epsilon}
$$

其中各符號定義如下：

-   $N_{\text{completed}}$: 在評估回合中完成的訂單總數（單位：個）。
-   $w_{\text{completion}}$: 訂單完成獎勵權重。
-   $E_{\text{total}}$: 系統在回合中的總能量消耗（單位：EU），其詳細定義見 3.2.5 節。
-   $S_{\text{energy}}$: 能量縮放因子，用以將能量成本與時間成本調整至可比較的量級。
-   $T_{\text{episode}}$: 評估回合的總時長（單位：ticks）。
-   $w_{\text{time}}$: 每 tick 的時間懲罰權重。
-   $P_{\text{spillback}}$: 若揀貨站發生嚴重回堵，則施加此項巨大懲罰。
-   $\epsilon$: 一個極小的正常數（例如 $10^{-6}$），用以避免分母為零。

這種比率形式的設計，鼓勵智能體在追求高訂單完成量的同時，必須兼顧能源和時間的效率，從而學習到一個更為均衡和可持續的運作策略。