### **修改後的論文內容**

**CHAPTER 1**
**INTRODUCTION**

**1.1 Background**

The development of Robotic Mobile Fulfillment Systems (RMFS), as demonstrated by industry giants such as Amazon, Swisslog, and Interlink, has transformed the warehouse business **(Wurman, 2008; Merschformann, 2019)**. These systems have improved rapidly, both horizontally, by integrating across many activities and processes, and vertically, by improving the capabilities and intelligence of individual robots. To illustrate, Figure 1.1 depicts several examples of different Autonomous Mobile Robots (AMRs) in the distribution centers **(Chauhan, 2022)**. In detail, from left to right are the robot MiR100 at Honeywell Safety & Productivity Solutions, then the CarryPick from Swisslog and the AutoStore RMFS system **(Chauhan, 2022)**. Instead, the focus of this work is based on the RMFS technology exemplified by the robots in Images 1 and 2. Nonetheless, AutoStore serves as a valuable reference to demonstrate the variety and innovation present in RMFS solutions within the industry. Another prominent example was when Amazon acquired Kiva Systems for 775 million US dollars **(Guizzo, 2012)**.

Figure 1.1 RMFS examples **(Chauhan, 2022)**

As the whole warehousing sector moves ahead, it calls for more than just infrastructure expansion. With the huge number of autonomous robots traveling within the warehouse area, pathfinding and traffic flow are the next focus area for development and advancement. Consequently, the challenge of coordinating numerous autonomous robots in dynamic environments has garnered considerable attention from researchers and industry stakeholders alike **(Benavides-Robles, 2024)**. Specifically, since 2017, interest in this area has grown considerably **(Benavides-Robles, 2024)**. Figure 1.2 illustrate the number of publications related to RMFS **(Benavides-Robles, 2024)**. Addressing these concerns is critical for a variety of reasons. Effective traffic control and pathfinding tactics can significantly improve operational performance by reducing delays and avoiding deadlocks. Warehouses can improve the flow of robots to enable shorter lead time, help meet customers’ expectation, resulting in higher customer satisfaction.

Figure 1.2 Number of publications related to RMFS **(Benavides-Robles, 2024)**

This work will focus on the issue of intersection traffic control for many autonomous mobile robots, with an emphasis on energy consumption in dynamic warehouse environments. The goal is to create creative traffic control systems that not only manage the flow of robots efficiently, but also reduce energy consumption. By attaining these objectives, this study hopes to improve the entire operational performance of RMFS, ensuring that the increasing complexity and demands of modern warehousing are addressed with efficient, sustainable, and cost-effective solutions.

**1.2 Objectives**

This research’s objectives can be described as follows:
*   Implement of Deep Q-Learning (DQL) to optimize intersection traffic control for autonomous mobile robots in RMFS.
*   Reduce energy consumption and improve overall operational efficiency.

**1.3 Scope and Limitations**

Taking into account all the previously mentioned circumstances, the following limitations of the research must be acknowledged:
*   The study will confine to one intersection line closest to the picking station
*   The simulations are conducted with the same order sequences. As long as the order throughput remains consistent within a similar timeframe and the waiting times are acceptable, the primary focus will be on optimizing energy consumption.

**1.4 Organization of Thesis**

The structure of this study includes five distinct chapters, each considering a specific aspect of the research. The structure of the study is as follows:
*   Chapter 1: Introduction. This chapter begins by explaining why this research is important and what prompted the interest in studying it. It establishes explicit objectives for the research and provides a glimpse of how the study will be organized.
*   Chapter 2: Literature Review. This chapter provides a complete assessment of the available literature on the research issue. The literature review provides a detailed summary of the status of the discipline, emphasizing key discoveries and contributions from prior studies.
*   Chapter 3: Methodology. This chapter elaborates on the research methods used in this study. It includes a full overview of the study's design, and analytical procedures. Furthermore, it provides a clear and simple description of the processes required to carry out the research, guaranteeing the study's conclusions are reliable and valid.
*   Chapter 4: Results and Discussion. This chapter examines the acquired data and reveals the study's findings. Additionally, it discusses these findings, taking into account both the observable outcomes and the statistical analysis.
*   Chapter 5: Conclusion and Future Work. This chapter summarizes the study's primary findings and examines how they may affect future work. Furthermore, it makes recommendations for future study projects.

---

**CHAPTER 2**
**LITERATURE REVIEW**

**2.1 Robotic Mobile Fulfillment Systems (RMFS)**

**2.1.1 RMFS Decision Problems**

Figure 2.1 A robot carrying a pod **(Wurman, 2008)**

Robotic Mobile Fulfillment Systems (RMFS), a successful multi-robot warehouse system employing autonomous vehicles for efficient inventory movement. Figure 2.1 given above display an example of a robot carrying a pod, which is typical and commonly seen in distribution centers **(Wurman, 2008)**. This section will consider various decision-making issues commonly experienced within a Robotic Mobile Fulfillment System (RMFS). These systems often represent operational demands as pick or replenishment orders (as shown in Figure 2.2). When these requests are received, pallets are methodically separated into smaller units, each with a specific Stock Keeping Unit (SKU). Individual units are carefully positioned on a pod to fulfill a replenishment order.

Recognizing the interconnected nature of these operational decision problems, it is crucial to cultivate a thorough comprehension of the current research in these fields. Numerous scholarly articles typically focus on addressing existing challenges or enhancing efficiency in one of these areas.

Figure 2.2 The internal storage/retrieval process in RMFSs **(Merschformann, 2019)**

According to Merschformann and partners, the RMFS operational decision problems can be classified into 4 main areas, which are listed as follows:
(1) Order Assignment (OA): OA decisions are classified into two types: assigning pick orders to pick stations (Pick Order Assignment or POA) and assigning replenishment orders to replenishment stations (Replenishment Order Assignment, or ROA).
(2) Task Creation (TC): Task Creation consists of two subproblems. The first is Pod Selection (PS), which identifies the selected pods for their storage trips. This subproblem includes Pick Pod Selection (PPS), which emphasizes the due times, and Replenishment Pod Selection (RPS), which employs an alternative approach. The second TC subproblem is Pod Storage Assignment (PSA), which determines the storage location for the pod’s retrieval trip.
(3) Task Allocation (TA): This process assigns jobs to robots. Hence, a trip will be formed while a series of tasks are arranged for the robots.
(4) Path Planning (PP): Having sequential tasks inherently defines routes and provides input for Path Planning algorithms, which construct paths for robots to follow.

**2.1.2 Previous research**

First of all, item and pod allocation strategies are of significant emphasis. In their paper published in 2020, Kim and colleagues created a technique for efficiently assigning things to inventory pods **(Kim, 2020)**. Specifically, their article ensures frequently ordered products are placed in the same pod by presenting an efficient heuristic approach for assigning items to pods in an RMFS **(Kim, 2020)**. A data-driven approach for zone clustering and storage site assignment was proposed by Keung and partners in 2021, which met their purpose of improving operational efficiency in the RMFS **(Keung, 2021)**. Another method emerged in 2020 when Li and colleagues developed a high-density storage arrangement to increase space usage **(Li, 2020)**. In detail, they utilize a cluster approach to temporal association analysis with the aim of identifying highly correlated items for storage in the same rack. Additionally, a new turnover-rate-based decentralized storage policy (TRBDSP) is presented to eliminate AMR blockage. Ultimately, the findings suggest that the novel method can significantly enhance order-picking efficiency while lowering AMR energy consumption **(Li, 2020)**. Related to this, Cechinel et al. propose an island model algorithm for multi-robot systems that takes into account journey time, energy consumption, robot payload, and order deadlines **(Cechinel, 2021)**. Consequently, their results reveal that the method produces a greater number of complete solutions, simultaneously meeting the deadlines and demonstrating energy consumption reductions **(Cechinel, 2021)**.

Several approaches for order-picking have been proposed. Meller and Pazour devised a heuristic for SKU allocation in an A-frame automated system to improve order fulfillment efficiency in pharmaceutical distribution centers, which has been considered one of the early studies **(Meller, 2008)**. Furthermore, the split order concept was introduced as a new method to serve the purpose of enhancing overall efficiency **(Xie, 2021)**. Xie and colleagues further explained that this approach allows portions of an order to be selected at various stations **(Xie, 2021)**. Another approach was employed by Yuan and partners, in which a mathematical model was implemented with a two-stage hybrid algorithm technique to investigate pod correlation and pod placements in the storage arrangement **(Yuan, 2019)**. Experimental results of their research later suggest that the proposed pod assignment model and algorithm optimization approach can improve order-picking efficiency **(Yuan, 2019)**.

By merging order and robot scheduling to improve picking efficiency, Allgor and colleagues particularly focus on the approach utilized by the Amazon Fulfillment Centers, which was the four-phase decomposition heuristic **(Allgor, 2023)**. Most recently, in 2024, SASORL algorithm was introduced, and it is a reinforcement learning-based solution for optimizing order allocation and sequencing that demonstrated considerable reductions in travel distance and processing time **(Perumaal Subramanian, 2024)**.

Regarding path planning decision problems, also known as pathfinding problems, are path arrangement problems in which a group of robots must find collision-free paths with different starting and ending points **(Stern, 2019)**. When addressing this problem, critical elements such as acceleration, deceleration, and turning time must all be considered. If there is no longer a collision-free path, the robot must wait for one to become available. Henceforth, traffic control is inextricably tied to path planning problems since traffic control solutions can help minimize the delay time for the robots. Since 1968, Hart, Nilsson, and Raphael invented the A* approach, and its versions are widely employed to solve the pathfinding problem **(Hart, 1968)**. Several scholars have developed and constructed several extensions of A* to address its shortcomings and improve overall efficiency **(Standley, 2010; Goldenberg, 2014; Wagner, 2015)**.

Later in 2013, Sharon and colleagues proposed the Increasing Cost Tree Search (ICTS) to find optimal solutions using a two-level search method **(Sharon, 2013)**. Finally, the Conflict-Based Search (CBS) algorithm is another commonly employed search-based algorithm **(Sharon, 2015)**. The mechanism of CBS approach is to divide a multi-agent pathfinding problem into multiple limited single-agent pathfinding challenges **(Sharon, 2015)**. More recently in 2019, Li and colleagues proposed two new viable heuristics that take into account the pairwise relationships between agents and empirically show improvement in success rates and run-time **(Li, 2019)**. In addition, there are papers implementing reinforcement learning with decentralized execution to solve this problem **(Ma, 2021; Chen, 2017; Long, 2018; Sartoretti, 2019; Liu, 2020)**. These papers will be discussed further in the following section, which will focus on the application of RL in RMFS research.

**2.1.3 Traffic Control**

As mentioned earlier, traffic control is intricately linked to the path planning stage in RMFS as a traffic control solution can reduce the deadlocks and delay time and subsequently enhance the system's overall efficiency. In 1988, traffic management regulations were established to navigate AMRs on a 2D grid while their motions were coordinated and linked by a control center **(Grossman, 1988)**. This policy is nearly optimal even for large numbers of AMRs and has since set a solid foundation for general RMFS traffic control and management.

Utilizing a decentralized mechanism, Bourbakis developed a formal modeling of a generic traffic priority language in 1997 **(Bourbakis, 1997)**. This traffic control solution does not require a central control to coordinate; the robots can move freely and are referred to as autonomous. For this reason, Bourbakis's approach is considered helpful in navigating dynamic contexts. Similarly, Wang and Premvuti have employed distributed robotic systems (DRS) in their paper to set up traffic regulation **(Wang, 1994)**. Their system lets the AMRs dynamically create their own unique targeted route, which is unknown to other robots, yet the clock, memory, and ground support are shared and synchronized **(Wang, 1994)**.

Delays typically occur at intersections while all the robots are moving. Minimizing the delay time at intersections is the shared target of traffic control in urban and warehouse contexts. Firstly, in the urban traffic context, Dunne and Potts, as the pioneers in this discipline, used rule-based solutions **(Dunne, 1964)**. In their paper, the intersectional traffic signal will respond to a control algorithm based on the number of vehicles requiring service. The proposed linear control algorithm has proven stable in undersaturated situations **(Dunne, 1964)**. With the same target of minimizing the total delay, a computer control scheme was introduced by Ross and colleagues **(Ross, 1971)**. Their aim differs from previously mentioned research as it concentrates on addressing the residual queues at a specific critical intersection **(Ross, 1971)**. In particular, reinforcement learning has greatly benefited real-time traffic signal control systems **(Jin, 2015; Jin, 2017)**. These papers will be discussed later in the following section.
在所有機器人移動時，延遲通常發生於交叉路口。無論是在城市交通還是倉儲環境中，減少交叉口的延遲時間都是交通控制的共同目標。首先，在城市交通領域，Dunne 和 Potts 作為該領域的先驅，採用了基於規則的解決方案 **(Dunne, 1964)**。在他們的論文中，交叉口的交通號誌會根據需要服務的車輛數量，響應一套控制演算法。所提出的線性控制演算法在未飽和情況下已被證明具有穩定性 **(Dunne, 1964)**。同樣以最小化總延遲為目標，Ross 等人提出了一種電腦控制方案 **(Ross, 1971)**。他們的研究重點與前述不同，主要聚焦於解決特定關鍵交叉口的殘留排隊問題 **(Ross, 1971)**。特別值得一提的是，強化學習技術對於即時交通號誌控制系統帶來了極大助益 **(Jin, 2015; Jin, 2017)**。這些相關文獻將於後續章節進一步討論。

Since there are certain similarities between intersections in warehousing and urban contexts, we can refer to and learn from their approach to solving intersectional problems. Nonetheless, traffic flow between these two environments is different by nature, so the possible issues are also distinctive. Within the RMFS, intersections pose a significant barrier to maximizing the system's overall efficiency and energy usage. One of the outstanding and relatable research was in 2009, written by Teja and colleagues **(Teja, 2009)**. Their research assigns an intersection agent to govern intersectional traffic flow by providing priority to robotic agents entering the intersection **(Teja, 2009)**.

To conclude, understanding different approaches to solving intersectional delay time is valuable, and therefore, the fundamental purpose of this research is to control the permissible directions at crossings by using virtual traffic lights and developing and training a learning model to manage intersection traffic dynamically.

**2.2 Deep Q Network (Reinforcement Learning)**

**2.2.1 Reinforcement Learning**

This section examines and discusses the use of Reinforcement Learning (RL) in this study. Sutton and Barto characterize RL as an agent learning optimal behavior by interacting with its environment and receiving feedback as reward signals for each action **(Sutton, 1999; Sutton, 2018)**. Based on this feedback, the agent then refines its control policy, which is appropriate for adjusting traffic signals to dynamic traffic scenarios. Several advantages highlight the efficacy of an RL method. RL overcomes this issue by allowing traffic lights to learn and alter their timings autonomously based on real-time observations, which is critical for ensuring adequate traffic flow **(Zheng, 2019; Wang, 2021; Vidali, 2019; Liu, 2023; Jiang, 2022; Chen, 2020)**. In general, this adapt-and-learn strategy has the potential to minimize congestion while also improving travel times and providing environmental advantages.

Similar implementations have been applied to RMFS traffic control over the years. Implementing RL within RMFS has received much attention because of its effectiveness in controlling the dynamic and unpredictable nature of path planning and traffic circumstances **(Ma, 2021; Chen, 2017; Long, 2018; Sartoretti, 2019; Liu, 2020; Jin, 2015; Jin, 2017; Jiang, 2022; Wiering, 2000)**. Particularly in traffic control, Jin and Ma let each traffic signal learn and adapt to traffic conditions and make optimal timing decisions based on perceived system states **(Jin, 2017)**. The intelligent control method uses reinforcement learning with multiple-step backups to update each traffic signal’s knowledge online **(Jin, 2017)**.

Likewise, some articles use reinforcement learning to tackle path-planning difficulties. First, Chen and colleagues created a unique two-agent collision avoidance system in 2017 that employed deep reinforcement learning, effectively shifting the computational load from online execution to offline training **(Chen, 2017)**. They also proposed a systematic approach for cases with more than two actors and an extended formulation that considers kinematic limitations **(Chen, 2017)**. Sartoretti and colleagues presented a novel solution to multi-agent pathfinding problems in 2019 by combining distributed reinforcement learning with imitation learning from a centralized expert planner **(Sartoretti, 2019)**. Another work published by Long and colleagues contributed by presenting a multi-scenario, multi-stage deep reinforcement learning system that uses the policy gradient approach to establish the optimal collision avoidance strategy **(Long, 2018)**.

**2.2.2 Deep Q-Learning**

Q-learning determines the value of performing a specific action in a given condition, increasing the overall reward over time. It is one of the Reinforcement Learning approaches that does not need models. Watkin proposed the convergence theorem for Q-learning in 1989 **(Watkins, 1989)**, and later on presented it again and proved it in his publish with Dayan in 1992 **(Watkins, 1992)**. In their paper in 2019, a Q-Learning flow chart was depicted, in which Chang and colleagues demonstrated the agent obtaining states and rewards from the environment based on its actions **(Chang, 2019)**. It calculates rewards for activities in each state through a Q-table and is updated using a method that weighs immediate rewards against expected future rewards. Eventually, an optimal policy is learned, enabling it to take the best action possible in each state to attain its goals.

With this mechanism, Q-Learning is appropriate for a dynamic environment as it does not involve prior awareness of the environment. Particularly, RMFS layout has such a dynamic nature, which means it can easily be changed, even the batches sometimes different for all warehouse, so one single model for all warehouse is not a sufficient selection. Furthermore, Q-learning seeks to identify the optimal policy to maximize the predicted future reward while simultaneously obeying a different policy **(Clifton, 2020)**. Q-learning has been widely used in traffic signal control in both urban and robotic contexts due to its benefits and remarkable advantages **(Ma, 2021; Jin, 2015)**. In urban traffic control, Jin and Ma proposed an adaptive signal control system based on a Q-Learning algorithm and a group-based phasing technique **(Jin, 2015)**. Simulation results indicate that RL-based techniques outperform fixed-time signal regulation by a significant margin, independent of demand levels **(Jin, 2015)**.

Notably, deep Q-Learning is also adopted in EMFS traffic control, specifically in Ma and colleagues published in 2021 **(Ma, 2021)**. It is explained that instead of mapping each state-action pair to the corresponding value, deep Q-Learning utilizes a deep neural network to map its states and actions **(Ma, 2021)**. In detail, a single agent's perspective is employed to treat each agent separately and train the model, and subsequently, the final taught policy is applied to each agent during decentralized execution **(Ma, 2021)**. Ultimately, it is shown that there is a high success rate with a low average step in an empirical evaluation with a multiple-obstacle environment.

Figure 2.3 Deep Q-learning flow chart

Finally, this paper will employ Deep Q-Learning to manage the permissible intersectional directions, resulting in virtual traffic signals. Figure 2.3 illustrate the deep Q-learning flow chart which will be implemented in this paper. The primary goal is to reduce the stop-and-go behavior and the waiting time. In addition, minimizing overall energy consumption inside the RMFS will also be taken into consideration. Henceforth, this study will design and train a Deep Q-Learning model to dynamically manage intersection traffic while emphasizing energy savings.

---

