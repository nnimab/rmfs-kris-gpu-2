Neuroevolution reinforcement learning for multi-echelon inventory
optimization with delivery options and uncertain discount

Engineering Applications of Artificial Intelligence 

Engineering Applications of Artificial Intelligence 134 (2024) 10867

ABSTRACT  

The advanced information technology has enabled supply chain to make centralized optimal decision, allowing
to make a global optimal solution. However, dealing with uncertainty is important in inventory management.
Besides demand and supply uncertainties, supplier discounts also often arise unexpectedly. Further, suppliers or
third-parties typically offer various delivery options in which trade-off occurs between cost and lead time. Thus,
this study introduces new problem namely Multi-Echelon Inventory Optimization with Delivery Options and
Uncertain Discount (MEIO-DO-UD). As a solution, Neuroevolution Reinforcement Learning (NERL) framework is
developed for minimizing total system cost. The environment is modeled via System Dynamics (SD) and actor is
presented by integration of Artificial Neural Network and Evolutionary Algorithm (EA), creating an effective
decision-making model under dynamic uncertainty. The experimental study has been conducted where two
different supply chain networks are given namely serial and divergence. Three EA algorithms are compared
namely Differential Evolution (DE), Memetic Algorithm (MA), and Evolution Strategy (ES). Furthermore, NERL is
also compared with the EA-optimized classical continuous review model namely (s,Q). The result shows that
regardless what EA type is used, the proposed NERL always outperforms EA-optimized (s,Q) model. The more
complex the problem, the further improvement can be made i.e. cost reduction up to 58%, followed by the fill
rate improvement. The result also shows that NERL can avoid overfitting. Managerial implications are high
lighted where NERL provides the more stable inventory level among all supply chain partners and bull-whip
effect can be damped.

1. Introduction
Due to high uncertainty environment in supply chain, inventory is
hold to handle the fluctuation of demand and supply (Baruah et al.,
2016). While having more inventory can enhance service levels, it also
results in elevated costs particularly holding cost. Thus, effective in
ventory management is essential to strike the right balance between
service levels and inventory expenditures (Gallego-garcía et al., 2021). It
is worth noting that inventory typically represents a substantial portion,
ranging from 20% to 60%, of a manufacturing company’s total assets
(Anyibuofu, 2014). This indicates the importance of effective inventory
management.
Many methods are employed in inventory management for shaping
the profitability of the businesses. It was started with the well-known
mathematical model so-called Economic Order Quantity (EOQ) for
determining the minimum cost for lot sizing (Harris, 1990). The

development was then continued by managing inventory through in
ventory control system such as two classical inventory models namely
continuous review (Mahapatra et al., 2021) and periodic review (Zhu
et al., 2022). Furthermore, due to development of information tech
nology, inventory decision is often integrated with production planning
such as Material Requirements Planning (MRP), Manufacturing Re
sources Planning (MRP II), Advanced Planning and Scheduling (APS),
and Enterprise Resources Planning (ERP) (Shofa and Widyarto, 2017)
(Chandraju et al., 2012). However, most of proposed methods work
under strict assumption namely deterministic and static condition. The
study conducted in 2011 illustrates that using those assumptions can
prove to be quite expensive when applying the corresponding models to
real-life situations (Tunc et al., 2011). Even more, most methods only
consider single-echelon model in which does not consider the other
parties leading to local optimal solution. When mathematical model is
used for multi-echelon inventory management, the problem can quickly
become too complex and computationally expensive due to growing

decision variables. This complicates the connection between models and
practical real-world applications.
Many previous studies have proposed the derived version of in
ventory model to consider uncertainty and dynamicity. A study (Patri
arca et al., 2020) derived EOQ model to consider demand uncertainty
and time-dependent product quality via Monte Carlo simulation. Using
similar method, some other studies considered lead time (Rizqi and
Khairunisa, 2020) and defective product (Rizqi et al., 2021) as uncer
tainty factors besides the demand by deriving Min-Max inventory
method. Another idea to consider uncertainty in inventory is by utilizing
predictive methods (Prak and Teunter, 2019) such as utilizing ensemble
deep learning for order-up-to-level inventory optimization for demand
forecasting (Seyedan et al., 2023). However, none of studies has
considered not only supply and demand uncertainties, but also the dis
count event uncertainty given by supplier at a time. This discount event
also can be seen as an uncertain or dynamic price which has become
more common recently. Previous studies tried to comprehend the EOQ
model with discount quantity but still in static and certain conditions
only (Kristiyani and Daryanto, 2019) (Firoozi et al., 2013). Further,
many previous studies also only assume single-delivery option in in
ventory decision or even not considering transportation at all when
single-echelon inventory is considered. Traditional inventory manage
ment literature predominantly focuses on a single-sourcing model
characterized by static customer demand and a deterministic lead time
(Liu et al., 2022). In many multi-echelon practical situations, supplier or
third-party logistics usually offer multiple delivery options or sometimes
called as multi-sourcing or multi-mode with the trade-off in cost and
responsiveness.
Therefore, considering all previous features, it leads to a new in
ventory problem namely Multi-Echelon Inventory Optimization with
Delivery Options and Uncertain Discount (MEIO-DO-UD). The purpose
of this study is to provide an effective model that can be practically
useful by applying Neuroevolution Reinforcement Learning (NERL) for
multi-period, multi-echelon, multi-network, and single-product in
ventory problems. Compared to the other reinforcement learning algo
rithms, NERL can avoid sample inefficiency without the need to update
Q-table, better exploration, flexibility, and faster convergence. The
systematic experiment is given to verify the effectiveness and the effi
ciency of the proposed model in solving MEIO-DO-UD under different
setups of algorithms. Comparative analysis is also conducted to ensure
the superiority of the proposed model compared to the classical
continuous inventory review model i.e. (s,Q) including its impact on the
bull-whip effect.
Actions/Variables
Q(i,t)
Order quantity of facility i at t {Integer}
DO(d,i,t) Delivery option d is chosen in facility i at t {Binary}
f(s,i,t)
Supplier s supplies facility i at t {Binary}

2, an examination of the literature review outcomes is provided,
including a comprehensive overview of reinforcement learning for in
ventory problem, NERL, along with a comparison between this study to
the previous studies. In section 3, the methodology is elaborated in
detail, outlining the design of the reinforcement learning algorithm,
simulation modeling, and how total cost is measured as expected
reward. Section 4 entails the case study given where serial and diver
gence supply chain networks are used, including the result and discus
sion, measuring the statistical difference between the proposed method
compared to other methods. Subsequently, section 5 furnishes conclu
sions aimed at summarizing the undertaken tasks and outcomes, while
also proposing potential directions for future research endeavors.

Literature review
Inventory management has vital functions in business for both small
scale i.e. operations management (Guo et al., 2019) and large scale i.e.
supply chain management (Singh and Verma, 2018). The main objective
in inventory management is to minimize total system cost while meeting
customer service level. Inventory management often involves sequential
decision-making challenges that can be represented as Markov Decision
Processes (MDPs), making them suitable for the application of Rein
forcement Learning (RL) (Wu et al., 2023) (Boute et al., 2022). In
contrast to analytical-based models, RL views the problem as a simu
lated environment, thus avoiding oversimplification of reality and
allowing for more flexibility, especially for considering a larger number
of variables (Kosasih and Brintrup, 2022). RL constitutes one of the three
primary machine learning paradigms, alongside supervised and unsu
pervised learning. RL involves agent or actor, which essentially act as
domain experts, making decisions and taking actions under specific
states (Almahamid and Grolinger, 2021). Unlike supervised learning, RL
does not rely on labeled data to learn, instead, it acquires knowledge
through experiential learning by interacting with its environment,
observing outcomes, and adapting its behavior accordingly as illustrated
in Fig. 1. In general reinforcement learning algorithms, the main
objective is to find an optimal policy (X
π
(S
t
)) based on certain state (S) in
time t in order to maximize the expected reward as shown in Equation
(1). This might include uncertain information in the initial state S
0
, thus,
the expected operator is used to average among random variables.

神經網路被用作函數近似器，將狀態映射到動作，這促成了深度強化學習（Deep Reinforcement Learning, DRL）的發展（Wang et al., 2022）。除了傳統的強化學習演算法外，神經演化強化學習（Neuroevolution Reinforcement Learning, NERL）因其在多種應用領域（如核能（Radaideh et al., 2023）、遊戲（Barmi et al., 2011）、自駕車（AbuZekry, 2019）等）展現出的競爭力而備受關注。NERL的核心在於運用演化演算法來建立並訓練神經網路（Heidrich-Meisner and Igel, 2009）。演化演算法（Evolutionary Algorithms, EAs）運作原理簡單，模仿自然選擇與演化的機制（Slowik and Kwasnicka, 2020）。在優化神經網路方面，一些知名且成功應用於此的演化演算法包括差分演化（Differential Evolution, DE）（Baioletti et al., 2020）、模因演算法（Memetic Algorithm, MA）（García-Ródenas et al., 2021）以及演化策略（Evolution Strategy, ES）（Friedrich and Maziero, 2023）。
NERL lies in the direct policy search with gradient-free process.
NERL also belongs to the DRL once the neural network has many hidden
layers. It has advantages ranging from better exploration to the ease of
parallelization and potentially more robust to the hyper-parameters
selection (Zhang et al., 2022). Compared to the other RL algorithms
such as Deep Q Network (DQN) and PPO, they face some disadvantages,
especially for solving MEIO. First, DQN and PPO algorithms work iter
atively for each time step that needs to collect data and make an update
such as the Q-Table by using Bellman’s equation. This will lead to
sample inefficiency and slower convergence. Second, DQN faces diffi
culty with a large number of discrete actions. Even though PPO is more
f
lexible, PPO can still face challenges in learning effective representa
tions. Third, both algorithms have limited exploration since DQN relies
on epsilon-greedy strategies while PPO uses policy gradients for explo
ration. These disadvantages can be solved through NERL due to the
capability of metaheuristics. In addition, this study also designs NERL as
single-agent instead of multi-agent e.g. multi-agent deep reinforcement
learning algorithm (Zhou et al., 2024) since it has better computational
complexity for large-scale problems.
Inventory model has developed rapidly in literature following the
more complex condition in reality. Due to effectivity, data-driven
framework has been used increasingly by emphasizing on optimality
(Rekabi et al., 2023) or practicality (Geevers et al., 2022), from
single-echelon inventory (Kara and Dogan, 2018) to multi-echelon in
ventory (Rizqi, 2023). Nevertheless, many previous studies in proposing
inventory models under uncertainty only consider demand variability
with constant lead time (Dittrich and Fohlmeister, 2021), (Prestwich
et al., 2012), (Kaynov et al., 2024) avoiding the stochasticity of the lead
time even though it is practical (Geevers et al., 2022). In terms of
decision-making, most studies also only consider providing order
quantity (Kaynov et al., 2024) (Peng et al., 2019). Whereas, multiple
decisions may be needed such as delivery options where several modes
are available offering the trade-off between delivery time and cost. By
seeing this gap, this study takes one step further in inventory manage
ment by providing multi-echelon inventory optimization model
considering not only demand and lead time uncertainties but also

discount uncertainty with delivery options decision, in addition to order
quantity. To achieve the minimum expected total cost, NERL framework
is proposed by utilizing the integration of evolutionary algorithm as the
policy optimizer and neural network as function approximator in map
ping state to action. In other words, evolutionary algorithm will help to
optimize the weight and bias of neural network to reach near-global
optimal solution. In previous studies, Q-Learning and Deep Q-Learning
were used (Oroojlooyjadid et al., 2022). However, they may lead to
other issues where the action space becomes larger and back propaga
tion cannot provide satisfactory results.
Several relevant studies that solve multi-echelon inventory problem
with reinforcement learning are reviewed and compared with this study
as shown in Table 1. It can be seen that there are already a few studies
utilizing different RL methods but only one study that have utilized
NERL based on Evolution Strategy without any capacity constraint and
complicated uncertainties considered. Further, they result show the
superiority of NERL algorithm compared to the stochastic programming
for small and large scale of simple inventory problem. In terms of
application to inventory management, most of studies considers only
uncertain demand while assuming other factors as certain or deter
ministic. In terms of supply chain network, some studies have consid
ered serial and divergence network. Most of studies also do not consider
delivery options. Different types in handling unmet demand are classi
f
ied into three groups namely backorders, lost sales, and hybrid or so-
called as partial backorder where some customers are willing to wait
through backorder but others are not. It can be seen that this study
differs with the others by emphasizing not only on the methodology, but
also introducing the comprehensive and practical problems in multi-
echelon inventory problem so-called MEIO-DO-UD.

以上的文獻: 

Baruah, P., Chinnam, R.B., Korostelev, A., Dalkiran, E., 2016. Optimal soft-order
revisions under demand and supply uncertainty and upstream information. Int. J.
Prod. Econ. https://doi.org/10.1016/j.ijpe.2016.08.009.

Gallego-garcía, D., Gallego-garcía, S., García-garcía, M., 2021a. An optimized system to
reduce procurement risks and stock-outs: a simulation case study for a component
manufacturer. Appl. Sci. https://doi.org/10.3390/app112110374.

Unyimadu, S. O., and K. A. Anyibuofu. "Inventory management practices in manufacturing firms." *Industrial Engineering Letters* 4.9 (2014): 40-44.

Harris, F.W., 1990. How many parts to make at once. Oper. Res. https://doi.org/
10.1287/opre.38.6.947.

Mahapatra, A.S., Soni, H.N., Mahapatra, M.S., Sarkar, B., Majumder, S., 2021.
A continuous review production-inventory system with a variable preparation time
in a fuzzy random environment. Mathematics. https://doi.org/10.3390/
math9070747.

Zhu, C., Yang, B., Ma, H., Gao, C., Chen, J., 2022. Optimal strategy for a periodic review
inventory system with discounted variable cost and finite ordering capacity. Oper.
Res. https://doi.org/10.1051/ro/2022154.

Shofa, M.J., Widyarto, W.O., 2017. Effective production control in an automotive
industry: MRP vs. demand-driven MRP. In: AIP Conference Proceedings. [https://doi](https://doi/).
org/10.1063/1.4985449.

Chandraju, S., B. Raviprasad, and C. Kumar. "Implementation of system application product (SAP) materials management (MM-Module) for material requirement planning (MRP) in sugar industry." *International Journal of Scientific and Research Publications* 2.9 (2012): 1-5.

Tunc, H., Kilic, O.A., Tarim, S.A., Eksioglu, B., 2011. The cost of using stationary
inventory policies when demand is non-stationary. Omega. https://doi.org/
10.1016/j.omega.2010.09.005.

Patriarca, R., Di Gravio, G., Costantino, F., Tronci, M., 2020. EOQ inventory model for
perishable products under uncertainty. J. Inst. Eng. Prod. https://doi.org/10.1007/
s11740-020-00986-5.

Rizqi, Z.U., Khairunisa, A., 2020. Integration of deterministic and probabilistic inventory
methods to optimize the balance between overstock and stockout. In: IOP
Conference Series: Materials Science and Engineering. https://doi.org/10.1088/
1757-899X/722/1/012060.

Rizqi, Z.U., Khairunisa, A., Maulani, A., 2021. Financial assessment on designing
inventory policy by considering demand, lead time, and defective product
uncertainties: A monte carlo simulation. Indonesian Scholars Scientific Summit
Taiwan Proceeding 3, 36–42. https://doi.org/10.52162/3.2021110.

Prak, D., Teunter, R., 2019. A general method for addressing forecasting uncertainty in
inventory models. Int. J. Forecast. https://doi.org/10.1016/j.ijforecast.2017.11.004

Seyedan, M., Mafakheri, F., Wang, C., 2023. Order-up-to-level inventory optimization
model using time-series demand forecasting with ensemble deep learning. Supply
Chain Analytics. https://doi.org/10.1016/j.sca.2023.100024.

Kristiyani, I.M., Daryanto, Y., 2019. An inventory model considering all unit discount
and carbon emissions. International Journal of Industrial Engineering and
Engineering Management. https://doi.org/10.24002/ijieem.v1i2.3410.

Firoozi, Z., Tang, S.H., Ariafar, S., Ariffin, M.K.A.M., 2013. An optimization approach for
A joint location inventory model considering quantity discount policy. Arabian J.
Sci. Eng. https://doi.org/10.1007/s13369-012-0360-9.

Liu, X., Hu, M., Peng, Y., Yang, Y., 2022. Multi-agent deep reinforcement learning for
multi-echelon inventory management. SSRN Electron. J. https://doi.org/10.2139/
ssrn.4262186.

Guo, S., Choi, T.M., Shen, B., Jung, S., 2019. Inventory management in mass
customization operations: a review. IEEE Trans. Eng. Manag. https://doi.org/
10.1109/TEM.2018.2839616.

Singh, D., Verma, A., 2018. Inventory management in supply chain. In: Materials Today:
Proceedings. https://doi.org/10.1016/j.matpr.2017.11.641.

Wu, G., de Carvalho Servia, M.´ A., Mowbray, M., 2023. Distributional reinforcement
learning for inventory management in multi-echelon supply chains. Digital Chemical
Engineering. https://doi.org/10.1016/j.dche.2022.100073.

Boute, R.N., Gijsbrechts, J., van Jaarsveld, W., Vanvuchelen, N., 2022. Deep
reinforcement learning for inventory control: a roadmap. Eur. J. Oper. Res. https://
[doi.org/10.1016/j.ejor.2021.07.016](http://doi.org/10.1016/j.ejor.2021.07.016).

Kosasih, E.E., Brintrup, A., 2022. Reinforcement learning provides a flexible approach for
realistic supply chain safety stock optimisation. IFAC-PapersOnLine. https://doi.org/
10.1016/j.ifacol.2022.09.609.

Almahamid, F., Grolinger, K., 2021. Reinforcement learning algorithms: an overview and
classification. In: Canadian Conference on Electrical and Computer Engineering.
https://doi.org/10.1109/CCECE53047.2021.9569056.

Wang, X., Wang, S., Liang, X., Zhao, D., Huang, J., Xu, X., Dai, B., Miao, Q., 2022. Deep
reinforcement learning: a survey. IEEE Transact. Neural Networks Learn. Syst.
https://doi.org/10.1109/TNNLS.2022.3207346.

Radaideh, M.I., Du, K., Seurin, P., Seyler, D., Gu, X., Wang, H., Shirvan, K., 2023. NEORL:
NeuroEvolution optimization with reinforcement learning—applications to carbon-
free energy systems. Nucl. Eng. Des. https://doi.org/10.1016/j.
nucengdes.2023.112423.

Barmi, Z.A., Ebrahimi, A.H., Feldt, R., 2011. Evolution strategies as a scalable alternative
to reinforcement learning tim. In: 2011 IEEE Fourth International Conference on
Software Testing, Verification and Validation Workshops.

AbuZekry, A., 2019. Comparative study of NeuroEvolution algorithms in reinforcement
learning for self-driving cars. European Journal of Engineering Science and
Technology. https://doi.org/10.33422/ejest.2019.09.38.

Heidrich-Meisner, V., Igel, C., 2009. Neuroevolution strategies for episodic
reinforcement learning. J. Algorithm. https://doi.org/10.1016/j.
jalgor.2009.04.002.

Slowik, A., Kwasnicka, H., 2020. Evolutionary algorithms and their applications to
engineering problems. Neural Comput. Appl. https://doi.org/10.1007/s00521-020-
04832-8.

Baioletti, M., Di Bari, G., Milani, A., Poggioni, V., 2020. Differential evolution for neural
networks optimization. Mathematics. https://doi.org/10.3390/math8010069.

García-R´ odenas, R., Linares, L.J., L´ opez-G´omez, J.A., 2021b. Memetic algorithms for
training feedforward neural networks: an approach based on gravitational search
algorithm. Neural Comput. Appl. https://doi.org/10.1007/s00521-020-05131-y.

Friedrich, L., Maziero, J., 2023. Evolution strategies: application in hybrid quantum-
classical neural networks. Quant. Inf. Process. https://doi.org/10.1007/s11128-023-
03876-8.

Zhang, N., Gupta, A., Chen, Z., Ong, Y.S., 2022. Multitask neuroevolution for
reinforcement learning with long and short episodes. IEEE Transactions on Cognitive
and Developmental Systems. https://doi.org/10.1109/TCDS.2022.3221805.

Zhou, Y., Guo, K., Yu, C., Zhang, Z., 2024. Optimization of multi-echelon spare parts
inventory systems using multi-agent deep reinforcement learning. Appl. Math.
Model. https://doi.org/10.1016/j.apm.2023.10.039.

Rekabi, S., Goodarzian, F., Garjan, H.S., Zare, F., Mu˜ nuzuri, J., Ali, I., 2023. A data-
driven mathematical model to design a responsive-sustainable pharmaceutical
supply chain network: a Benders decomposition approach. Ann. Oper. Res. https://
[doi.org/10.1007/s10479-023-05734-3](http://doi.org/10.1007/s10479-023-05734-3).

Geevers, K., van Hezewijk, L., Mes, M.R.K., 2022. Multi-echelon inventory optimization
using deep reinforcement learning. SSRN Electron. J. https://doi.org/10.2139/
ssrn.4227665.

Kara, A., Dogan, I., 2018. Reinforcement learning approaches for specifying ordering
policies of perishable inventory systems. Expert Syst. Appl. https://doi.org/10.1016/
j.eswa.2017.08.046.

Dittrich, M.A., Fohlmeister, S., 2021. A deep q-learning-based optimization of the
inventory control in a linear process chain. J. Inst. Eng. Prod. https://doi.org/
10.1007/s11740-020-01000-8.

Prestwich, S.D., Tarim, S.A., Rossi, R., Hnich, B., 2012. A neuroevolutionary approach to
stochastic inventory control in multi-echelon systems. Int. J. Prod. Res. [https://doi](https://doi/).
org/10.1080/00207543.2011.574503.

Kaynov, I., van Knippenberg, M., Menkovski, V., van Breemen, A., van Jaarsveld, W.,
2024. Deep reinforcement learning for one-warehouse multi-retailer inventory
management. Int. J. Prod. Econ. https://doi.org/10.1016/j.ijpe.2023.109088.

Peng, Z., Zhang, Y., Feng, Y., Zhang, T., Wu, Z., Su, H., 2019. Deep reinforcement
learning approach for capacitated supply chain optimization under demand
uncertainty. In: Proceedings - 2019 Chinese Automation Congress (CAC). https://
[doi.org/10.1109/CAC48633.2019.8997498](http://doi.org/10.1109/CAC48633.2019.8997498).

Oroojlooyjadid, A., Nazari, M.R., Snyder, L.V., Tak´ aˇ c, M., 2022. A deep Q-network for
the beer game: deep reinforcement learning for inventory optimization. Manuf. Serv.
Oper. Manag. https://doi.org/10.1287/MSOM.2020.0939.

Li, X., Hua, G., Huang, A., Sheu, J. B., Cheng, T. C. E., & Huang, F. (2020). Storage assignment policy with awareness of energy consumption in the Kiva mobile fulfilment system. Transportation Research Part E: Logistics and Transportation Review, 144, 102158.

Luo, L., Zhao, N., Zhu, Y., & Sun, Y. (2023). A* guiding DQN algorithm for automated guided vehicle pathfinding problem of robotic mobile fulfillment systems. Computers & Industrial Engineering, 178, 109112.

Yuan, R., Dou, J., Li, J., Wang, W., & Jiang, Y. (2023). Multi-robot task allocation in e-commerce RMFS based on deep reinforcement learning.. Mathematical biosciences and engineering : MBE, 20 2, 1903-1918 . https://doi.org/10.3934/mbe.2023087.

Zhou, X., Shi, X., Chu, W., Jiang, J., Zhang, L., & Deng, F. (2024). Learning to Solve Multi-AGV Scheduling Problem with Pod Repositioning Optimization in RMFS. 2024 IEEE International Conference on Industrial Technology (ICIT), 1-8. https://doi.org/10.1109/ICIT58233.2024.10541015.

Cao, K., Wang, L., Zhang, S., Duan, L., Jiang, G., Sfarra, S., Zhang, H., & Jung, H. (2024). Optimization Control of Adaptive Traffic Signal with Deep Reinforcement Learning. Electronics. https://doi.org/10.3390/electronics13010198.

