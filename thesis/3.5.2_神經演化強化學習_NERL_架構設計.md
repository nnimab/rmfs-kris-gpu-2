# 3.4.2 神經演化強化學習 (NERL) 架構設計

神經演化強化學習（NERL）是本研究採用的第二種 DRL 方法，它代表了一種與 DQN 截然不同的優化範式。NERL 不依賴於梯度下降來優化網路參數，而是將神經網路的權重和偏置視為個體的「基因」，並應用演化演算法（Evolutionary Algorithms, EA）直接在參數空間中進行搜索。這種方法屬於黑箱優化，它不關心獎勵函數是否可微，僅根據每個策略（即每個神經網路個體）在與環境互動後獲得的最終適應度分數（Fitness Score）來進行優化。

### 1. 核心思想與運作流程

NERL 的核心在於維護一個由多個神經網路組成的**族群 (Population)**。其基本運作流程遵循「評估 -> 選擇 -> 繁殖」的演化循環：
1.  **評估 (Evaluation)**：族群中的每一個神經網路個體都會獨立地在模擬環境中運行一個完整的評估回合（Episode）。在此期間，該個體全權負責所有路口的交通決策。回合結束後，系統會根據其宏觀表現（如 `3.4.3` 節定義的全局獎勵）計算出一個單一的適應度分數。
2.  **選擇 (Selection)**：根據所有個體獲得的適應度分數，演算法會從當前族群中選擇表現優異的個體作為「父代」(Parents) 參與繁殖。本研究採用**錦標賽選擇 (Tournament Selection)** 機制，即隨機選取 `k` 個個體進行小組競賽，其中適應度最高的個體勝出並被選為父代。這種方法在選擇壓力和族群多樣性之間提供了良好的平衡。
3.  **繁殖 (Reproduction)**：通過對選出的父代進行**交叉 (Crossover)** 與**變異 (Mutation)** 操作來生成新一代的「子代」(Offspring)，以填充下一個族群。
    - **精英保留 (Elitism)**：為了確保最優的基因能夠傳承下去，每一代中適應度最高的少數個體（稱為「精英」）會被直接、完整地複製到下一代族群中。
    - **交叉**：模擬生物繁殖過程，將兩個父代網路的權重向量進行混合，以產生新的子代網路。本研究採用**均勻交叉 (Uniform Crossover)**，即對權重向量中的每一位，以 50% 的機率隨機選擇繼承自父代一或父代二。
    - **變異**：在交叉之後，對子代網路的權重進行微小的隨機擾動。本研究採用**高斯變異 (Gaussian Mutation)**，即以一定的變異機率 (`mutation_rate`) 選定部分權重，並為其疊加一個從平均值為 0、標準差為 `mutation_strength` 的高斯分布中採樣的噪聲。變異是維持族群多樣性、防止早熟並探索新解的關鍵。

通過不斷重複上述循環，整個族群的平均適應度會逐步提升，最終收斂到能夠高效解決問題的策略網路上。

### 2. NERL 與 DQN 的關鍵區別

- **優化機制**: DQN 使用梯度下降和反向傳播，依賴於可微的損失函數；NERL 使用演化算法，是一種無梯度的黑箱優化方法，對獎勵函數的連續性和可微性沒有要求。
- **探索機制**: DQN 通過 ε-greedy 等策略在動作空間進行探索；NERL 的探索則是在策略的**參數空間**中通過交叉和變異來實現。
- **獎勵訊號**: DQN 對即時、頻繁的獎勵訊號（Step Reward）更敏感；而 NERL 的架構天然地適合處理延遲、稀疏的全局獎勵（Global Reward），因為它評估的是整個回合的最終表現。
- **並行性**: NERL 的評估過程具有極高的並行性，族群中的每個個體都可以被分配到獨立的處理器核心上同時進行評估，使其能夠有效利用多核心 CPU 或計算集群。

### 3. 架構細節

`NEController` 的狀態表示、動作空間以及底層的神經網路結構與 `DQNController` 完全相同，以確保兩種方法之間比較的一致性與公平性。兩者的根本差異僅在於驅動網路權重更新的學習演算法不同。

---
**【圖表建議：圖 3.4.2 - NERL 演化循環示意圖】**

建議在此處繪製一張循環圖，說明 NERL 的核心運作流程。圖中應包含：
1.  **族群 (Population)**：展示多個神經網路個體。
2.  **並行評估 (Parallel Evaluation)**：從族群中取出每個個體，在獨立的環境中運行並計算其適應度分數。
3.  **選擇 (Selection)**：根據適應度分數，通過錦標賽選擇出父代。
4.  **繁殖 (Reproduction)**：展示精英保留、交叉和變異操作，生成新一代族群。
5.  箭頭應清晰地指向下一個階段，形成一個完整的閉環。 