# 3.4.3 獎勵函數設計

獎勵函數 (Reward Function) 是強化學習的核心，它向智慧體 (Agent) 提供回饋訊號，引導其學習出期望的行為。本研究設計了兩種截然不同的獎勵模式：**即時獎勵 (Step Reward)** 和 **全局獎勵 (Global Reward)**，以從不同維度探索交通控制策略的優化路徑。

### 1. 即時獎勵 (Step Reward)

此模式旨在為控制器在每個時間步 (tick) 的決策提供立即的回饋。其核心思想是基於**局部可觀測事件**來獎勵促進交通流動的行為，並懲罰導致延遲或頻繁變換的行為。獎勵訊號 \\(R_{step}\\) 是一個複合函數，其核心組成部分是旨在鼓勵交通流動的**流通獎勵 (\\(R_{flow}\\))** 和**能源獎勵 (\\(R_{energy}\\))**，以及旨在抑制低效行為的**等待成本 (\\(C_{wait}\\))** 和**切換成本 (\\(C_{switch}\\))**。具體而言，當機器人成功通過路口時，系統會根據其任務優先級給予相應的流通獎勵；若其能以低速、低能耗的方式通過，則會獲得額外的能源獎勵。相對地，在路口等待的機器人會產生等待成本，而路口信號燈的切換則會觸發一次性的切換成本，以維持系統穩定性。

此外，為了引導智慧體關注系統瓶頸，我們對**關鍵路口**（如通往主幹道的入口）的所有獎懲訊號乘以一個更高的權重 \\(w_{critical}\\)。綜合以上各點，在路口 \\(i\\) 的單個時間步 \\(t\\) 的即時獎勵 \\(R_{step}(i, t)\\) 可以概念化地表示為：

\\[
R_{step}(i, t) = w_i \cdot \alpha \cdot \left( \sum_{r \in \text{Passed}} (R_{flow}(p_r) + R_{energy}(r)) - \sum_{r' \in \text{Waiting}} C_{wait}(p_{r'}) - C_{switch}(i, t) \right)
\\]

其中 \\(w_i\\) 是路口 \\(i\\) 的權重（關鍵路口為 \\(w_{critical}\\)，否則為 1），\\(\alpha\\) 是一個訊號放大係數，而 \\(\text{Passed}\\) 和 \\(\text{Waiting}\\) 分別是該時間步在路口 \\(i\\) 通過和等待的機器人集合。

### 2. 全局獎勵 (Global Reward)

與即時獎勵不同，全局獎勵模式在整個評估回合 (episode) 結束時，才根據**最終的宏觀系統表現**給予一個單一的、延遲的獎勵分數。這種模式迫使智慧體學習行為與其長期後果之間的關聯。全局獎勵 \\(R_{global}\\) 的設計旨在最大化一個**綜合效率分數**，其核心是一個成本效益分析。該分數的分子部分代表完成訂單所帶來的效益，由訂單總數 \\(N_{orders}\\) 與單筆訂單的獎勵權重 \\(W_{completion}\\) 的乘積構成。分母部分則代表總成本，由正規化後的總能源消耗 \\(E_{total} / C_{energy}\\)、時間成本 \\(T_{sim} \cdot W_{time}\\) 以及因下游堵塞造成的溢出懲罰 \\(P_{spillback}\\) 加總而成。此外，若整個回合未發生溢出，系統還會給予一筆額外獎勵 \\(B_{no\_spillback}\\)。完整的公式如下：

\\[
R_{global} = \frac{N_{orders} \cdot W_{completion}}{(\frac{E_{total}}{C_{energy}}) + (T_{sim} \cdot W_{time}) + P_{spillback} + \epsilon} + B_{no\_spillback}
\\]

這個公式清晰地引導智慧體在**完成盡可能多訂單**的同時，必須努力**降低能源和時間的總成本**，並**避免造成系統瓶頸**。其中 \\(\epsilon\\) 是一個極小值，用以防止分母為零。 