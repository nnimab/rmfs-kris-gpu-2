# 1.1 研究背景與動機

全球零售業正經歷由電子商務驅動的結構性轉變。根據預測，全球零售電商年產值將增長至 2025 年的 6.42 兆美元，佔整體零售總額的比重將突破 20% [1]。為應對電商訂單「小批量、高頻次、時效敏感」的新常態，傳統倉儲模式已不敷使用，促使業者大規模導入**自動化移動揀選系統 (Robotic Mobile Fulfillment System, RMFS)**。在 RMFS 系統中，上百台自主移動機器人 (Autonomous Mobile Robot, AMR) 於網格化倉儲內協同作業，可將揀貨效率提升數倍。

然而，高密度的 AMR 運行也衍生出新的運營瓶頸。**交叉路口的交通壅塞、機器人之間的等待延遲，以及頻繁的加減速行為**，不僅直接限制了系統的整體吞吐量，更顯著增加了能源消耗與碳排放。在全球日益嚴格的永續發展與碳中和政策下，此問題變得至關重要：

*   **高額的碳足跡**：運輸與倉儲相關活動的碳排放約佔全球溫室氣體總排放的 24%，已被 DHL 等國際物流龍頭視為優先的減碳戰場 [2]。
*   **迫切的政策壓力**：歐盟的**「碳邊境調整機制 (CBAM)」**已於 2023 年啟動，預計在 2026 年全面生效。該機制將對高碳密集度的進口產品課徵額外成本，此舉將迫使供應鏈上下游的企業，嚴格審視並管理其倉儲與運輸環節的碳排放 [3]。

在此背景下，學術界已開始應用強化學習 (Reinforcement Learning, RL) 等智慧化方法來優化 RMFS 的運營。部分研究透過深度強化學習對訂單進行排程以最小化成本 [4]，或是透過調整交通策略與機器人速度來降低約 3–5% 的總能耗 [5]。然而，這些研究或側重於訂單層面的調度，或是在能耗與效率的權衡上仍有探索空間，**目前仍缺乏一個專注於路口層級、且以「能耗–效率」為雙重優化目標的控制框架**。

為填補此研究缺口，本研究提出一種基於**神經演化強化學習 (Neuroevolution Reinforcement Learning, NERL)** 的路口控制器。此方法結合了演化演算法的廣域搜索能力與深度強化學習的即時決策優勢，旨在為 RMFS 路口提供一個「自適應、能耗感知」的通行權分配機制，期望在維持訂單處理效率的基礎上，有效降低 AMR 的單位任務能耗。

# 1.2 研究目標

為驗證上述構想並回答核心研究問題，本研究擬定以下四項具體目標，其成果將於後續章節中詳細闡述：

1.  **建構高擬真度之 RMFS 模擬與控制平台**
    *   建立一個包含中央儲存區、單向通道、工作站與充電站的倉儲物理環境。
    *   設計一套基於策略模式與工廠模式的模組化交通控制系統架構，以支持不同演算法的彈性整合與公平比較（詳見 3.2 節）。

2.  **設計並實現多種交通控制策略**
    *   開發兩種啟發式基線控制器：一種基於固定時間週期的**時間基礎控制器 (Time-Based Controller)**，與一種基於即時等待隊列的**佇列基礎控制器 (Queue-Based Controller)**（詳見 3.3 節）。
    *   實現基於價值之深度強化學習方法，即**深度 Q 網路 (Deep Q-Network, DQN)**，作為 DRL 的比較基準（詳見 3.4.1 節）。
    *   開發本研究的核心貢獻，即**神經演化強化學習 (NERL) 控制器**，並探討不同演化超參數（探索型 vs. 利用型）的影響（詳見 3.4.2 節）。

3.  **設計並執行嚴謹的對照實驗**
    *   定義一套涵蓋效率、流量與穩定性的綜合性效能評估指標（詳見 3.2.4 節）。
    *   設計一個包含十二個獨立實驗組的對照矩陣，系統性地比較不同控制器在兩種獎勵模式（步階獎勵 vs. 全局獎勵）與不同評估時長下的表現（詳見 3.6.1 節）。

4.  **量化分析與統計驗證**
    *   根據實驗數據，對所有控制策略的效能進行深入的量化分析與比較。
    *   採用獨立樣本 t-檢定等統計學方法，科學地驗證本研究提出的 NERL 方法相較於基線控制器，其效能提升是否具有統計顯著性（詳見 3.5.4 節）。

# 1.3 研究範圍與限制

為聚焦於核心問題並確保研究的深度，本研究對其範圍做出明確界定。以下條列了本研究包含的核心範疇以及為簡化模型而未涵蓋的部分。

## 1.3.1 研究範疇

| 項目 | 內容 | 說明 |
| :--- | :--- | :--- |
| **核心問題** | 交叉路口通行權的動態分配。 | 研究聚焦於解決由路口交通協調不當引發的壅塞與能源浪費問題。 |
| **控制策略** | 時間基礎、佇列基礎、DQN、NERL。 | 涵蓋了從靜態規則、動態啟發式到兩種不同 DRL 框架的全面比較。 |
| **優化目標** | 最小化總能量消耗、最大化訂單完成數、最小化平均等待時間。 | 採用多目標優化的視角，評估策略在效率與能耗上的綜合表現。 |
| **評估指標** | 詳見 3.2.4 節定義的效率、流量與穩定性指標。 | 使用一套完整的量化指標來全面評估各控制器的優劣。 |

## 1.3.2 研究限制

本研究有意地將以下議題排除在討論範圍之外，以確保對核心問題的專注。這些議題可作為未來研究的延伸方向。

*   **路徑規劃 (Path Planning)**：本研究不涉及機器人從起點到終點的全域路徑規劃演算法，假設所有機器人遵循預設的最短路徑。
*   **訂單批次處理 (Order Batching)**：本研究不考慮將多個訂單合併為一個揀貨任務的策略，假設訂單被依序分解為獨立的任務。
*   **庫位指派 (Slotting Assignment)**：本研究不優化貨物（SKU）在貨架上的存放位置或貨架在倉庫中的儲存位置，假設其為隨機或預設配置。
*   **充電策略管理 (Charging Strategy Management)**：雖然模擬環境中存在充電站，但本研究不涉及對機器人電量進行監控並主動調度其進行充電的策略。