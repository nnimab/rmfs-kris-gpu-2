# 3.5.3 DRL 模型超參數設定

為確保本研究中 DRL 實驗的可複現性與結果的有效性，本節詳細列出在訓練 `DQN` 與 `NERL` 控制器時所使用的關鍵超參數。這些參數的設定基於初步的收斂性與穩定性實驗，並在正式訓練期間保持固定。

### 1. 通用神經網路架構

為確保比較基線 (DQN) 與核心方法 (NERL) 的公平性，兩者採用了完全相同的神經網路架構。此架構在模型的表達能力與計算效率之間取得平衡，足以應對本研究中的交通控制問題。

| 層級 (Layer) | 類型 (Type) | 輸入維度 (Input Dim) | 輸出維度 (Output Dim) | 激活函數 (Activation) |
| :--- | :--- | :--- | :--- | :--- |
| 1 | 輸入層 (Input) | 17 | 17 | - |
| 2 | 全連接層 (FC 1) | 17 | 128 | ReLU |
| 3 | 全連接層 (FC 2) | 128 | 64 | ReLU |
| 4 | 輸出層 (Output) | 64 | 6 | - |


### 2. DQN 特有超參數

下表為 `DQN` 控制器（實驗組 3、4）在訓練過程中使用的主要超參數。

| 參數 (Hyperparameter) | 程式碼變數 | 數值 (Value) | 說明 |
| :--- | :--- | :--- | :--- |
| 學習率 (Learning Rate) | `learning_rate` | 5e-4 | Adam 優化器的學習率。 |
| 折扣因子 (Gamma) | `gamma` | 0.99 | 未來獎勵的折扣係數，值越接近 1 代表越重視長期回報。 |
| 初始探索率 (Epsilon) | `epsilon` | 1.0 | 訓練初期完全隨機選擇動作的機率。 |
| 最小探索率 | `epsilon_min` | 0.01 | 探索率衰減的下限值。 |
| 探索率衰減率 | `epsilon_decay` | 0.9995 | 每個訓練步驟後，探索率乘以的指數衰減係數。 |
| 經驗回放記憶體大小 | `memory_size` | 50,000 | 存儲 $(s, a, r, s')$ 轉換樣本的最大數量。 |
| 批次大小 | `batch_size` | 8,192 | 每次從記憶體中抽樣進行網路更新的樣本數量。 |
| 目標網路更新頻率 | `target_update_freq` | 1,000 | 每隔多少個訓練**步驟**將策略網路的權重複製到目標網路。 |

### 3. NERL 特有超參數

下表為 `NERL` 控制器（實驗組 5-12）在演化過程中使用的主要超參數。其中，變異率與變異強度根據 **3.6.1 節** 中定義的**探索型 (A)** 與**利用型 (B)** 變體而有所不同。

| 參數 (Hyperparameter) | 程式碼變數 | 變體 A (探索型) | 變體 B (利用型) | 說明 |
| :--- | :--- | :--- | :--- | :--- |
| 族群大小 | `population_size` | 20 | 20 | 每一代中包含的個體（神經網路）數量。 |
| 精英保留比例 | `elite_ratio` | 0.2 | 0.2 | 每一代中適應度最高的個體被直接保留的比例。 |
| 錦標賽選擇大小 | `tournament_size` | 4 | 4 | 在錦標賽選擇中，每次隨機比較的個體數量。 |
| 交叉率 | `crossover_rate` | 0.8 | 0.8 | 兩個父代個體進行基因交叉的機率。 |
| 變異率 | `mutation_rate` | **0.2** | **0.1** | 個體基因（網路權重）發生變異的基礎機率。 |
| 變異強度 | `mutation_strength` | **0.15** | **0.15** | 高斯變異的標準差，控制變異的幅度大小。 |
| 評估時長 | `eval_ticks` | 3000 / 8000 | 3000 / 8000 | 每個個體進行評估的持續時間 (ticks)。 |

這些超參數共同定義了兩種 DRL 方法的學習行為，是後續實驗分析與結果比較的重要基礎。 