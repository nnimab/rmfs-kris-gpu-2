# 3.4.1 深度Q網路 (DQN) 控制器設計

深度Q網路 (Deep Q-Network, DQN) 是本研究中用於建立交通控制策略的**比較基線 (Baseline)** 方法。作為深度強化學習領域的一項奠基性演算法，DQN 將深度神經網路與經典的 Q-Learning 相結合，使其能夠處理高維度的狀態空間。選擇 DQN 作為基線，是為了在一個公認的、穩定的 DRL 框架下，評估本研究提出的 NERL 方法所能帶來的改進。

DQN 的核心思想是學習一個動作價值函數 (Action-Value Function)，即 Q 函數。該函數 $Q(s, a; \theta)$ 使用一個由參數 $\theta$ 定義的神經網路來近似，其目標是預測在給定狀態 $s$ 下執行動作 $a$ 後，未來可能獲得的期望累積獎勵。最優的 Q 函數 $Q^*(s, a)$ 遵循貝爾曼最優方程 (Bellman Optimality Equation)：

$$
Q^*(s, a) = \mathbb{E}_{s' \sim P(\cdot|s,a)} \left[ r + \gamma \max_{a'} Q^*(s', a') \right]
$$

其中，$r$ 是立即獎勵，$\gamma$ 是折扣因子，代表未來獎勵的重要性，$s'$ 是後繼狀態。一旦學會了準確的 Q 函數，智能體在任何狀態 $s$ 下便可通過選擇使 $Q(s, a; \theta)$ 最大化的動作 $a$ 來執行最優策略。

### 1. 核心穩定性機制

為了應對使用非線性函數近似（如神經網路）時可能出現的訓練不穩定問題，本研究採用的 DQN 架構整合了兩種關鍵技術：

*   **經驗回放 (Experience Replay)**: 控制器與環境互動產生的轉換樣本 $(s_t, a_t, r_t, s_{t+1})$ 被儲存在一個固定大小的記憶體緩衝區 $\mathcal{D}$ 中。在訓練階段，演算法會從 $\mathcal{D}$ 中隨機採樣一小批 (minibatch) 的樣本進行學習，而非使用連續的時間序列樣本。這種做法打破了樣本之間的時序相關性，使得訓練數據更接近獨立同分佈 (i.i.d.) 的假設，從而顯著提高了訓練的穩定性。

*   **目標網路 (Target Network)**: 演算法會維護兩個獨立的神經網路。一個是**策略網路** (Policy Network)，其參數為 $\theta$，用於在每個時間步選擇動作。另一個是**目標網路** (Target Network)，其參數為 $\theta^-$。在計算時序差分 (Temporal Difference, TD) 目標時，目標 Q 值由目標網路計算得出，即 $y_i = r_i + \gamma \max_{a'} Q(s'_{i}, a'; \theta^-)$。目標網路的參數 $\theta^-$ 會定期（而非每個步驟）從策略網路的參數 $\theta$ 複製而來 ($\theta^- \leftarrow \theta$)。這種延遲更新的機制解耦了目標 Q 值與當前 Q 值之間的依賴關係，有效抑制了自舉 (bootstrapping) 過程中可能出現的震盪與發散。

### 2. 學習過程

DQN 的訓練是通過最小化一個隨機採樣的轉換樣本批次的損失函數來進行的。損失函數 $L_i(\theta_i)$ 定義為 TD 目標與策略網路輸出之間的均方誤差 (Mean Squared Error, MSE)：

$$
L_i(\theta_i) = \mathbb{E}_{(s, a, r, s') \sim U(\mathcal{D})} \left[ \left( \underbrace{r + \gamma \max_{a'} Q(s', a'; \theta_i^-)}_{\text{TD 目標}} - \underbrace{Q(s, a; \theta_i)}_{\text{當前 Q 值}} \right)^2 \right]
$$

該損失函數的梯度會通過隨機梯度下降 (SGD) 或其變體 (如 Adam) 更新策略網路的權重 $\theta_i$。

### 3. 神經網路架構

本研究中的 DQN 控制器所使用的策略網路與目標網路均為一個前饋神經網路 (Feed-Forward Neural Network)。其輸入層維度對應於在 **3.4.3 節** 中定義的狀態空間，輸出層維度則對應於 **3.4.4 節** 中定義的動作空間。網路包含兩個隱藏層，並使用 ReLU 作為激活函數，其具體架構如在**3.5.3 節** 有更明確的定義。

---
**【圖表建議：圖 3.4.1 - DQN 訓練與決策流程圖】**

建議在此處繪製一張圖，清晰地展示 DQN 的完整運作流程。圖中應包含以下兩個並行的循環：
1.  **決策循環 (Agent-Environment Interaction)**:
    *   從環境接收狀態 $s_t$。
    *   策略網路 $Q(s, a; \theta)$ 接收 $s_t$ 並輸出所有動作的 Q 值。
    *   使用 $\epsilon$-greedy 策略選擇動作 $a_t$。
    *   在環境中執行 $a_t$，獲得獎勵 $r_t$ 和新狀態 $s_{t+1}$。
    *   將轉換樣本 $(s_t, a_t, r_t, s_{t+1})$ 存入經驗回放記憶體 $\mathcal{D}$。
2.  **訓練循環 (Network Update)**:
    *   從 $\mathcal{D}$ 中隨機採樣一批轉換樣本。
    *   使用目標網路 $Q(s, a; \theta^-)$ 計算 TD 目標。
    *   計算策略網路 $Q(s, a; \theta)$ 的損失函數。
    *   執行梯度下降以更新策略網路的權重 $\theta$。
    *   定期將策略網路的權重複製到目標網路 ($\theta^- \leftarrow \theta$)。 