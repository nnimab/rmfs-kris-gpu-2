# 3.5.2 模型訓練流程

為確保 DRL 模型能夠充分學習並收斂到一個較優的策略，同時保證不同模型之間比較的公平性，本研究設計了一套標準化的模型訓練流程。此流程詳細規定了從模型初始化到最終模型保存的每一個步驟。

### 1. DQN 訓練流程 (對應組別 I, J)

DQN 的訓練是一個線上（On-policy）持續學習的過程。單次完整的 DQN 訓練實驗流程如下：

1.  **初始化**：
    a. 根據 `3.4.4` 節中定義的超參數及實驗組別指定的獎勵模式（`step` 或 `global`），創建一個 `DQNController` 實例。
    b. 創建一個模擬倉庫環境 `Warehouse` 的實例。

2.  **訓練迴圈**：
    a. 啟動一個持續 `N = 550,000` 個時間步（ticks）的模擬。
    b. 在每一個時間步 `t`，`IntersectionManager` 會遍歷所有交叉路口。
    c. 對於每一個交叉路口 `i`：
        i.   控制器從環境中獲取當前狀態 `s_t`。
        ii.  使用策略網路和 ε-greedy 策略選擇一個動作 `a_t`。
        iii. 執行動作 `a_t`，環境轉移到下一個狀態 `s_{t+1}`，並由 `UnifiedRewardSystem` 計算出即時獎勵 `r_t` (在 `global` 模式下此獎勵為 0)。
        iv.  將經驗元組 `(s_t, a_t, r_t, s_{t+1})` 存入經驗回放記憶體。
    d. **經驗回放**：每隔 `k=32` 個時間步，從記憶體中隨機抽樣一個批次（batch）的經驗進行學習。
    e. **目標網路更新**：每隔 `M=1,000` 個時間步，將策略網路的權重複製到目標網路。

3.  **模型保存**：訓練完全結束後，將最終的策略網路權重保存為最終模型。

### 2. NERL 訓練流程 (對應組別 A-H)

NERL 的訓練是一個世代迭代的離線（Off-policy）學習過程。其核心流程對所有 NERL 組別是統一的，但會根據具體組別的配置，代入不同的超參數。

1.  **初始化**：
    a. 根據 `3.4.4` 節以及**具體實驗組別**（A-H）的定義，創建一個 `NEController` 實例。此步驟將確定以下關鍵超參數：
        - **獎勵模式**: `step` 或 `global`。
        - **變異配置 (Variant)**: **A (探索型)** 或 **B (利用型)**，這將決定 `mutation_rate` 和 `mutation_strength` 的值。
        - **評估時長 (Eval Ticks)**: `3,000` 或 `8,000`。
    b. 控制器隨機初始化一個包含 `20` 個網路個體的族群。

2.  **演化迴圈**：
    a. 啟動一個持續 `G = 30` 個世代（Generations）的演化。
    b. 在每一個世代 `g`：
        i.   **並行評估 (Parallel Evaluation)**：為族群中的 `20` 個個體，分別啟動 `20` 個獨立、並行的模擬環境。
        ii.  每個個體 `j` 在其專屬的環境中運行一個完整的評估回合。回合的持續時間由該實驗組的 `eval_ticks` 參數決定（`3,000` 或 `8,000` ticks）。
        iii. 回合結束後，`UnifiedRewardSystem` 根據該實驗組指定的獎勵模式（`step` 或 `global`）計算出個體 `j` 的適應度分數 `f_j`。
        iv.  **演化操作**：當所有個體的適應度分數都計算完畢後，控制器根據該實驗組的變異配置（`A` 或 `B`）執行一次完整的演化操作（選擇、交叉、變異），生成一個全新的子代族群。
        v.   新的子代族群將成為下一個世代 `g+1` 的起始族群。

3.  **模型保存**：
    a. 在每一個世代結束時，演算法都會將該世代中適應度最高的個體保存為當代最佳模型。
    b. 在所有 `30` 個世代的演化完全結束後，從所有世代的最佳模型中，選出那個擁有歷史最高適應度分數的模型，將其作為該實驗組的最終模型。 