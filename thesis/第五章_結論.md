# 5.1 研究總結

本研究旨在解決現代自動化倉儲機器人移動履行系統 (RMFS) 中，因交通壅塞而導致運作效率低落的核心問題。為此，本論文聚焦於設計一套有效且能自我適應的智慧交叉路口交通控制策略，以期在最小化機器人無效等待與能源消耗的同時，最大化系統整體的訂單履行能力。

為達此目標，本研究首先建構了一個高擬真度的倉儲模擬環境，並設計了兩種基於傳統規則的控制器（時間基礎與佇列基礎）作為效能基線。接著，引進了深度強化學習 (DRL) 方法，分別實現了基於標準深度 Q 網路 (DQN) 的控制器，以及一種結合了神經演化與強化學習的 NERL 控制器。針對 NERL 模型，本研究進一步設計了涵蓋不同獎勵模式、探索強度與評估時長的多組對照實驗，以系統性地探討各項超參數對模型行為與效能的影響。

實驗結果的分析遵循一條從「訓練過程」到「最終驗證」的嚴謹路徑。在訓練過程的分析中，研究發現採用「全局獎勵」與「長時評估」的 NERL 模型（如 H 組）展現出學習複雜、宏觀策略的巨大潛力。然而，在更長、更具挑戰性的標準化最終驗證中，結果出現了關鍵性的轉折：前述在訓練中表現優異的模型，因其策略過度擬合訓練環境而顯得脆弱，泛化能力不足，最終效能並非最佳。

本研究的最終結論指出，綜合性能最優的控制器為 F 組 (NERL-Step, 低探索, 8000 Ticks)。該模型所採用的「步階獎勵、低探索性、長時評估」組合，使其學會了一套不過於冒進、兼具高訂單完成率與卓越能源效率的均衡策略。它的成功證明，在複雜的隨機動態環境中，一個策略的**穩健性**與**泛化能力**，是比在特定訓練條件下達成極致指標更為重要的特質。此發現不僅為本研究的核心問題提供了具體的解決方案，也為未來在真實世界部署 DRL 系統提供了深刻的實務性洞見。 

# 5.2 研究貢獻

本研究在理論、方法與實務應用層面，為自動化倉儲的交通控制領域帶來了數點具體貢獻。在理論層面，本論文最核心的貢獻之一，是透過嚴謹的對照實驗，系統性地揭示了深度強化學習模型在訓練效能與最終泛化能力之間的潛在鴻溝。研究明確指出，在訓練階段表現最優的策略（如全局獎勵驅動的複雜策略），在更長、更通用的驗證環境中可能因過擬合而失效。此發現為 DRL 領域提供了一個重要的實證案例，強調在評估模型時，必須將標準化的泛化能力測試作為最終的黃金標準，而非僅僅依賴訓練過程中的指標。

在方法論上，本研究成功地將神經演化強化學習 (NERL) 應用於複雜的 RMFS 交通控制問題，並通過實驗驗證，找到了一套能夠催生出兼具高產出與高能效的均衡型策略的訓練配置，即步階獎勵、低探索性與長時評估的組合。此成果不僅為 NERL 這種混合式 DRL 方法在物流自動化領域的應用提供了成功案例，更為未來研究者在設計 DRL 系統時，如何在「探索的廣度」與「學習的穩定性」之間進行權衡，提供了具體的實驗依據與啟示。

基於此方法論，本研究在實務應用層面也取得了顯著成果。我們所提出的 F 組模型 (NERL-Step, 低探索, 8000 Ticks) 在所有測試控制器中，展現了最佳的綜合性能，尤其是在能效比方面顯著優於傳統基線與標準 DQN 模型。這為倉儲營運商在追求提升自動化系統吞吐量的同時，如何有效控制日益增長的能源成本與碳足跡，提供了一個具備高度潛在應用價值的先進技術方案。

最後，本研究所有成果的取得，皆奠基於我們從頭建構的可擴展、高擬真度 RMFS 模擬平台。該平台不僅為本論文的各項實驗提供了堅實的基礎，其模組化的設計也使其具備良好的可擴展性，可供未來研究者用於測試更先進的演算法、不同的倉儲佈局或更複雜的任務調度策略，從而構成了推動整個領域發展的基礎設施貢獻。 

# 5.3 研究限制

儘管本研究透過嚴謹的實驗設計得出了一系列重要結論，但仍需正視其存在的若干限制，這些限制不僅界定了本研究結論的適用範圍，也為未來的研究工作提供了明確的探索方向。

首先，本研究的核心限制之一在於模擬環境與物理現實之間必然存在的差距 (Sim-to-Real Gap)。本研究的所有實驗雖均在一個高擬真度的模擬平台中進行，但該平台終究無法完全捕捉真實物理環境中的所有隨機性與複雜性，例如硬體磨損、感測器延遲或電池老化等未被建模的因素，這些都可能影響控制器在實際部署時的效能。因此，本研究得出的最優模型，其在真實物理環境中的表現仍需經過實地驗證與校準。

其次，本研究的結論建立在一個固定的倉儲佈局與路網結構之上，這構成了其結論在推廣性上的另一限制。雖然所選佈局具有代表性，但本研究關於最優控制器及其訓練配置的結論，是否能直接泛化至其他具有不同拓撲結構的倉儲，仍有待進一步的實證研究，因為不同的路網結構可能會催生出截然不同的交通瓶頸與動態特性。

再者，為了聚焦於交叉路口的交通控制核心問題，本研究對任務與交通模型進行了必要的簡化。例如，未考慮更複雜的訂單結構、機器人間的協同作業，或將揀貨站擁堵作為內生學習目標。這些簡化有助於隔離變量進行分析，但也意味著本研究的控制器尚未應對更高層次的系統性挑戰。

最後，本研究在探索的演算法廣度上也存在限制。雖然比較了多種代表性的控制器，但近年來深度強化學習領域發展迅速，本研究未能將如近端策略優化 (PPO) 或軟性致動評論家 (SAC) 等同樣具有潛力的先進演算法納入橫向比較，這些演算法在樣本效率或策略穩定性上可能表現出不同的特性。 

# 5.4 未來工作

基於本研究的發現與前述的若干限制，未來的研究工作可在數個方向上進行深入探索，以期進一步推動智慧倉儲交通控制技術的發展。

首先，為應對模擬與現實的差距 (Sim-to-Real Gap)，一個關鍵的未來方向是研究如何將在模擬環境中訓練好的高效能模型，以低成本、高效率的方式遷移並部署到真實的物理機器人系統中。這可能涉及領域自適應 (Domain Adaptation) 技術、在少量真實數據上進行模型微調 (Fine-tuning)，或是研究對物理參數變化更不敏感的魯棒性強化學習演算法，以提升模型的實地部署可行性。

其次，針對本研究在單一倉儲佈局下進行的限制，未來的研究應當系統性地評估本研究最優控制器在多樣化拓撲結構中的泛化能力。更進一步地，可以研究如何讓 DRL 智能體在訓練時就接觸到多樣化的佈局，從而學習到一種能夠自主適應不同路網結構的、更為通用的交通控制元策略 (Meta-Policy)，這對於提升演算法的通用性至關重要。

再者，本研究主要聚焦於路口的微觀交通控制，因此一個極具價值的拓展方向是，將本研究的交通控制器作為一個底層執行模組，向上整合一個更高層次的智慧決策系統。該系統可負責更宏觀的任務分配與動態路徑規劃，甚至將揀貨站擁堵等系統性因素納入考量，藉此探索分層式或多智能體協同決策架構，以實現系統全局的深層次優化。

此外，為擴展本研究在演算法探索上的廣度，未來的研究應當引入如近端策略優化 (PPO)、軟性致動評論家 (SAC) 等在其他領域已證明其高效性的現代 DRL 演算法。將這些演算法與本研究中表現最優的 NERL 模型進行全面的性能比較，將有助於更清晰地定位不同演算法在解決 RMFS 交通問題上的優劣勢。

最後，為了更貼近綠色物流的目標，未來的模型可以整合更精細的能源動態模型，例如將電池的充放電循環壽命、不同速度下的能量消耗曲線等因素納入獎勵函數的設計中，以驅使智能體學習到一種不僅節能，而且有助於延長硬體使用壽命的、更具可持續性的運作策略。 