# 3.4.1 深度 Q 網路 (DQN) 架構設計

深度 Q 網路（DQN）是本研究採用的第一種 DRL 方法。它是一種基於價值的演算法，其核心目標是學習一個稱為「Q 函數」的評估模型。Q 函數 \\(Q(s, a)\\) 的作用是預測在給定狀態 \\(s\\) 下，執行動作 \\(a\\) 所能帶來的長期累積獎勵。通過學習一個準確的 Q 函數，智慧體便可以在任何狀態下，通過選擇具有最高 Q 值的動作來做出最優決策。

本研究實現的 `DQNController` 在經典 DQN 的基礎上進行了擴展，整合了經驗回放（Experience Replay）、目標網路（Target Network）等關鍵技術以提高學習的穩定性，並引入了一套混合式決策框架，將傳統規則與神經網路模型相結合。

### 1. 核心組件

`DQNController` 由以下幾個核心組件構成：
- **策略網路 (Policy Network)**：一個深度神經網路，用於近似 Q 函數。它的輸入是狀態 \\(s\\)，輸出是該狀態下每個可能動作的 Q 值。此網路的權重會在訓練過程中通過梯度下降不斷更新。
- **目標網路 (Target Network)**：一個與策略網路結構完全相同但參數不同步更新的神經網路。在計算目標 Q 值時，使用目標網路可以打破數據間的自相關性，有效防止學習過程中的震盪與發散。目標網路的權重會定期從策略網路複製而來，而非每個時間步都更新。
- **經驗回放記憶體 (Replay Memory)**：一個固定容量的佇列，用於存儲智慧體與環境互動的經驗元組 \\((s, a, r, s')\\)，其中 \\(s'\\) 是執行動作 \\(a\\) 後的新狀態，\\(r\\) 是獲得的即時獎勵。在訓練時，控制器會從記憶體中隨機抽樣一個小批次（minibatch）的經驗進行學習，這打破了經驗之間的時間相關性，使訓練數據更符合獨立同分布的假設，從而大幅提升了學習的穩定性與效率。
- **混合決策邏輯**：除了神經網路，`DQNController` 還內建了一套與 `QueueBasedController` 類似的防鎖死與優先級規則。在做出最終決策前，系統會先檢查是否存在諸如機器人等待時間超限、潛在死鎖等緊急情況。若觸發這些規則，則直接執行預設的避險操作；否則，才將決策權交給 DQN 模型。這種設計確保了系統在學習初期或面對極端情況時，依然能保持最基本的運作穩定性。

### 2. 狀態表示 (State Representation)

為了讓模型能夠做出明智的決策，必須為其提供一個資訊豐富的狀態向量。本研究設計了一個 17 維的狀態向量，全面地描述了一個路口的局部與周邊交通態勢：
- **路口自身狀態 (8 維)**：包括當前允許方向、信號持續時間、各方向排隊的機器人總數、高優先級任務機器人數量、以及各方向的平均等待時間。
- **相鄰路口狀態 (8 維)**：包括周圍有效鄰居的數量、鄰居路口的機器人總數、高優先級任務總數、平均等待時間、以及方向分佈等，為模型提供了更廣闊的視野。
- **全域系統狀態 (1 維)**：引入下游關鍵瓶頸——揀貨站的總排隊長度。這一全域資訊使得路口控制器能夠感知到下游的擁塞情況，從而做出更具預見性的決策，避免向上游傳播擁堵。

在將這些原始特徵輸入網路之前，我們會使用一個自適應正規化器（`AdaptiveNormalizer`）對其進行標準化處理，以確保不同尺度特徵的均衡性，加速模型的收斂。

### 3. 動作空間 (Action Space)

控制器在每個決策點可以選擇的動作集合。為簡化問題，我們定義了 3 個離散動作：
- **動作 0**: 保持當前方向不變。
- **動作 1**: 將通行權切換至水平方向。
- **動作 2**: 將通行權切換至垂直方向。

### 4. 神經網路架構

策略網路與目標網路均採用一個結構相同的前饋神經網路（Feed-Forward Neural Network）。該網路包含兩個隱藏層，具體架構如下：

**輸入層 (17) -> 全連接層 (128) -> ReLU 激活 -> 全連接層 (64) -> ReLU 激活 -> 輸出層 (3)**

這個相對精簡的架構在表達能力與計算效率之間取得了良好的平衡，足以處理我們定義的狀態-動作空間。

---
**【圖表建議：圖 3.4.1 - DQN 控制器架構與決策流程圖】**

建議在此處繪製一張流程圖，清晰地展示 `DQNController` 的完整決策流程。圖中應包含以下步驟：
1.  接收到路口狀態 `s`。
2.  進入混合決策模塊，檢查防鎖死規則。
3.  若觸發規則，則直接輸出確定性動作。
4.  若未觸發，則將狀態 `s` 傳遞給策略網路。
5.  策略網路輸出各動作的 Q 值。
6.  通過 Epsilon-Greedy 策略選擇最終動作 `a`。
7.  同時展示經驗 \\((s, a, r, s')\\) 被存入經驗回放記憶體的過程。
8.  獨立地展示從記憶體中採樣進行網路訓練的迴圈。 