# 3.5.1 深度Q網路 (DQN) 架構設計

深度Q網路 (DQN) 是本研究採用的核心DRL方法之一。它是一種基於價值的 (value-based) 演算法，旨在學習一個最優的動作價值函數 (action-value function)，即Q函數。Q函數 $Q(s, a)$ 用於預測在給定狀態 $s$ 下，執行特定動作 $a$ 後所能獲得的未來累積獎勵的期望值。通過學習一個準確的Q函數，智能體便能遵循一個貪婪策略，在任何狀態下選擇具有最高Q值的動作，從而做出最優決策。

本研究實現的 `DQNController` 在經典DQN的基礎上進行了擴展，整合了經驗回放 (Experience Replay) 與目標網路 (Target Network) 等關鍵技術以提升學習穩定性。

### 1. 核心組件

- **策略網路 (Policy Network)**: 用於近似Q函數 $Q(s, a; \theta)$ 的深度神經網路，其參數為 $\theta$。它接收狀態 $s$ 作為輸入，輸出該狀態下每個可能動作的Q值。此網路的權重會在訓練過程中通過梯度下降法持續更新。
- **目標網路 (Target Network)**: 一個與策略網路結構完全相同但參數 $\theta^-$ 不同步更新的神經網路。在計算損失函數所需的目標Q值時，使用固定的目標網路可以打破數據間的自相關性，有效防止學習過程中的震盪與發散。目標網路的權重會定期從策略網路複製而來 ($\theta^- \leftarrow \theta$)，而非每個時間步都更新。
- **經驗回放記憶體 (Replay Memory)**: 一個固定容量的先進先出佇列，用於存儲智能體與環境互動的經驗元組 $(s, a, r, s')$。在訓練時，控制器會從記憶體中隨機抽樣一個小批次 (minibatch) 的經驗進行學習，這打破了經驗之間的時間相關性，使訓練數據更符合獨立同分布的假設，從而大幅提升了學習的穩定性與效率。

### 2. 神經網路架構

策略網路與目標網路均採用一個結構相同的前饋神經網路 (Feed-Forward Neural Network)，其詳細架構如下表所示。

**【表格建議：表 3.5.1 - DQN 神經網路架構】**

| 層級 (Layer) | 類型 (Type) | 輸入維度 (Input Dim) | 輸出維度 (Output Dim) | 激活函數 (Activation) |
| :--- | :--- | :--- | :--- | :--- |
| 1 | 輸入層 (Input) | 17 | 17 | - |
| 2 | 全連接層 (FC 1) | 17 | 128 | ReLU |
| 3 | 全連接層 (FC 2) | 128 | 64 | ReLU |
| 4 | 輸出層 (Output) | 64 | 6 | - |

這個相對精簡的架構在表達能力與計算效率之間取得了良好的平衡，足以處理我們在 3.4.3 和 3.4.4 節中定義的狀態-動作空間。

### 3. 核心訓練參數

DQN的學習過程受到一系列關鍵超參數的影響，這些參數共同決定了模型的收斂速度與最終性能。本研究中使用的核心訓練參數如下表所列。

**【表格建議：表 3.5.2 - DQN 核心訓練超參數】**

| 參數 (Hyperparameter) | 符號 (Symbol) | 數值 (Value) | 描述 |
| :--- | :--- | :--- | :--- |
| 學習率 (Learning Rate) | $\alpha$ | $10^{-4}$ | 控制網路權重更新步長的優化器參數。 |
| 折扣因子 (Discount Factor) | $\gamma$ | 0.99 | 衡量未來獎勵相對於即時獎勵重要性的因子。 |
| 經驗回放記憶體大小 | - | 50,000 | 存儲經驗元組的最大容量。 |
| 批次大小 (Batch Size) | - | 8192 | 每次從記憶體中採樣用於訓練的經驗數量。 |
| Epsilon-Greedy 策略 | $\epsilon$ | - | 採用 $\epsilon$-遞減策略，$\epsilon$ 從 1.0 線性衰減至 0.01。 |
| 目標網路更新頻率 | - | 1,000 ticks | 每隔1,000個訓練步，將策略網路的權重複製到目標網路。 |

---
**【圖表建議：圖 3.5.1 - DQN 控制器架構與決策流程圖】**

建議在此處繪製一張流程圖，清晰地展示 `DQNController` 的完整決策流程。圖中應包含以下步驟：
1.  接收到路口狀態 $s$。
2.  狀態 $s$ 經過正規化後傳遞給策略網路。
3.  策略網路輸出各動作的 Q 值。
4.  通過 Epsilon-Greedy 策略選擇最終動作 $a$。
5.  智能體執行動作 $a$，從環境獲得獎勵 $r$ 和新狀態 $s'$。
6.  將經驗元組 $(s, a, r, s')$ 存入經驗回放記憶體。
7.  獨立地展示從記憶體中隨機採樣小批次經驗，用於計算損失並更新策略網路權重的訓練迴圈。
8.  獨立地展示目標網路的延遲更新機制。 