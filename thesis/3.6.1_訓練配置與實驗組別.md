# 3.6.1 實驗設計與組別定義

為系統性地評估不同交通控制策略的效能，本研究設計了一套包含十二個獨立實驗組的對照實驗。此設計旨在將本研究提出的兩種深度強化學習方法（DQN 和 NERL），在不同獎勵模式與超參數配置下的表現，與兩種啟發式基線控制器進行全面的比較。

### 實驗組別定義

所有實驗組均在 **3.2.1 節** 所述的標準化倉儲模擬環境下運行，唯一的變數為交叉路口所採用的交通控制器及其特定配置。各實驗組的詳細定義如下表所示。

**表 3.6.1：實驗組別定義與說明**

| 組別 | 控制器類型 | 獎勵模式 | NERL 變體 | NERL 評估時長 (ticks) | 類別 | 說明 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 1 | `TimeBased` | - | - | - | 基線 | 固定時間週期的靜態控制器 |
| 2 | `QueueBased` | - | - | - | 基線 | 基於即時佇列長度的動態反應式控制器 |
| 3 | `DQN` | `step` | - | - | DRL | 使用步階獎勵訓練的 DQN (詳細參數見 3.6.2 節) |
| 4 | `DQN` | `global` | - | - | DRL | 使用全域獎勵訓練的 DQN (詳細參數見 3.6.2 節) |
| 5 | `NERL` | `step` | **A (探索型)** | 3,000 | DRL | 高變異率，短評估時長 (詳細參數見 3.6.2 節) |
| 6 | `NERL` | `global` | **A (探索型)** | 3,000 | DRL | 高變異率，短評估時長 (詳細參數見 3.6.2 節) |
| 7 | `NERL` | `step` | **B (利用型)** | 3,000 | DRL | 低變異率，短評估時長 (詳細參數見 3.6.2 節) |
| 8 | `NERL` | `global` | **B (利用型)** | 3,000 | DRL | 低變異率，短評估時長 (詳細參數見 3.6.2 節) |
| 9 | `NERL` | `step` | **A (探索型)** | 8,000 | DRL | 高變異率，長評估時長 (詳細參數見 3.6.2 節) |
| 10 | `NERL` | `global` | **A (探索型)** | 8,000 | DRL | 高變異率，長評估時長 (詳細參數見 3.6.2 節) |
| 11| `NERL` | `step` | **B (利用型)** | 8,000 | DRL | 低變異率，長評估時長 (詳細參數見 3.6.2 節) |
| 12| `NERL` | `global` | **B (利用型)** | 8,000 | DRL | 低變異率，長評估時長 (詳細參數見 3.6.2 節) |

### NERL 變體參數詳解

為探究演化過程中「探索」(Exploration) 與「利用」(Exploitation) 之間的平衡對最終策略效能的影響，本研究設計了兩種具有不同演化超參數的 NERL 變體，其核心差異在於變異操作的設置：

- **變體 A (探索型)**：此配置旨在促進在參數空間中的廣泛搜索。其設定具有較高的變異率 (`mutation_rate = 0.2`) 和較大的變異強度 (`mutation_strength = 0.1`)。這使得子代個體有更大的潛力跳出現有解的鄰域，發現全新的、可能更優的策略，但也可能帶來收斂速度較慢的風險。

- **變體 B (利用型)**：此配置專注於對已發現的較優解進行精細微調。其設定採用較低的變異率 (`mutation_rate = 0.1`) 和較小的變異強度 (`mutation_strength = 0.05`)。這種保守的變異策略有助於策略的穩定收斂，但亦可能增加了陷入局部最優的風險。

此外，為研究個體策略評估的充分性對學習效果的影響，每個 NERL 變體都將分別在 `3,000` ticks 和 `8,000` ticks 兩種評估時長下進行訓練與評估。

### 硬體與軟體配置

為確保實驗結果的一致性與可複現性，所有實驗均在標準化的環境下執行。詳細的硬體與軟體堆疊資訊已在 **3.2.3 節** 中進行了闡述。 