# 3.5.1 訓練配置與實驗組別

為系統性地評估不同交通控制策略的效能，本研究設計了一套包含十二個獨立實驗組的對照實驗矩陣。此設計旨在將本研究提出的兩種深度強化學習方法（DQN 和 NERL），在不同配置下的表現，與兩種基線控制器進行全面的比較。

### 實驗組別定義

所有實驗組將在完全相同的模擬環境（如 3.2.1 節所述）下運行，唯一的變數為交叉路口所採用的交通控制器及其配置。實驗組別定義如下表所示。

**表 3.5.1：實驗組別定義與說明**

| 組別 | 控制器類型 | 獎勵模式 | NERL 變體 | NERL 評估時長 | 類別 | 說明 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 1 | `TimeBased` | - | - | - | 基線 | 固定時間週期的靜態控制器 |
| 2 | `QueueBased` | - | - | - | 基線 | 基於即時隊列長度的動態反應式控制器 |
| 3 | `DQN` | `step` | - | - | DRL | 使用即時獎勵訓練的深度 Q 網路 |
| 4 | `DQN` | `global` | - | - | DRL | 使用全局獎勵訓練的深度 Q 網路 |
| 5 | `NERL` | `step` | **A (探索型)** | 3,000 | DRL | 高變異率，鼓勵探索新策略 |
| 6 | `NERL` | `global` | **A (探索型)** | 3,000 | DRL | 高變異率，鼓勵探索新策略 |
| 7 | `NERL` | `step` | **B (利用型)** | 3,000 | DRL | 低變異率，精細優化現有策略 |
| 8 | `NERL` | `global` | **B (利用型)** | 3,000 | DRL | 低變異率，精細優化現有策略 |
| 9 | `NERL` | `step` | **A (探索型)** | 8,000 | DRL | 高變異率，延長評估時間 |
| 10 | `NERL` | `global` | **A (探索型)** | 8,000 | DRL | 高變異率，延長評估時間 |
| 11| `NERL` | `step` | **B (利用型)** | 8,000 | DRL | 低變異率，延長評估時間 |
| 12| `NERL` | `global` | **B (利用型)** | 8,000 | DRL | 低變異率，延長評估時間 |

### NERL 變體參數詳解

為探究演化過程中「探索」（Exploration）與「利用」（Exploitation）之間的平衡對最終策略效能的影響，本研究設計了兩種具有不同演化超參數的 NERL 變體：
- **變體 A (探索型)**：此配置具有更高的變異率（`mutation_rate=0.3`）和變異強度（`mutation_strength=0.2`）。較大的變異幅度和頻率使得族群能夠在參數空間中進行更廣泛的搜索，有潛力發現更具創新性的解決方案，但可能面臨收斂較慢的風險。
- **變體 B (利用型)**：此配置採用較低的變異率（`mutation_rate=0.1`）和變異強度（`mutation_strength=0.05`）。較小的變異使得演化過程更傾向於在已發現的較優解附近進行精細微調，有助於策略的穩定收斂，但可能陷入局部最優。

此外，為研究個體評估時間的充分性對學習效果的影響，每個 NERL 變體都將分別在 `3,000` ticks 和 `8,000` ticks 兩種評估時長（`eval_ticks`）下進行訓練。

### 硬體與軟體配置

為確保實驗結果的一致性與可複現性，所有實驗均在標準化的環境下執行。詳細的硬體與軟體堆疊資訊已在 `3.2.3 節` 中進行了闡述。總結而言，計算密集型的模型訓練與評估主要在配備 NVIDIA GeForce RTX 4090 GPU 的雲端伺服器上進行，所有實驗均使用 `Python 3.10` 和 `PyTorch 2.1.0` 作為核心開發與執行環境。 