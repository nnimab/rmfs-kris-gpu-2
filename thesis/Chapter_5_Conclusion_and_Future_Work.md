# Chapter 5: Conclusion and Future Work

## 5.1 Research Summary
This research aimed to solve the core problem of reduced operational efficiency due to traffic congestion in modern automated Robotic Mobile Fulfillment Systems (RMFS). To this end, this thesis focused on designing an effective and adaptive intelligent intersection traffic control strategy, with the goal of maximizing the system's overall order fulfillment capability while minimizing robot idle waiting and energy consumption.

To achieve this goal, the study first constructed a high-fidelity warehouse simulation environment and designed two traditional rule-based controllers (Time-Based and Queue-Based) as performance baselines. Subsequently, Deep Reinforcement Learning (DRL) methods were introduced, implementing controllers based on the standard Deep Q-Network (DQN) and a NERL controller that combines neuroevolution with reinforcement learning. For the NERL model, this study further designed multiple sets of comparative experiments covering different reward schemes, exploration strengths, and evaluation durations to systematically investigate the impact of various hyperparameters on model behavior and performance.

The analysis of the experimental results followed a rigorous path from "training process" to "final validation." The analysis of the training process revealed that NERL models using "global reward" and "long evaluation" (such as Group H) showed great potential for learning complex, macroscopic strategies. However, a critical turning point occurred in the longer, more challenging standardized final validation: the aforementioned models that performed excellently in training proved to be fragile due to their policies overfitting the training environment, exhibiting insufficient generalization ability and ultimately not achieving the best performance.

The final conclusion of this study indicates that the controller with the best comprehensive performance is Group F (NERL-Step, Low-Explore, 8000 Ticks). The "step reward, low exploration, long evaluation" combination used by this model enabled it to learn a balanced policy that is not overly aggressive and possesses both a high order completion rate and excellent energy efficiency. Its success demonstrates that in a complex, stochastic, and dynamic environment, the **robustness** and **generalization ability** of a policy are more important traits than achieving extreme metrics under specific training conditions. This finding not only provides a concrete solution to the core problem of this research but also offers profound practical insights for deploying DRL systems in the real world in the future.

## 5.2 Research Contributions
This research has made several specific contributions to the field of traffic control in automated warehousing at the theoretical, methodological, and practical application levels. At the theoretical level, one of the core contributions of this thesis is the systematic revelation of the potential gap between the training performance and final generalization ability of deep reinforcement learning models through rigorous comparative experiments. The study clearly points out that policies that perform optimally during the training phase (such as complex strategies driven by global rewards) may fail in longer, more general validation environments due to overfitting. This finding provides an important empirical case for the DRL field, emphasizing that when evaluating models, standardized generalization ability tests must be the ultimate gold standard, rather than relying solely on metrics from the training process.

Methodologically, this study successfully applied Neuroevolution Reinforcement Learning (NERL) to the complex RMFS traffic control problem and, through experimental validation, identified a training configuration capable of producing a balanced policy with both high throughput and high energy efficiencyâ€”namely, the combination of step rewards, low exploration, and long evaluation duration. This achievement not only provides a successful case for the application of hybrid DRL methods like NERL in the field of logistics automation but also offers concrete experimental evidence and inspiration for future researchers on how to balance the "breadth of exploration" and "stability of learning" when designing DRL systems.

Based on this methodology, this study also achieved significant results in practical applications. The Group F model (NERL-Step, Low-Explore, 8000 Ticks) we proposed demonstrated the best comprehensive performance among all tested controllers, especially showing a significantly superior energy-to-performance ratio compared to traditional baselines and standard DQN models. This provides a high-potential advanced technology solution for warehouse operators on how to effectively control rising energy costs and carbon footprints while pursuing increased throughput in automated systems.

Finally, all achievements of this research are built upon the scalable, high-fidelity RMFS simulation platform we constructed from scratch. This platform not only provided a solid foundation for all experiments in this thesis but its modular design also gives it good extensibility, allowing future researchers to use it for testing more advanced algorithms, different warehouse layouts, or more complex task scheduling strategies, thereby constituting an infrastructural contribution to the advancement of the entire field.

## 5.3 Research Limitations
Although this study has drawn a series of important conclusions through rigorous experimental design, it is still necessary to acknowledge its existing limitations. These limitations not only define the scope of applicability of the study's conclusions but also provide clear directions for future research.

First, one of the core limitations of this study is the inevitable gap between the simulation environment and physical reality (Sim-to-Real Gap). Although all experiments in this study were conducted on a high-fidelity simulation platform, this platform ultimately cannot fully capture all the randomness and complexity of a real physical environment. Factors not modeled, such as hardware wear and tear, sensor delays, or battery aging, could all affect the controller's performance during actual deployment. Therefore, the performance of the optimal model identified in this study in a real physical environment still requires on-site validation and calibration.

Second, the conclusions of this study are based on a fixed warehouse layout and network structure, which constitutes another limitation on the generalizability of its findings. Although the chosen layout is representative, whether the conclusions of this study regarding the optimal controller and its training configuration can be directly generalized to other warehouses with different topological structures remains to be confirmed by further empirical research, as different network structures may give rise to vastly different traffic bottlenecks and dynamic characteristics.

Furthermore, to focus on the core problem of intersection traffic control, this study made necessary simplifications to the task and traffic models. For example, more complex order structures, collaborative operations between robots, or treating picking station congestion as an endogenous learning objective were not considered. These simplifications help to isolate variables for analysis but also mean that the controller in this study has not yet addressed higher-level systemic challenges.

Finally, this study is also limited in the breadth of algorithms explored. Although it compared several representative controllers, the field of deep reinforcement learning is rapidly evolving. This study was unable to include other promising advanced algorithms like Proximal Policy Optimization (PPO) or Soft Actor-Critic (SAC) in the comparison, which might exhibit different characteristics in terms of sample efficiency or policy stability.

## 5.4 Future Work
Based on the findings of this study and the aforementioned limitations, future research can be pursued in several directions to further advance the technology of intelligent warehouse traffic control.

First, to address the Sim-to-Real Gap, a key future direction is to investigate how to transfer and deploy high-performance models trained in a simulation environment to real physical robot systems in a low-cost and efficient manner. This could involve domain adaptation techniques, fine-tuning models on a small amount of real data, or researching robust reinforcement learning algorithms that are less sensitive to changes in physical parameters, to enhance the feasibility of real-world deployment.

Second, addressing the limitation of conducting the study in a single warehouse layout, future research should systematically evaluate the generalization ability of this study's optimal controller across diverse topological structures. Furthermore, one could investigate how to expose the DRL agent to a variety of layouts during training to learn a more universal traffic control meta-policy that can autonomously adapt to different network structures, which is crucial for improving the algorithm's versatility.

Moreover, as this study focused on microscopic traffic control at intersections, a valuable extension would be to integrate this study's traffic controller as a low-level execution module into a higher-level intelligent decision-making system. Such a system could be responsible for more macroscopic task allocation and dynamic path planning, and even consider systemic factors like picking station congestion. This would allow for the exploration of hierarchical or multi-agent collaborative decision-making architectures to achieve deeper, system-wide optimization.

In addition, to broaden the scope of algorithmic exploration, future research should introduce modern DRL algorithms that have proven effective in other domains, such as Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC). A comprehensive performance comparison of these algorithms against the best-performing NERL model from this study would help to more clearly identify the strengths and weaknesses of different algorithms in solving the RMFS traffic problem.

Finally, to better align with the goals of green logistics, future models could integrate more refined energy dynamics models. For example, incorporating factors like battery charge/discharge cycle life and energy consumption curves at different speeds into the reward function design could drive the agent to learn a more sustainable operational strategy that is not only energy-efficient but also helps to extend hardware lifespan. 