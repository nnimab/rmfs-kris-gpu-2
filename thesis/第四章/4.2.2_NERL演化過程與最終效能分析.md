# 4.2.2 NERL 演化過程與最終效能分析

本節旨在深入剖析 NERL 控制器在不同配置下的訓練動態及其最終效能。我們將從兩個維度展開分析：首先，通過觀察單一實驗中**精英個體**的演化軌跡，理解模型如何在多個績效指標（KPIs）之間進行學習與權衡；其次，通過**跨實驗組比較**各組訓練出的最終模型的效能，以確定最優的超參數配置。

### 1. 精英個體的 KPI 演化軌跡

為了理解模型在演化過程中的詳細行為，我們不僅僅關注抽象的「適應度」分數，而是追蹤每一代中表現最好的個體（即精英個體）在一系列關鍵績效指標上的具體表現。圖 4.2.1 以 `A_nerl_step_a3000ticks`（高探索性，步階獎勵，短評估）為例，展示了其精英個體的 KPI 演化軌跡。

![圖 4.2.1：A_nerl_step_a3000ticks 精英個體 KPI 演化過程](analysis_results/NERL_Elite_KPI_Evolution_A_nerl_step_a3000ticks.png)
*圖 4.2.1：`A_nerl_step_a3000ticks` 實驗組中，每一代精英個體在六個關鍵績效指標上的表現。虛線為五代移動平均線，用於展示平滑後的趨勢。*

從圖中可以觀察到以下幾點：
*   **多目標的同步優化**：隨著適應度分數（Best Fitness）的穩步提升，我們可以看到多個期望的業務指標也在同步改善。例如，訂單完成率（Completion Rate）呈現明顯的上升趨勢，而每單能耗（Energy per Order）則穩步下降。這表明演化過程成功地將抽象的適應度分數轉化為了實際的、多維度的效能提升。
*   **學習過程中的權衡（Trade-offs）**：模型的學習並非一帆風順。在約第10至15代，`total_stop_go_events` 和 `signal_switch_count` 出現了峰值，這可能意味著模型在早期為了提升訂單完成率，採用了更為激進、頻繁變換信號燈的策略。而在後期，隨著策略的成熟，這兩個指標逐漸下降並趨於穩定，表明模型學會了在保持高完成率的同時，找到更平順、高效的交通控制方式。
*   **收斂性**：在約25代之後，所有KPI曲線都開始收斂，波動性顯著降低，表明精英策略已達到一個相對成熟和穩定的狀態。其他實驗組的精英演化過程也表現出類似的趨勢，證實了NERL框架在不同配置下均具有良好的收斂能力。

### 2. 最終模型效能比較

在確認了模型能夠有效學習後，我們將比較所有NERL實驗組訓練出的最終模型（即第30代的精英個體）的效能。這有助於我們識別不同超參數組合對控制器最終能力的影響。

#### 2.1 適應度分數與訂單完成率

![圖 4.2.2：最終模型在適應度與完成率上的表現](analysis_results/NERL_Final_Comparison_best_fitness.png)
*圖 4.2.2：各NERL實驗組最終模型的適應度分數比較。*

![圖 4.2.3：最終模型在訂單完成率上的表現](analysis_results/NERL_Final_Comparison_completion_rate.png)
*圖 4.2.3：各NERL實驗組最終模型的訂單完成率比較。*

從適應度和訂單完成率這兩個核心指標來看：
*   **評估時長的決定性影響**：無論採用何種獎勵模式或變異策略，使用 `8000 ticks` 進行長時評估的實驗組（帶有斜線填充的長條）在最終效能上**全面超越**了 `3000 ticks` 的短時評估組。這有力地證明了，更長的評估時間能讓策略的長期效益得到更充分的體現和優化，從而訓練出更優秀的模型。
*   **步階獎勵的優勢**：在同為長時評估的頂尖組別中，採用 `step` 獎勵的 `E` 組和 `F` 組在完成率上略微領先於採用 `global` 獎勵的 `G` 組和 `H` 組。這表明，雖然 `global` 獎勵更貼近宏觀目標，但 `step` 獎勵提供的更頻繁、穩定的學習信號，在實踐中似乎更有助於模型收斂到一個高產出的策略。

#### 2.2 效率與流暢度指標

![圖 4.2.4：最終模型在每單能耗上的表現](analysis_results/NERL_Final_Comparison_energy_per_order.png)
*圖 4.2.4：各NERL實驗組最終模型的每單能耗比較（越低越好）。*

![圖 4.2.5：最終模型在走走停停次數上的表現](analysis_results/NERL_Final_Comparison_total_stop_go_events.png)
*圖 4.2.5：各NERL實驗組最終模型的總走走停停次數比較（越低越好）。*

在能耗和交通流暢度方面：
*   **長時評估再次勝出**：`8000 ticks` 評估組在每單能耗和走走停停次數上，再次表現出明顯優勢，其數值普遍低於 `3000 ticks` 組。這意味著長時評估不僅提升了產出，也同時優化了運營效率和交通流的平順性。
*   **利用型變體（B）的節能潛力**：在頂尖的 `8000 ticks` 組別中，採用低變異率、偏向利用的變體B（`F`組和`H`組）在`Energy per Order`指標上表現最為出色。這可能是因為其保守的變異策略有助於對現有高效解進行精細微調，從而找到了能耗與效率的最佳平衡點。

#### 2.3 交通控制行為指標

![圖 4.2.6：最終模型在信號切換次數上的表現](analysis_results/NERL_Final_Comparison_signal_switch_count.png)
*圖 4.2.6：各NERL實驗組最終模型的信號燈切換次數比較。*

![圖 4.2.7：最終模型在平均路口擁堵度上的表現](analysis_results/NERL_Final_Comparison_avg_intersection_congestion.png)
*圖 4.2.7：各NERL實驗組最終模型的平均路口擁堵度比較。*

最後，從控制器的具體行為來看：
*   **`global` 獎勵的“智慧”**：在平均路口擁堵度指標上，採用 `global` 獎勵的 `G` 組和 `H` 組表現最佳，其擁堵程度最低。這揭示了 `global` 獎勵模式的一個重要優點：由於其獎勵信號基於系統的宏觀、長期表現，它似乎更能引導模型學會一種“顧全大局”的智慧，主動避免可能導致長期擁堵的決策，即使這在短期內可能不是最優的。

### 3. 綜合結論

綜合以上所有分析，我們可以得出關於NERL控制器訓練的幾個關鍵結論：
1.  **評估時長是關鍵**：延長評估時間（從3000 ticks到8000 ticks）是提升NERL控制器最終效能**最有效**的手段，其在產出、效率和流暢度等多個核心指標上都帶來了全面的、顯著的提升。
2.  **獎勵模式的權衡**：`step` 獎勵在引導模型達成高訂單完成率方面表現出色，而 `global` 獎勵則在優化系統長期流暢度、降低擁堵方面具有獨特優勢。這表明兩者各有千秋，未來的研究或可探索將兩者結合的混合獎勵模式。
3.  **變異策略的影響**：高探索性（變體A）與高利用性（變體B）的差異在最終效能上的體現並不如前兩者顯著，但在能耗等特定指標的精細優化上，利用型策略顯示出一定潛力。

總體而言，**`E_nerl_step_a8000ticks`** 和 **`F_nerl_step_b8000ticks`** 在多個指標上取得了最優的綜合表現，是本次研究中訓練出的最為強大的控制器。 