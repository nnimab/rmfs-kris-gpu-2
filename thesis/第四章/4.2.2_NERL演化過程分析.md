# 4.2.2 NERL演化過程分析

在驗證NERL控制器的最終效能之前，檢視其在訓練過程中的演化動態至關重要。這有助於我們理解模型是否有效學習、族群是否收斂，以及不同超參數配置（如變異率、獎勵模式、評估時長）對學習過程的具體影響。本節將以 `A_nerl_step_a3000ticks`（高探索性變體A，步階獎勵，3000 ticks評估時長）作為基準案例進行詳細分析，並與其他關鍵實驗組進行對比，以揭示不同配置下的演化特性。

### 1. 基準案例分析：高探索性變體 (A) 的演化曲線

圖 4.2.1 展示了 `A_nerl_step_a3000ticks` 實驗組在 30 個世代的演化過程中，其族群適應度分數的變化情況。

![圖 4.2.1：A_nerl_step_a3000ticks 演化過程](analysis_results/NERL_Evolution_A_nerl_step_a3000ticks.png)
*圖 4.2.1：`A_nerl_step_a3000ticks` 實驗組的演化過程。圖中顯示了每一代族群的最高、平均和最低適應度分數。*

從圖中可以觀察到幾個關鍵現象：

*   **持續學習與收斂趨勢**：藍線所代表的**平均適應度**在整個演化過程中呈現出明顯的上升趨勢，從初始世代的約 100,000 分穩定增長至最終世代的接近 240,000 分。這清晰地表明，基於演化策略的選擇、交叉和變異操作是有效的，族群的整體性能隨著世代的推進在不斷提升。

*   **族群多樣性的動態變化**：由淺藍色區域（最低與最高適應度之間的範圍）所表示的**族群多樣性**，在演化初期相對較寬。這得益於變體A所採用的較高變異率（`mutation_rate = 0.3`），促進了廣泛的參數空間探索。隨著演化的進行，該區域逐漸收窄，特別是在第 20 代之後，表明族群正從廣泛的「探索」階段過渡到對較優解的「利用」階段，逐步收斂到一個高效的策略附近。

*   **精英個體的持續進化**：綠色虛線代表每一代中**適應度最高的個體**。該曲線的穩步攀升，且始終位於平均適應度之上，證明了精英保留策略的有效性。它確保了每一代的最優基因能夠被傳承下去，並在此基礎上通過變異和交叉產生潛在的更優解。

總體而言，`A_nerl_step_a3000ticks` 的演化曲線符合預期，展示了一個健康的、從探索到利用的學習過程。

### 2. 對比分析：不同演化配置的影響

為進一步理解各項超參數的作用，我們將基準案例與其他三個具有代表性的實驗組進行比較。

#### 探索 vs. 利用 (變體 A vs. B)

圖 4.2.2 比較了高探索性變體 A (`A_nerl_step_a3000ticks`) 與低探索性、高利用性變體 B (`B_nerl_step_b3000ticks`) 的演化過程。

| 變體 A (高探索) | 變體 B (高利用) |
|:---:|:---:|
| ![A_nerl_step_a3000ticks](analysis_results/NERL_Evolution_A_nerl_step_a3000ticks.png) | ![B_nerl_step_b3000ticks](analysis_results/NERL_Evolution_B_nerl_step_b3000ticks.png) |
*圖 4.2.2：探索型（左）與利用型（右）NERL變體的演化過程對比。*

*   **收斂速度與多樣性**：變體 B（右圖）的適應度範圍（淺藍色區域）從一開始就比變體 A（左圖）更窄，且收斂速度更快。這符合其較低的變異率（`mutation_rate = 0.1`）設定，該設定專注於對現有較優解的精細微調。然而，這種快速收斂也可能帶來陷入局部最優的風險。
*   **最終適應度**：儘管變體 B 的收斂速度更快，但其最終達到的最高適應度（約 220,000）略低於變體 A（約 240,000）。這初步表明，在有限的世代內，給予更多的探索能力（變體 A）可能更有助於發現全局最優解。

#### 獎勵模式：步階 vs. 全域

圖 4.2.3 對比了在相同變異配置（變體 A）下，採用步階獎勵 (`step`) 與全域獎勵 (`global`) 的演化差異。

| 步階獎勵 (Step) | 全域獎勵 (Global) |
|:---:|:---:|
| ![A_nerl_step_a3000ticks](analysis_results/NERL_Evolution_A_nerl_step_a3000ticks.png) | ![C_nerl_global_a3000ticks](analysis_results/NERL_Evolution_C_nerl_global_a3000ticks.png) |
*圖 4.2.3：步階獎勵（左）與全域獎勵（右）下的演化過程對比。*

*   **學習信號的穩定性**：採用 `global` 獎勵的實驗組（右圖）其適應度曲線表現出比 `step` 獎勵組（左圖）更大的波動性。這是因為全域獎勵是一個更稀疏、延遲的信號，單個隨機事件（如機器人偶然的擁堵）可能對整個回合的最終得分產生巨大影響，從而導致世代間的適應度評估出現較大噪音。
*   **收斂趨勢**：儘管存在波動，`global` 組的平均適應度依然呈現上升趨勢，表明模型仍在學習。然而，其收斂過程顯然比 `step` 組更不穩定，需要更多的世代或更精細的參數調整才能達到穩定的高性能狀態。

#### 評估時長：3,000 vs. 8,000 Ticks

圖 4.2.4 比較了在相同變異配置（變體 A）和獎勵模式（`step`）下，不同評估時長的影響。

| 短評估 (3,000 Ticks) | 長評估 (8,000 Ticks) |
|:---:|:---:|
| ![A_nerl_step_a3000ticks](analysis_results/NERL_Evolution_A_nerl_step_a3000ticks.png) | ![E_nerl_step_a8000ticks](analysis_results/NERL_Evolution_E_nerl_step_a8000ticks.png) |
*圖 4.2.4：短評估時長（左）與長評估時長（右）下的演化過程對比。*

*   **適應度評估的準確性**：使用更長的 8,000 ticks 進行評估的實驗組（右圖），其演化曲線明顯比 3,000 ticks 的實驗組（左圖）更為平滑，世代間的波動更小。這表明更長的評估時間能夠讓系統達到更穩定的狀態，從而更準確地衡量一個策略的真實效能，減少了短期隨機事件的干擾。
*   **更高的適應度上限**：長評估時間的實驗組最終達到的適應度分數顯著更高。這不僅僅是因為評估時間更長，也意味著在更長的時間維度下，策略能夠完成更多訂單，其長期效益得以更充分地體現。

### 3. 演化過程小結

綜合以上分析，我們可以得出以下結論：
1.  **NERL能夠有效學習**：所有實驗組的平均適應度都呈現上升趨勢，證明了演化框架的有效性。
2.  **探索與利用的權衡**：較高的變異率（變體 A）有助於在演化後期發現更優的解，但代價是初期收斂較慢。
3.  **獎勵信號的品質至關重要**：步階獎勵提供了更穩定、平滑的學習信號，而全域獎勵雖然更符合宏觀目標，但其稀疏性給學習帶來了更大的挑戰和不確定性。
4.  **評估的充分性影響策略品質**：更長的評估時間能夠帶來更可靠的適應度評估和更優的最終策略。

這些從訓練過程中獲得的洞見，為我們在下一節中深入分析和理解各控制器最終的效能表現，提供了重要的理論基礎和解釋依據。 