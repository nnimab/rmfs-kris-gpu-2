# 4.4 本章總結

本章旨在透過一系列系統性的實驗，對基於規則的基線控制器、標準深度強化學習 (DQN) 控制器，以及多種神經演化強化學習 (NERL) 變體在複雜倉儲環境中的交通控制效能，進行深入的量化評估與比較。本章的分析遵循一條從「過程」到「結果」，從「訓練潛力」到「泛化實證」的嚴謹路徑。

首先，在 4.2 節中，本研究對深度學習模型的訓練與演化過程進行了剖析。分析結果表明，不同的獎勵模式、探索策略與評估時長，對模型的學習動態具有顯著影響。特別地，研究觀察到採用「全局獎勵」與「長時評估」的組合，如 H 組 (NERL-Global, 低探索, 8000 Ticks)，在訓練環境中展現了學習複雜、長遠策略的較高潛力。

然而，在 4.3 節的標準化最終效能驗證中，研究發現訓練階段的效能與最終表現存在顯著差異。此轉折凸顯了機器學習研究中的核心議題——**泛化能力 (Generalization)**。訓練中表現最優的模型，如 H 組，其所學策略因過於適應特定的訓練環境（即**過擬合, Overfitting**），而在更長、更通用的測試場景中表現不佳，暴露了其策略的「脆弱性」。

最終的比較結果清晰地表明：
1.  一個設計精良的啟發式控制器，如 K 組 (QueueBased)，憑藉其策略的**穩健性**，可在「訂單完成率」這一單項指標上取得最優值，但此優勢伴隨著較高的能源消耗。
2.  本研究中的綜合性能最優模型為 F 組 (NERL-Step, 低探索, 8000 Ticks)。該模型採用的訓練配置，使其學會了一種不過於激進、兼具穩定與效率的**均衡策略**。它不僅在訂單完成率上表現頂尖，更以顯著優於其他高產出模型的能源效率，在多目標權衡中展現出最佳的綜合性能。

總結而言，本章的實驗不僅是評選出一個最優模型，更重要的是揭示了在智慧倉儲控制這類複雜問題中，**一個能夠促進穩健、泛化學習的訓練框架，是將深度強化學習從理論潛力轉化為現實世界應用價值的關鍵**。F 組的成功證明了追求一個不僅「精巧」、而且「可靠」的解決方案的重要性。這些發現為本論文的最終結論提供了堅實的數據支持，也為未來相關研究指明了方向。 