# 4.2.1 NERL演化過程分析

在驗證NERL控制器的最終效能之前，有必要對其在訓練過程中的演化動態進行檢視。此分析有助於理解模型是否有效學習、族群是否收斂，以及不同超參數配置對學習過程的具體影響。本節將以 A 組 (NERL-Step, 高探索, 3000 Ticks) 作為基準案例，對其演化過程中的多個關鍵績效指標（KPI）進行量化的趨勢分析。

### 1. 基準案例分析：高探索性變體 (A) 的多維度演化趨勢

圖 4.2.1 展示了 A 組 (NERL-Step, 高探索, 3000 Ticks) 實驗組在 30 個世代的演化過程中，其精英個體在三個核心 KPI 上的表現：總體適應度（Best Fitness）、訂單完成率（Completion Rate）與單均能耗（Energy per Order）。

| (a) 適應度 (Best Fitness) | (b) 訂單完成率 (Completion Rate) | (c) 單均能耗 (Energy per Order) |
|:---:|:---:|:---:|
|在此插入圖片A|在此插入圖片B|在此插入圖片C|
*圖 4.2.1：A 組 (NERL-Step, 高探索, 3000 Ticks) 實驗組的精英個體在 (a)適應度、(b)訂單完成率、(c)單均能耗上的演化趨勢。*

根據趨勢分析，各項指標的演化斜率與趨勢評估如下：

- **適應度 (Best Fitness)**: `斜率 = +1594.24` (正向發展)
- **訂單完成率 (Completion Rate)**: `斜率 = +0.0008` (正向發展)
- **單均能耗 (Energy per Order)**: `斜率 = -2.69` (正向發展)
- **相位切換次數 (Signal Switch Count)**: `斜率 = +3.58` (逆向發展)

從數據與圖表可歸納出以下幾點：

1.  **有效的學習與優化**：如圖 4.2.1(a) 所示，精英個體的**適應度**呈現顯著的上升趨勢（斜率 `+1594`），證明演化算法有效引導模型朝向最大化獎勵函數（定義於 3.4.5 節）的目標演進。此宏觀分數的提升，是由具體的子目標所支撐。圖 4.2.1(b) 的**訂單完成率**雖有波動，但趨勢線依然為正向（斜率 `+0.0008`），而圖 4.2.1(c) 的**單均能耗**則表現出顯著的持續下降趨勢（斜率 `-2.69`）。這兩點共同說明，模型不僅學會了完成更多訂單，還學會了以更節能的方式完成，意味著獎勵函數的設計達到了預期效果。

2.  **策略的權衡與演化 (Trade-off & Evolution)**：一個值得關注的現象是**相位切換次數 (Signal Switch Count)** 的演化趨勢。儘管步階獎勵的設計中包含了對「相位切換」的懲罰項（$C_{\text{switch}}$），但數據顯示該指標的斜率卻是正向的（`+3.58`），即模型傾向於更頻繁地切換交通號誌。這並非學習的失敗，而是智能體自主決策的體現。此現象揭示了模型在學習過程中發現，**適度增加相位切換所帶來的局部懲罰，可換取交通流動性的顯著改善，從而獲得遠超該懲罰的、更大的全局利益（更高的訂單完成率與更低的等待時間）。** 這種為了宏觀最優而犧牲局部指標的複雜權衡行為，是傳統基於固定規則的控制器難以實現的。

3.  **探索的代價與價值**：從圖 4.2.1(b) 訂單完成率的劇烈波動可以看出，高探索性的 A 組在演化過程中，每一代都在嘗試不同的策略。部分世代的嘗試可能不成功（如第 9-10 代的大幅下降），但這種探索的廣度，是最終能夠找到高效且節能策略（如第 25 代之後的穩定高點）的基礎。

總結而言，對基準案例的分析表明，NERL 不僅能夠成功學習並優化多個核心 KPI，還能展現出複雜的策略權衡能力。

### 2. 對比分析：不同演化配置的影響

為進一步理解各項超參數的作用，我們將基準案例與其他代表性實驗組進行比較。

#### a. 獎勵模式：Step vs Global

在比較不同獎勵模式時，一個重要的前提是，不應直接比較其「適應度 (Fitness)」的絕對值或斜率，因為 `步階獎勵` 與 `全域獎勵` 的計算方式和數值尺度完全不同。

因此，本節的重點是分析這兩種獎勵模式作為訓練的驅動力，分別對共通的關鍵績效指標 (KPIs)——即最終的實際運作效能——產生了何種不同的影響。為此，我們將比較 A 組 (NERL-Step, 高探索, 3000 Ticks) 與 C 組 (NERL-Global, 高探索, 3000 Ticks)。

**【圖表建議：圖 4.2.2 - 不同獎勵模式下 (Step vs. Global) 關鍵績效指標的演化趨勢對比圖】**

為進行量化比較，表 4.2.1 彙總了兩個實驗組在核心產出與效率指標上的演化趨勢斜率。

| 實驗組 | 獎勵模式 | 關鍵績效指標 (KPI) | 演化趨勢斜率 | 趨勢評估 |
| :--- | :--- | :--- | :--- | :--- |
| **A (高探索, 3000 Ticks)** | **步階 (Step)** | **訂單完成率** | **`+0.000825`** | **正向發展** |
| C (高探索, 3000 Ticks) | 全域 (Global) | 訂單完成率 | `-0.000698` | 逆向發展 |
| **A (高探索, 3000 Ticks)** | **步階 (Step)** | **單均能耗** | **`-2.695533`** | **正向發展** |
| C (高探索, 3000 Ticks) | 全域 (Global) | 單均能耗 | `-0.720350` | 正向發展 |
*表 4.2.1：步階獎勵與全域獎勵在訂單完成率與單均能耗兩個KPI上的演化趨勢斜率對比。*

從數據分析中，可得出以下兩點結論：

1.  **步階獎勵對產出提升的引導性**：
    訂單完成率是衡量系統核心產出的直接指標。如表 4.2.1 所示，採用步階獎勵的 A 組，其完成率呈現正向發展趨勢（斜率 `+0.000825`），表明其密集的即時獎勵信號成功引導智能體學習到了能有效提升系統總體產出的策略。相比之下，採用全域獎勵的 C 組，其完成率趨勢線反而呈現負向發展（斜率 `-0.000698`）。此現象表明，在當前的訓練設定下，僅依靠稀疏、延遲的全域獎勵，智能體難以建立起從微觀決策到宏觀結果之間的有效信用分配，導致學習信號充滿噪音。

2.  **步階獎勵對效率優化的驅動力**：
    數據顯示，雖然兩個組別的單均能耗都在下降，但步階獎勵組 (A) 的下降斜率（`-2.69`）遠大於全域獎勵組 (C) 的斜率（`-0.72`）。這意味著步階獎勵不僅引導模型完成訂單，也驅使其以更節能的方式完成。

綜上所述，實驗證據支持步階獎勵模式在引導 NERL 控制器學習複雜倉儲任務時的有效性，它不僅能確保模型朝著提升系統總產出的方向演化，也能更高效地發掘運作細節中的優化潛力。

#### b. 探索策略的影響：高探索性 (A) vs. 低探索性 (B)

在神經演化中，族群的多樣性與個體的探索強度，是決定演算法能否跳出局部最優的關鍵。本節旨在探討 NERL 控制器中，探索策略超參數的設定對演化結果的影響。我們將比較兩個採用相同步階獎勵，但探索強度不同的實驗組：A 組 (NERL-Step, 高探索, 3000 Ticks) 與 B 組 (NERL-Step, 低探索, 3000 Ticks)。

**【圖表建議：圖 4.2.3 - 不同探索強度下 (高 vs. 低) 關鍵績效指標的演化趨勢對比圖】**

表 4.2.2 彙總了高、低兩種探索強度配置下的關鍵指標演化趨勢。

| 實驗組 | 探索強度 | 關鍵績效指標 (KPI) | 演化趨勢斜率 | 趨勢評估 |
| :--- | :--- | :--- | :--- | :--- |
| **A (Step, 3000 Ticks)** | **高** | **訂單完成率** | **`+0.000825`** | **正向發展** |
| B (Step, 3000 Ticks) | 低 | 訂單完成率 | `-0.000334` | 逆向發展 |
| **A (Step, 3000 Ticks)** | **高** | **適應度 (Fitness)** | **`+1594.24`** | **正向發展** |
| B (Step, 3000 Ticks) | 低 | 適應度 (Fitness) | `+693.45` | 正向發展 |
| A (Step, 3000 Ticks) | 高 | 單均能耗 | `-2.695533` | 正向發展 |
| B (Step, 3000 Ticks) | 低 | 單均能耗 | `-1.753562` | 正向發展 |
*表 4.2.2：高探索性 (A) 與低探索性 (B) 在核心KPI上的演化趨勢斜率對比。*

數據分析的核心結論為：**過低的探索性可能導致演化過早收斂至局部最優，甚至陷入無法提升系統產出的策略。**

1.  **對系統產出的影響**：
    如表 4.2.2 所示，高探索性的 A 組，其訂單完成率呈現上升趨勢（斜率 `+0.000825`）；然而，低探索性的 B 組，其完成率卻是下降的（斜率 `-0.000334`）。此現象說明在複雜的交通控制問題中，若智能體探索不足，不敢嘗試可能暫時降低效率的行為，則可能被困在低效的策略中。B 組的演化可能收斂到了一種「為了避免任何潛在碰撞風險而導致交通停滯」的次優策略，最終損害了系統的總體目標。

2.  **學習的廣度與潛力**：
    適應度（Fitness）的增長斜率也印證了此觀點。A 組的適應度增長速度（斜率 `+1594`）是 B 組（斜率 `+693`）的兩倍以上。這並不意味著 A 組的學習效率更高，而是其「學習的視野」更廣。更高的探索性允許族群在更廣闊的策略空間中進行搜索，雖然過程可能伴隨更大的波動，但這種廣度是發現高效策略的必要前提。B 組雖也在學習，但其搜索範圍過於狹窄，導致其優化潛力受到限制。

總結而言，此對比分析強調了在 NERL 框架中設定充足探索強度的重要性。對於需要解決複雜權衡問題的任務，給予演化過程足夠的自由度去探索，是通往高效、魯棒解決方案的必要過程。

#### c. 評估時長的影響：3000 Ticks vs. 8000 Ticks

評估時長（`evaluation_ticks`）決定了在演化過程中，每個智能體與環境互動以展現其策略優劣的時間。本節透過比較 A 組 (NERL-Step, 高探索, 3000 Ticks) 與 E 組 (NERL-Step, 高探索, 8000 Ticks)，分析評估時長對學習效果的實際影響。

**【圖表建議：圖 4.2.4 - 不同評估時長下 (3000 vs. 8000 ticks) 關鍵績效指標的演化趨勢對比圖】**

表 4.2.3 彙總了兩種評估時長配置下的關鍵指標演化趨勢。

| 實驗組 | 評估時長 | 關鍵績效指標 (KPI) | 演化趨勢斜率 | 趨勢評估 |
| :--- | :--- | :--- | :--- | :--- |
| **A (NERL-Step, 高探索)** | **3000** | **訂單完成率** | **`+0.000825`** | **正向發展** |
| E (NERL-Step, 高探索) | 8000 | 訂單完成率 | `-0.001454` | 逆向發展 |
| **A (NERL-Step, 高探索)** | **3000** | **單均能耗** | **`-2.695533`** | **正向發展** |
| E (NERL-Step, 高探索) | 8000 | 單均能耗 | `+0.064461` | 逆向發展 |
| **A (NERL-Step, 高探索)** | **3000** | **相位切換次數** | **`+3.583537`** | **逆向發展** |
| E (NERL-Step, 高探索) | 8000 | 相位切換次數 | `-3.322136` | 正向發展 |
*表 4.2.3：3000 Ticks (A) 與 8000 Ticks (E) 評估時長在核心KPI上的演化趨勢斜率對比。*

數據分析結果表明：**單純延長評估時間，可能導致學習過程惡化，並非越長越好。**

1.  **信用分配延遲與獎勵信號稀釋**：
    此為核心問題。在步階獎勵的設計中（見 3.4.5 節），智能體在每個時間步都會收到即時回饋。當評估時長從 3000 延長至 8000 ticks 時，一個完整 Episode 包含的時間步數大幅增加，導致任何具體的、有益的微觀動作所產生的獎勵，在漫長的總時長中被稀釋。智能體變得難以將最終結果與數千步之前的某個關鍵決策聯繫起來，此即強化學習中典型的「信用分配問題」。A 組的成功在於其相對較短的評估窗口，使得行為與回饋之間的因果鏈條更為清晰，學習信號更強。

2.  **短視懲罰的失效與策略漂移**：
    更長的時間窗口也可能使某些懲罰項失效。例如，對相位切換的懲罰 $C_{\text{switch}}$。在 3000 ticks 的窗口內，頻繁切換的代價顯著，智能體需在「切換的懲罰」與「流動性的增益」之間做出權衡（如 A 組斜率 `+3.58` 所示）。但在 8000 ticks 的尺度下，智能體可能發現，透過極力避免切換（如 E 組斜率 `-3.32` 所示）以累積微小獎勵，即使這會導致後期長期的交通癱瘓，其總體適應度依然可能較高。模型可能學會了一種「短視的保守策略」，為了逃避眼前的微小懲罰而犧牲了長期的、最終的系統目標。

綜上所述，此對比分析揭示了設定評估時長的一個重要原則：評估窗口必須與獎勵函數的設計及任務本身的時間尺度相匹配。一個長度適中的評估窗口，才能確保獎勵信號的有效性和學習過程的穩定性。

### 3. 訓練階段綜合效能比較

前述章節分析了不同超參數配置對模型**演化過程（斜率）**的影響。然而，演化過程的趨勢不完全等同於最終效能的優越性。本節將視角從「過程」轉向「初步結果」，透過對各實驗組在訓練結束時的**精英模型**在各自訓練評估場景下的表現進行橫向比較，以檢視不同策略組合在訓練結束時達成的初步效能，並為下一節更嚴苛的、標準化的最終驗證提供參照。

**【圖表建議：圖 4.2.5 - 所有NERL實驗組精英模型在訓練評估中的最終訂單完成率對比】**
**【圖表建議：圖 4.2.6 - 所有NERL實驗組精英模型在訓練評估中的最終單均能耗對比】**
**【圖表建議：圖 4.2.7 - 所有NERL實驗組精英模型在訓練評估中的最終平均路口擁堵度對比】**
**【圖表建議：圖 4.2.8 - 所有NERL實驗組精英模型在訓練評估中的最終相位切換次數對比】**
**【圖表建議：圖 4.2.9 - 所有NERL實驗組精英模型在訓練評估中的最終啟停事件總數對比】**

透過對上述圖表的綜合分析，可得到在訓練階段的幾個關鍵觀察，這些觀察描繪了不同策略的特性，但其最終的有效性仍有待驗證：

1.  **全局獎勵與長時評估的潛力：訓練場上的「大局觀」策略**
    在訓練階段的評估中，一個顯著的趨勢是，「全局獎勵 + 長時評估」的組合，如 D 組 (NERL-Global, 低探索, 3000 Ticks) 和 H 組 (NERL-Global, 低探索, 8000 Ticks)，在**訂單完成率**與**單均能耗**這兩個核心指標上，展現出較強的潛力。
    這揭示了一個可能的機制：雖然全局獎勵信號稀疏，但當給予足夠長的評估時間時，演化算法擁有了探索更宏大、更複雜長遠策略的空間。智能體不再被短期的步階獎勵所束縛，而是能夠在漫長的評估中「發現」某些犧牲短期利益以換取長期回報的策略。然而，這種高度適應訓練環境的複雜策略是否過於「精巧」而導致「脆弱」，是其在下一節最終驗證中的一大隱憂。

2.  **步階獎勵的「短視」風險：長時評估下的策略漂移**
    與此相對，在 3000 ticks 短時評估中表現優異的步階獎勵模型，在延長至 8000 ticks 後，如 E 組 (NERL-Step, 高探索, 8000 Ticks) 和 F 組 (NERL-Step, 低探索, 8000 Ticks)，其在訓練結束時的訂單完成率表現平平。這似乎印證了 c 小節中的假設，即步階獎勵與過長的評估窗口存在不匹配。智能體在 8000 ticks 的時間內，可能會陷入「獎勵駭客 (Reward Hacking)」的狀態：過度專注於執行能最大化短期、即時步階獎勵的行為，而這些行為在長時間尺度下卻可能損害完成訂單這一終極目標。

3.  **效能的權衡：高績效模型的激進性格**
    從相位切換次數和啟停事件總數的圖表中可以看出，在訓練評估中取得較高訂單完成率的實驗組，如 G 組 (NERL-Global, 高探索, 8000 Ticks) 和 H 組 (NERL-Global, 低探索, 8000 Ticks)，其交通系統的內部擾動也最為劇烈。這揭示了這些模型學到的是一種「激進」的管理風格，為了提升流通效率而頻繁干預。這種策略的有效性高度依賴對環境的精準預測，在面對更長、更不可預測的真實驗證場景時，其穩定性將受到考驗。

**小結與展望**：綜合演化過程與訓練最終態的分析，可得出一個初步結論：不存在單一的「最優」超參數配置，而是存在適用於不同目標的**策略組合**。
- `全局獎勵 + 長時評估` 的組合在訓練中學會了最有潛力的、宏觀的、但也許最複雜的策略。
- `步階獎勵 + 短評估時長` 的組合則產生了最穩定、但可能潛力有限的策略。

這些在訓練環境中觀察到的特性與潛力，是否能轉化為在標準化、長週期驗證下的真正實力？哪個模型的策略更具**泛化能力**與**魯棒性**？這些問題的答案，將在下一節的最終效能驗證中揭曉。這為我們從「過程分析」到「決賽驗證」的過渡，提供了清晰的問題導向。

