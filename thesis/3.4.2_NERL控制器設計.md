# 3.4.2 神經演化強化學習 (NERL) 控制器設計

神經演化強化學習 (Neuroevolution Reinforcement Learning, NERL) 是本研究提出的**核心方法**。此方法將演化演算法 (Evolutionary Algorithms, EA) 的全局搜索能力與神經網路的非線性函數近似能力相結合，旨在克服傳統基於梯度之 DRL 方法（如 DQN）在面對稀疏獎勵、複雜參數空間時可能遇到的挑戰，例如收斂不穩定或陷入局部最優。

與 DQN 尋求近似價值函數不同，神經演化的目標是直接在策略的**參數空間**中進行優化。在 NERL 框架下，一個神經網路控制器（即一個策略 $\pi_\theta$）的權重和偏置 $\theta$ 被視為一個個體的**基因型 (Genotype)**。演算法通過對一個由眾多個體組成的**族群 (Population)** 進行演化操作，直接搜索最優的策略參數 $\theta^*$。

### 1. 演化運作流程

NERL 的核心流程圍繞一個「評估 → 選擇 → 繁殖」的演化循環，不斷迭代以提升族群的整體性能。假設在第 $g$ 代有一個族群 $P_g = \{\theta_1, \theta_2, ..., \theta_N\}$，其中包含 $N$ 個個體。

1.  **評估 (Evaluation)**: 族群中的每一個體 $\theta_i$ 都會被部署到一個獨立的模擬環境實例中，執行一個完整的評估回合 (Episode)。在該回合中，策略 $\pi_{\theta_i}$ 獨立做出所有決策。回合結束後，根據系統的宏觀性能指標（詳見 **3.4.5 節** 中定義的全局獎勵），計算出該個體的**適應度分數 (Fitness Score)** $F(\theta_i)$。此過程具有高度的並行性，可顯著縮短訓練時間。

2.  **選擇 (Selection)**: 根據所有個體計算出的適應度分數，演算法會從當前族群 $P_g$ 中選出表現優異的個體作為**父代 (Parents)**，為生成下一代提供基因。本研究採用**錦標賽選擇 (Tournament Selection)**，該機制在選擇壓力 (Selection Pressure) 與維持族群多樣性之間取得了良好的平衡。

3.  **繁殖 (Reproduction)**: 通過對父代進行遺傳操作，生成新的子代 (Offspring) 族群 $P_{g+1}$。主要操作包括：
    *   **精英保留 (Elitism)**: 為防止在演化過程中丟失已發現的最優解，每一代中適應度最高的 $k$ 個精英個體將被直接、完整地複製到下一代族群 $P_{g+1}$ 中。
    *   **交叉 (Crossover)**: 模擬生物繁殖，從父代中選擇兩個個體 $\theta_a$ 和 $\theta_b$，將其參數向量進行混合以創造新的子代 $\theta_c$。本研究採用**均勻交叉 (Uniform Crossover)**，新個體的每個參數都以等機率繼承自 $\theta_a$ 或 $\theta_b$。
    *   **變異 (Mutation)**: 在交叉後，對子代個體的參數 $\theta_c$ 施加一個小的隨機擾動，生成最終的 $\theta'_c$。此操作是維持族群多樣性、避免早熟收斂的關鍵。本研究採用**高斯變異 (Gaussian Mutation)**，即以一定的變異機率 $p_m$ 為每個參數疊加一個從高斯分佈 $\mathcal{N}(0, \sigma^2)$ 中採樣的噪聲，其中 $\sigma$ 為變異強度。

通過迭代執行上述循環，族群的平均適應度將會穩步提升，最終收斂於能夠高效解決複雜交通控制問題的高性能策略網路上。

### 2. NERL 與 DQN 的關鍵區別

*   **優化域**: DQN 在**價值函數空間**進行優化（學習 Q 函數），而 NERL 直接在**策略參數空間**進行搜索。
*   **優化機制**: DQN 依賴基於梯度的反向傳播，要求獎勵信號密集且損失函數可微。NERL 採用無梯度 (gradient-free) 的黑箱優化，對獎勵函數的性質（如稀疏性、延遲性）具有更強的魯棒性，使其特別適用於評估全局性能的場景。
*   **探索機制**: DQN 通過在**動作空間**中引入隨機性（如 $\epsilon$-greedy）來進行探索。NERL 的探索則內在地通過在**參數空間**中的交叉和變異操作來完成。

### 3. 神經網路架構

為確保與 DQN 基線進行公平、一致的比較，NERL 控制器採用了完全相同的神經網路架構。其輸入與輸出維度分別對應於 **3.4.3 節** 和 **3.4.4 節** 中定義的狀態與動作空間。

---
**【圖表建議：圖 3.4.2 - NERL 演化循環示意圖】**

建議在此處繪製一張循環圖，說明 NERL 的核心運作流程。圖中應包含：
1.  **初始族群 $P_g$**: 展示多個神經網路個體 $\theta_i$。
2.  **並行評估**: 每個 $\theta_i$ 在獨立環境中運行，並計算其適應度 $F(\theta_i)$。
3.  **選擇**: 根據適應度分數，通過錦標賽選擇出父代。
4.  **繁殖生成 $P_{g+1}$**: 展示精英保留、交叉和變異操作，生成新一代族群。
5.  箭頭應清晰地指向下一個階段，形成一個從 $P_g$ 到 $P_{g+1}$ 的完整演化閉環。 