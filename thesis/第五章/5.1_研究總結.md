# 5.1 研究總結

本研究旨在解決現代自動化倉儲機器人移動履行系統 (RMFS) 中，因交通壅塞而導致運作效率低落的核心問題。為此，本論文聚焦於設計一套有效且能自我適應的智慧交叉路口交通控制策略，以期在最小化機器人無效等待與能源消耗的同時，最大化系統整體的訂單履行能力。

為達此目標，本研究首先建構了一個高擬真度的倉儲模擬環境，並設計了兩種基於傳統規則的控制器（時間基礎與佇列基礎）作為效能基線。接著，引進了深度強化學習 (DRL) 方法，分別實現了基於標準深度 Q 網路 (DQN) 的控制器，以及一種結合了神經演化與強化學習的 NERL 控制器。針對 NERL 模型，本研究進一步設計了涵蓋不同獎勵模式、探索強度與評估時長的多組對照實驗，以系統性地探討各項超參數對模型行為與效能的影響。

實驗結果的分析遵循一條從「訓練過程」到「最終驗證」的嚴謹路徑。在訓練過程的分析中，研究發現採用「全局獎勵」與「長時評估」的 NERL 模型（如 H 組）展現出學習複雜、宏觀策略的巨大潛力。然而，在更長、更具挑戰性的標準化最終驗證中，結果出現了關鍵性的轉折：前述在訓練中表現優異的模型，因其策略過度擬合訓練環境而顯得脆弱，泛化能力不足，最終效能並非最佳。

本研究的最終結論指出，綜合性能最優的控制器為 F 組 (NERL-Step, 低探索, 8000 Ticks)。該模型所採用的「步階獎勵、低探索性、長時評估」組合，使其學會了一套不過於冒進、兼具高訂單完成率與卓越能源效率的均衡策略。它的成功證明，在複雜的隨機動態環境中，一個策略的**穩健性**與**泛化能力**，是比在特定訓練條件下達成極致指標更為重要的特質。此發現不僅為本研究的核心問題提供了具體的解決方案，也為未來在真實世界部署 DRL 系統提供了深刻的實務性洞見。 