
# **第 2 章 文獻探討**

## **2.1 本章概述**

本章旨在系統性地回顧與機器人移動履行系統（Robotic Mobile Fulfillment System, RMFS）中交通管理相關的研究。全文將循序漸進地展開：首先，深入闡述 RMFS 中交通壅塞問題的根源及其對系統效能的深遠影響；其次，將依序評估與本研究相關的三類控制策略——傳統的啟發式方法、主流的深度強化學習（DRL）方法，以及新興的神經演化（Neuroevolution）方法——在解決此問題上的應用、成就與局限。透過此一脈絡化的分析，本章旨在精準定位出現有研究的不足之處（Research Gap），最終闡明本研究提出一套整合能源考量、並基於神經演化強化學習（NERL）的交通控制策略之必要性與創新性。

## **2.2 機器人移動履行系統：運作效率與交通瓶頸**

RMFS 作為一種「貨到人」（Goods-to-Person）的倉儲自動化典範，其核心是透過大量自主移動機器人（Autonomous Mobile Robots, AMR）搬運可移動貨架（Pods），以取代傳統的人工揀選路徑，從而顯著提升訂單履行效率 **[Wurman et al., 2008]**。自 Kiva Systems 被 Amazon 收購並大規模部署以來 **[Guizzo, 2012]**，RMFS 已成為現代電子商務物流的基礎設施。研究表明其產能可達傳統倉儲的 2 至 4 倍，而隨著 Amazon 等企業部署的機器人數量突破 75 萬台，系統的運作密度與複雜性也達到了前所未有的高度 **[Merschformann et al., 2019]**。

然而，高密度部署帶來了新的、嚴峻的瓶頸：**交通壅塞（Traffic Congestion）**。正如本研究在第三章所定義的，當大量機器人在有限的通道網路中執行任務時，尤其是在路網的交叉點（Intersections），極易因路權協調不當而產生衝突與等待 **[Stern, 2019]**。這會引發一系列連鎖負面效應，不僅造成訂單履行時間延遲、降低系統整體吞吐量，近期研究更指出，頻繁的停止-啟動（Stop-and-Go）行為會顯著增加整體能耗 **[Li et al., 2020]**，並可能加速機器人電池的老化，增加營運成本。因此，設計一套能夠自我適應、且兼顧效率與能源可持續性的**交叉路口管理策略**，已成為提升 RMFS 整體效能的核心挑戰。

在 RMFS 的複雜決策體系中，交通控制與路徑規劃是與系統即時表現最為相關的環節。近年來，許多研究致力於解決此問題，例如 **Luo et al. (2023)** 提出了結合 A* 演算法與 DQN 的路徑規劃方法，而 **Zhou et al. (2024)** 則使用基於注意力機制的 DRL 模型來協同處理多 AGV 調度與貨架重配置問題。這些研究凸顯了智能演算法在 RMFS 運營優化中的巨大潛力。本研究雖聚焦於交叉口的交通控制，但其優化目標——提升系統流動性與效率——與這些研究在本質上是相通的。

## **2.3 交叉路口交通控制方法的回顧與評析**

為解決 RMFS 中的交叉路口交通問題，學界與業界已提出多種控制策略。其發展脈絡大致可分為啟發式方法與基於學習的方法兩大類，反映了從靜態規則到動態自適應控制的演進趨勢。

### **2.3.1 啟發式與規則式控制器 (Heuristic and Rule-based Controllers)**

早期的解決方案多依賴於人工設計的啟發式規則，其優點是實現簡單、計算開銷低，因此常作為評估複雜演算法效能的基準線（Baseline）。

一類是**靜態時制控制 (Static Time-based Control)**，此方法借鑒了傳統城市交通號誌系統，為交叉路口設定固定的通行權切換週期。其思想與早期基於固定規則的城市交通方案一脈相承 **[Dunne, 1964]**，這也對應了本研究在 3.3.1 節中設計的**時間基礎控制器 (`TimeBasedController`)**。然而，其根本缺陷在於完全無法感知即時交通流量的變化。當流量波動劇烈時，這種靜態策略極易在空閒方向浪費綠燈時間，或在繁忙方向造成不必要的隊列累積。

為彌補靜態方法的不足，另一類**動態反應式控制 (Dynamic Reactive Control)** 應運而生。這類方法根據路口的即時狀態（如等待隊列長度）來做出反應。例如，**Teja (2009)** 的研究為交叉路口指派代理，根據進入路口的機器人優先級來管理交通流。這類方法構成了本研究在 3.3.2 節中設計的**佇列基礎控制器 (`QueueBasedController`)** 的思想基礎。儘管反應更為靈活，但這些啟發式規則的設計高度依賴專家經驗與先驗知識，其預設的權重與邏輯難以捕捉複雜交通模式下的最優協同策略，在面對高度動態和非線性的交通情境時，往往只能達到次優的表現 **[Ross, 1971]**。

### **2.3.2 基於深度強化學習 (DRL) 的控制器**

為了克服人工規則的局限性，深度強化學習（DRL）被廣泛應用於交通控制問題。DRL 使得智能體（控制器）能從與環境的互動中自主學習複雜的決策策略，以最大化長期累積獎勵 **[Sutton & Barto, 2018]**。

此一範式在宏觀的**都市交通信號控制**領域已取得巨大成功。大量研究證實，基於深度 Q 網路（DQN）及其變體的方法在減少車輛延誤、提升路網吞吐量方面顯著超越了傳統方法 **[Zheng et al., 2019]**。例如，**Cao et al. (2024)** 提出 G-DQN 以提升收斂速度；**Zhu et al. (2021)** 比較了 PPO 與 DQN 在單交叉口控制上的表現；而 **Angela et al. (2024)** 則設計了融合壓力、排隊和速度等多維度資訊的獎勵函數來訓練 DQN 代理。這些研究充分展示了 DRL 在處理序列決策問題上的強大能力。

近年來，研究者開始將此範式遷移至 RMFS 場景。他們通常將交叉路口的局部交通資訊（如佇列長度、等待時間）作為**狀態 (State)**，將切換通行相位作為**動作 (Action)**，並以最大化通行量或最小化等待時間作為**獎勵 (Reward)** 來訓練控制器 **[Chen et al., 2017; Ma et al., 2021]**。儘管基於 DQN 的方法展現了巨大潛力，但它也面臨一些固有的挑戰：
1.  **獎勵設計敏感性**：DQN 的性能高度依賴於密集且設計精良的即時獎勵信號。如本研究在 3.4.3 節所分析，不良或過於簡化的獎勵設計可能導致智能體學習到非預期的次優行為（例如，為了獲得切換獎勵而頻繁抖動信號）。
2.  **局部最優風險**：在複雜、高維的狀態空間中，基於梯度的優化方法有時會收斂到局部最優解，難以發現全局最優的協同策略。
3.  **樣本效率問題**：DQN 通常需要大量的環境互動樣本才能學習到有效的策略，訓練過程可能非常耗時。

鑑於 DQN 在該領域的代表性與成熟度，本研究將其作為一個關鍵的**比較基線 (Baseline)**，用以客觀評估本研究提出的新方法所能帶來的潛在改進。

## **2.4 神經演化強化學習 (NERL) 的潛力**

為應對傳統 DRL 方法所面臨的挑戰，本研究引入了另一種優化範式——神經演化強化學習（NERL）。與 DQN 試圖學習一個價值函數不同，NERL 直接在策略網路的**參數空間 (Parameter Space)** 中進行搜索。它將神經網路的權重視為個體的「基因」，透過演化演算法（如精英選擇、交叉、變異）來優化整個智能體族群，最終得到高性能的控制策略 **[Heidrich-Meisner & Igel, 2009]**。

NERL 在解決複雜控制問題上展現出多種獨特優勢，使其特別適合本研究的目標：
1.  **對獎勵函數的魯棒性**：演化算法屬於無梯度（gradient-free）的黑箱優化，不要求獎勵函數連續或可微。這使得它天然適用於處理**稀疏、延遲或基於最終全局性能的獎勵信號**。例如，可以直接將整個模擬週期的「總訂單完成數」或「總能耗」作為適應度函數（Fitness Function），這與本研究在 3.4.3 節設計的**全局獎勵 (Global Reward)** 思想高度契合，從而避免了繁瑣的獎勵工程（Reward Engineering）。
2.  **更強的全局探索能力**：演化過程中的交叉和變異操作有助於在參數空間中進行更廣泛的探索，通過維護族群多樣性，降低了陷入局部最優的風險，更有可能發現創新且高效的控制策略。
3.  **內在的並行性**：族群中每個個體（策略網路）的評估過程是相互獨立的，可以大規模並行化部署在多核心 CPU 或計算集群上，從而有效利用現代計算資源，大幅縮短訓練牆鐘時間（wall-clock time）**[AbuZekry & Zulkernine, 2019]**。

儘管神經演化已在機器人控制、遊戲 AI 等領域取得卓越成就 **[Barmi et al., 2011]**，但將其**專門應用於 RMFS 交叉路口交通控制，並系統性地與傳統啟發式方法和主流 DRL 方法（如 DQN）在一個同時考慮了時間效率與能源消耗的綜合平台上進行比較的研究，目前尚屬空白。**

## **2.5 本章總結與研究缺口**

綜合上述分析，現有 RMFS 交通控制研究的發展脈絡清晰地指向了幾個關鍵的待解議題。控制策略正從簡單的靜態規則，演進到動態啟發式，再到基於 DRL 的自適應方法。然而，現有的 DRL 方法（主要是 DQN）在獎勵設計、避免局部最優和對全局長期目標的優化方面仍存在挑戰。與此同時，多數交通控制研究的核心目標是吞吐量和時間延遲，而將**能源消耗**作為與效率同等重要的核心優化目標之一的研究相對匱乏。

更重要的是，神經演化作為一種具備更強全局探索能力、且對稀疏全局獎勵更具魯棒性的方法，其在 RMFS 交通控制問題上的應用潛力尚未得到系統性的發掘與驗證。

因此，本研究的核心**研究缺口**可以概括為：**缺乏一套能夠在 RMFS 中有效平衡並優化交通效率與能源消耗，同時能克服傳統 DRL 在獎勵設計和局部最優等方面局限的智能控制策略。**

為填補此缺口，本論文提出了一套基於神經演化強化學習（NERL）的交叉路口控制器。我們將在一個考慮了精細化能耗模型的高擬真度模擬平台上，將 NERL 與傳統啟發式方法和標準的 DQN 方法進行全面的比較。此研究不僅旨在提出一種性能更優的控制策略，也為評估不同學習範式在解決複雜多智能體協同問題上的優劣提供了堅實的實證依據。

為了更直觀地將本研究與相關文獻進行對比，下表 2.1 總結了部分代表性研究與本研究在問題定義、決策準則和演算法途徑上的異同。

**表 2.1 與相關研究的比較**

| 文獻 | 問題陳述 | 決策準則 | 演算法途徑 |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| | 效能評估 | 能源消耗 | 動態路由 | 交通策略 | 碰撞處理 | 多智能體 | 分析法 | 啟發式 | 模擬 | 強化學習 |
| Li et al. (2020) | ✓ | ✓ | | | | ✓ | ✓ | | ✓ | |
| Luo et al. (2023) | ✓ | | ✓ | | ✓ | | | ✓ | ✓ | ✓ |
| Yuan et al. (2023) | ✓ | | | | ✓ | ✓ | | | ✓ | ✓ |
| Zhou et al. (2024) | ✓ | | ✓ | ✓ | | ✓ | | | ✓ | ✓ |
| Cao et al. (2024) | ✓ | | | ✓ | | | | | ✓ | ✓ |
| **本研究 (Ours)** | **✓** | **✓** | ✓ | **✓** | **✓** | **✓** | **✓** | **✓** | **✓** | **✓** |

