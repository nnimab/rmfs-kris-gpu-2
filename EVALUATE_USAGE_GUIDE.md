# evaluate.py çµ±ä¸€è©•ä¼°æ¡†æž¶ä½¿ç”¨æŒ‡å—

## ðŸŽ¯ æ¦‚è¿°

evaluate.py æ˜¯éšŽæ®µäºŒçš„æ ¸å¿ƒå·¥å…·ï¼Œç”¨æ–¼å…¬å¹³åœ°æ¯”è¼ƒ6ç¨®æŽ§åˆ¶å™¨çš„æ€§èƒ½ï¼Œä¸¦ç”Ÿæˆè«–æ–‡æ‰€éœ€çš„æ ¸å¿ƒæ•¸æ“šã€‚

### æ”¯æ´çš„æŽ§åˆ¶å™¨
- **å‚³çµ±æŽ§åˆ¶å™¨**: Time-based, Queue-based
- **AIæŽ§åˆ¶å™¨**: DQN-step, DQN-global, NERL-step, NERL-global

### ä¸»è¦åŠŸèƒ½
- çµ±ä¸€è©•ä¼°æµç¨‹ï¼Œç¢ºä¿å…¬å¹³å°æ¯”
- è‡ªå‹•ç”Ÿæˆä¸‰ç¨®å°æ¯”åˆ†æžå ±å‘Š
- æ”¯æ´å¯è¦–åŒ–å’Œç„¡é ­æ¨¡å¼
- å®Œæ•´çš„æ€§èƒ½æŒ‡æ¨™æ”¶é›†

## ðŸ“‹ å¿«é€Ÿé–‹å§‹

### 1. æª¢æŸ¥å¯ç”¨æŽ§åˆ¶å™¨
```bash
python evaluate.py --help
```

### 2. è©•ä¼°å‚³çµ±æŽ§åˆ¶å™¨ï¼ˆç„¡éœ€æ¨¡åž‹æ–‡ä»¶ï¼‰
```bash
# è©•ä¼°æ™‚é–“åŸºå’ŒéšŠåˆ—åŸºæŽ§åˆ¶å™¨
python evaluate.py --controllers time_based queue_based --ticks 5000

# ä½¿ç”¨æè¿°æ€§å‘½å
python evaluate.py --controllers time_based queue_based --description "traditional_only"

# ä½¿ç”¨è‡ªå®šç¾©è¼¸å‡ºç›®éŒ„ï¼ˆè¦†è“‹è‡ªå‹•å‘½åï¼‰
python evaluate.py --controllers time_based queue_based --output result/my_custom_test
```

### 3. è©•ä¼°æ‰€æœ‰å¯ç”¨æŽ§åˆ¶å™¨
```bash
# è‡ªå‹•æª¢æ¸¬å¯ç”¨çš„æŽ§åˆ¶å™¨ä¸¦è©•ä¼°ï¼ˆä½¿ç”¨æ–°çš„ç›®éŒ„çµæ§‹ï¼‰
python evaluate.py --ticks 10000

# æŒ‡å®šéš¨æ©Ÿç¨®å­å’Œæè¿°
python evaluate.py --seed 123 --ticks 10000 --description "paper_final"

# è«–æ–‡æœ€çµ‚çµæžœè©•ä¼°
python evaluate.py --ticks 20000 --description "thesis_results" --seed 42
```

## ðŸ”§ è©³ç´°ä½¿ç”¨æ–¹æ³•

### å‘½ä»¤è¡Œåƒæ•¸

| åƒæ•¸ | æè¿° | é è¨­å€¼ | ç¯„ä¾‹ |
|------|------|--------|------|
| `--controllers` | è¦è©•ä¼°çš„æŽ§åˆ¶å™¨åˆ—è¡¨ | æ‰€æœ‰å¯ç”¨ | `time_based queue_based` |
| `--ticks` | æ¯å€‹æŽ§åˆ¶å™¨çš„è©•ä¼°æ™‚é•· | 20000 | `10000` |
| `--seed` | éš¨æ©Ÿç¨®å­ | 42 | `123` |
| `--output` | çµæžœè¼¸å‡ºç›®éŒ„ï¼ˆè¦†è“‹è‡ªå‹•å‘½åï¼‰ | è‡ªå‹•ç”Ÿæˆ | `result/my_test` |
| `--description` | è©•ä¼°æè¿°ï¼ˆç”¨æ–¼ç›®éŒ„å‘½åï¼‰ | None | `paper_final` |
| `--analysis-only` | åƒ…åŸ·è¡Œåˆ†æžï¼Œè·³éŽè©•ä¼° | False | - |

### æŽ§åˆ¶å™¨IDå°ç…§è¡¨

| æŽ§åˆ¶å™¨ID | åç¨± | é¡žåž‹ | éœ€è¦æ¨¡åž‹æ–‡ä»¶ |
|----------|------|------|-------------|
| `time_based` | Time-Based | å‚³çµ± | âŒ |
| `queue_based` | Queue-Based | å‚³çµ± | âŒ |
| `dqn_step` | DQN-Step | AI | âœ… models/dqn_step.pth |
| `dqn_global` | DQN-Global | AI | âœ… models/dqn_global.pth |
| `nerl_step` | NERL-Step | AI | âœ… models/nerl_step.pth |
| `nerl_global` | NERL-Global | AI | âœ… models/nerl_global.pth |

## ðŸ“Š è¼¸å‡ºçµæžœ

### ç”Ÿæˆçš„æ–‡ä»¶

åŸ·è¡Œå¾Œæœƒåœ¨æŒ‡å®šç›®éŒ„ç”Ÿæˆä»¥ä¸‹æ–‡ä»¶ï¼š

**ðŸ†• æ–°çš„ç›®éŒ„çµæ§‹ï¼ˆèˆ‡NetLogoçµæžœåˆ†é›¢ï¼‰**ï¼š
```
result/
â”œâ”€â”€ evaluations/                           # ðŸ†• è©•ä¼°æ¡†æž¶å°ˆç”¨ç›®éŒ„
â”‚   â””â”€â”€ EVAL_20250708_143000_6controllers_20kticks_paper_final/
â”‚       â”œâ”€â”€ evaluation_config.json        # ðŸ†• è©•ä¼°é…ç½®è¨˜éŒ„
â”‚       â”œâ”€â”€ algorithm_comparison.csv      # ç®—æ³•å°æ¯”åˆ†æž
â”‚       â”œâ”€â”€ reward_comparison.csv         # çŽå‹µæ©Ÿåˆ¶å°æ¯”åˆ†æž
â”‚       â”œâ”€â”€ overall_comparison.csv        # æ•´é«”æ€§èƒ½å°æ¯”
â”‚       â”œâ”€â”€ performance_rankings.json     # æ€§èƒ½æŽ’è¡Œæ¦œ
â”‚       â””â”€â”€ charts/                       # ðŸ†• åœ–è¡¨å­ç›®éŒ„
â”‚           â”œâ”€â”€ performance_radar_chart.png
â”‚           â”œâ”€â”€ algorithm_comparison_chart.png
â”‚           â””â”€â”€ ...
â””â”€â”€ 2025-07-08-143000/                    # NetLogoåŽŸæœ‰çµæžœï¼ˆä¸å½±éŸ¿ï¼‰
    â”œâ”€â”€ intersection-energy-consumption.csv
    â””â”€â”€ ...
```

**ç›®éŒ„å‘½åè¦å‰‡**ï¼š
- `EVAL_[æ™‚é–“æˆ³]_[æŽ§åˆ¶å™¨æè¿°]_[è©•ä¼°æ™‚é•·]_[è‡ªå®šç¾©æè¿°]`
- ç¯„ä¾‹ï¼š
  - `EVAL_20250708_143000_traditional_only_5kticks`
  - `EVAL_20250708_150000_6controllers_20kticks_paper_final`
  - `EVAL_20250708_160000_dqn_step_nerl_step_10kticks`

### æ ¸å¿ƒæ€§èƒ½æŒ‡æ¨™

| æŒ‡æ¨™ | æè¿° | å–®ä½ |
|------|------|------|
| `total_energy` | ç¸½èƒ½æºæ¶ˆè€— | èƒ½æºå–®ä½ |
| `completed_orders` | å®Œæˆè¨‚å–®æ•¸ | å€‹ |
| `completion_rate` | è¨‚å–®å®ŒæˆçŽ‡ | 0-1 |
| `avg_wait_time` | å¹³å‡ç­‰å¾…æ™‚é–“ | ticks |
| `max_wait_time` | æœ€å¤§ç­‰å¾…æ™‚é–“ | ticks |
| `wait_time_std` | ç­‰å¾…æ™‚é–“æ¨™æº–å·® | ticks |
| `robot_utilization` | æ©Ÿå™¨äººåˆ©ç”¨çŽ‡ | 0-1 |
| `energy_per_order` | å–®ä½è¨‚å–®èƒ½æºæ¶ˆè€— | èƒ½æº/è¨‚å–® |
| `global_reward` | ç¶œåˆçŽå‹µè©•åˆ† | ç„¡é‡ç¶± |

### ðŸ†• è©•ä¼°é…ç½®è¨˜éŒ„ (`evaluation_config.json`)

æ¯æ¬¡è©•ä¼°éƒ½æœƒè‡ªå‹•ç”Ÿæˆé…ç½®è¨˜éŒ„æ–‡ä»¶ï¼š
```json
{
  "timestamp": "2025-07-08 14:30:25",
  "evaluation_ticks": 20000,
  "random_seed": 42,
  "description": "paper_final",
  "planned_controllers": ["time_based", "queue_based"],
  "result_directory": "result/evaluations/EVAL_20250708_143000_...",
  "framework_version": "2.0",
  "directory_structure": {
    "type": "evaluation_namespace",
    "separate_from_netlogo": true,
    "charts_subdirectory": "charts/"
  }
}
```

## ðŸ§ª æ¸¬è©¦å’Œèª¿è©¦

### 1. åŸºæœ¬åŠŸèƒ½æ¸¬è©¦
```bash
# å¿«é€Ÿæ¸¬è©¦å‚³çµ±æŽ§åˆ¶å™¨ï¼ˆä¸éœ€è¦æ¨¡åž‹æ–‡ä»¶ï¼‰
python evaluate.py --controllers time_based --ticks 1000

# æª¢æŸ¥æ˜¯å¦æœ‰èªžæ³•éŒ¯èª¤
python evaluate.py --help
```

### 2. æ¨¡åž‹æ–‡ä»¶æª¢æŸ¥
```bash
# æª¢æŸ¥ç•¶å‰å¯ç”¨çš„æ¨¡åž‹æ–‡ä»¶
ls models/

# å¦‚æžœç¼ºå°‘AIæŽ§åˆ¶å™¨æ¨¡åž‹ï¼Œæœƒè‡ªå‹•è·³éŽ
python evaluate.py --controllers dqn_step nerl_step --ticks 1000
```

### 3. éŒ¯èª¤è™•ç†æ¸¬è©¦
```bash
# æ¸¬è©¦ç„¡æ•ˆæŽ§åˆ¶å™¨ID
python evaluate.py --controllers invalid_controller

# æ¸¬è©¦æ¥µçŸ­è©•ä¼°æ™‚é–“
python evaluate.py --controllers time_based --ticks 10
```

## ðŸ”„ èˆ‡train.pyçš„é…åˆä½¿ç”¨

### å®Œæ•´å·¥ä½œæµç¨‹

```bash
# 1. è¨“ç·´AIæ¨¡åž‹ï¼ˆéšŽæ®µ1.5å·²å®Œæˆï¼‰
python train.py --agent dqn --reward_mode step --training_ticks 10000
python train.py --agent dqn --reward_mode global --training_ticks 10000
python train.py --agent nerl --reward_mode step --generations 20
python train.py --agent nerl --reward_mode global --generations 20

# 2. é‡å‘½åæ¨¡åž‹æ–‡ä»¶
mv models/dqn_traffic_10000.pth models/dqn_step.pth
# ï¼ˆå…¶ä»–æ¨¡åž‹æ–‡ä»¶é¡žä¼¼è™•ç†ï¼‰

# 3. é‹è¡Œçµ±ä¸€è©•ä¼°ï¼ˆä½¿ç”¨æ–°çš„ç›®éŒ„çµæ§‹ï¼‰
python evaluate.py --ticks 20000 --description "thesis_final"

# 4. æŸ¥çœ‹çµæžœï¼ˆæ–°çš„ç›®éŒ„çµæ§‹ï¼‰
cd result/evaluations/EVAL_[timestamp]_[description]/
```

## ðŸ“ˆ åˆ†æžå ±å‘Šèªªæ˜Ž

### 1. algorithm_comparison.csv
æ¯”è¼ƒç›¸åŒçŽå‹µæ©Ÿåˆ¶ä¸‹çš„DQN vs NERLï¼š
- `reward_mode`: çŽå‹µæ¨¡å¼ï¼ˆstep/globalï¼‰
- `algorithm_1/2`: å°æ¯”çš„ç®—æ³•
- `metric`: æ¯”è¼ƒæŒ‡æ¨™
- `improvement`: æ”¹é€²ç™¾åˆ†æ¯”

### 2. reward_comparison.csv
æ¯”è¼ƒç›¸åŒç®—æ³•ä¸‹çš„step vs globalï¼š
- `algorithm`: ç®—æ³•åç¨±ï¼ˆDQN/NERLï¼‰
- `mode_1/2`: å°æ¯”çš„çŽå‹µæ¨¡å¼
- `improvement`: æ”¹é€²ç™¾åˆ†æ¯”

### 3. overall_comparison.csv
æ‰€æœ‰æŽ§åˆ¶å™¨çš„å®Œæ•´æ€§èƒ½æ•¸æ“šï¼ŒåŒ…å«ï¼š
- æ‰€æœ‰æ ¸å¿ƒKPIæŒ‡æ¨™
- æŽ§åˆ¶å™¨é…ç½®ä¿¡æ¯
- è©•ä¼°åƒæ•¸è¨˜éŒ„

### 4. performance_rankings.json
æŒ‰ä¸åŒæŒ‡æ¨™çš„æ€§èƒ½æŽ’è¡Œæ¦œï¼š
```json
{
  "total_energy": [
    ["Queue-Based", 1250.5],
    ["DQN-Global", 1380.2],
    ...
  ],
  "completion_rate": [
    ["NERL-Global", 0.95],
    ["DQN-Step", 0.92],
    ...
  ]
}
```

## âš ï¸ æ³¨æ„äº‹é …

### Windowsç’°å¢ƒé…ç½®
1. ç¢ºä¿Pythonç’°å¢ƒæ­£ç¢ºé…ç½®
2. æª¢æŸ¥æ‰€æœ‰ä¾è³´å¥—ä»¶å·²å®‰è£
3. NetLogoè·¯å¾‘é…ç½®æ­£ç¢º

### æ¨¡åž‹æ–‡ä»¶ç®¡ç†
1. AIæŽ§åˆ¶å™¨éœ€è¦å°æ‡‰çš„.pthæ¨¡åž‹æ–‡ä»¶
2. ç¼ºå°‘æ¨¡åž‹æ–‡ä»¶çš„æŽ§åˆ¶å™¨æœƒè‡ªå‹•è·³éŽ
3. å»ºè­°ä½¿ç”¨çµ±ä¸€çš„æ¨¡åž‹å‘½åè¦ç¯„

### è©•ä¼°æ™‚é–“å»ºè­°
- **å¿«é€Ÿæ¸¬è©¦**: 1000-5000 ticks
- **å®Œæ•´è©•ä¼°**: 20000 ticksï¼ˆè«–æ–‡æ¨™æº–ï¼‰
- **è©³ç´°åˆ†æž**: 50000+ ticks

### å…§å­˜å’Œæ™‚é–“è€ƒé‡
- é•·æ™‚é–“è©•ä¼°å¯èƒ½éœ€è¦è¼ƒå¤šå…§å­˜
- å»ºè­°åˆ†æ‰¹è©•ä¼°æŽ§åˆ¶å™¨
- ä½¿ç”¨ `--analysis-only` é‡æ–°åˆ†æžå·²æœ‰çµæžœ

## ðŸš€ é€²éšŽç”¨æ³•

### 1. æ‰¹é‡è©•ä¼°è…³æœ¬
```bash
# å‰µå»ºæ‰¹é‡è©•ä¼°è…³æœ¬
cat > batch_evaluate.sh << 'EOF'
#!/bin/bash
echo "é–‹å§‹æ‰¹é‡è©•ä¼°..."

# çŸ­æ™‚é–“å¿«é€Ÿæ¸¬è©¦ï¼ˆä½¿ç”¨æ–°çš„æè¿°æ€§å‘½åï¼‰
python evaluate.py --controllers time_based queue_based --ticks 5000 --description "quick_test"

# ä¸­ç­‰æ™‚é–“æ¸¬è©¦
python evaluate.py --controllers time_based queue_based --ticks 15000 --description "medium_test"

# å®Œæ•´é•·æ™‚é–“è©•ä¼°
python evaluate.py --ticks 20000 --description "full_evaluation"

echo "æ‰¹é‡è©•ä¼°å®Œæˆï¼"
EOF

chmod +x batch_evaluate.sh
./batch_evaluate.sh
```

### 2. çµæžœæ¯”è¼ƒåˆ†æž
```bash
# æ¯”è¼ƒä¸åŒè©•ä¼°æ™‚é–“çš„çµæžœï¼ˆä½¿ç”¨æ–°çš„ç›®éŒ„çµæ§‹ï¼‰
python -c "
import pandas as pd
df1 = pd.read_csv('result/evaluations/EVAL_*_quick_test/overall_comparison.csv')
df2 = pd.read_csv('result/evaluations/EVAL_*_medium_test/overall_comparison.csv')
print('5k vs 15k ticksçµæžœå·®ç•°:')
print(df2['total_energy'] - df1['total_energy'])
"
```

## ðŸ“ž æ•…éšœæŽ’é™¤

### å¸¸è¦‹å•é¡Œ
1. **ImportError**: æª¢æŸ¥Pythonå¥—ä»¶å®‰è£
2. **æ¨¡åž‹æ–‡ä»¶ä¸å­˜åœ¨**: ç¢ºèªmodels/ç›®éŒ„ä¸­æœ‰å°æ‡‰æ–‡ä»¶
3. **è©•ä¼°ä¸­æ–·**: æª¢æŸ¥å…§å­˜ä½¿ç”¨æƒ…æ³
4. **çµæžœç•°å¸¸**: å˜—è©¦ä¸åŒéš¨æ©Ÿç¨®å­

### èª¿è©¦æŠ€å·§
```bash
# å•Ÿç”¨è©³ç´°è¼¸å‡º
python evaluate.py --controllers time_based --ticks 100 -v

# æª¢æŸ¥å–®å€‹æŽ§åˆ¶å™¨
python evaluate.py --controllers time_based --ticks 1000

# æŸ¥çœ‹ç”Ÿæˆçš„çµæžœæ–‡ä»¶ï¼ˆæ–°çš„ç›®éŒ„çµæ§‹ï¼‰
ls -la result/evaluations/EVAL_*/
head -5 result/evaluations/EVAL_*/overall_comparison.csv
```

---

ðŸŽ‰ **æ­å–œï¼** æ‚¨ç¾åœ¨å¯ä»¥ä½¿ç”¨evaluate.pyé€²è¡Œçµ±ä¸€çš„æŽ§åˆ¶å™¨æ€§èƒ½è©•ä¼°äº†ï¼

æœ‰ä»»ä½•å•é¡Œè«‹åƒè€ƒæ­¤æŒ‡å—æˆ–æŸ¥çœ‹ä»£ç¢¼è¨»é‡‹ã€‚