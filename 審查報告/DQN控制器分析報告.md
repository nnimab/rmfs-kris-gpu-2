# DQN 控制器 (DQN Controller) 分析報告

**審查日期**: 2025-06-18
**審查人員**: AI助理

---

## 1. 核心邏輯解釋

- **原始碼位置**: `ai/controllers/dqn_controller.py`
- **歷史沿革**: 於 `v0.3.0` 版本首次實現，在 `v0.2.0` 時為專注優化基礎控制器而被暫時移除，其後回歸並成為更先進的 NERL 控制器的設計藍本。
- **總體設計思想**: 這是一個**混合式 (Hybrid)** 控制器。它並非讓 AI 單獨決策，而是建立了一套**以規則為基礎的安全框架**，再讓 DQN 模型在這個框架內進行優化。可以理解為「人類專家制定規則，AI 負責在規則之間做出最佳選擇」。
    - **規則1：防餓死機制 (`max_wait_threshold`)**: 如果任何方向有機器人等待時間超過閾值，系統會**強制切換**信號燈，此時 AI 的決策會被忽略。這是最高優先級的規則。
    - **規則2：防抖動機制 (`min_green_time`)**: 為避免交通燈頻繁切換導致效率低下，一旦燈色改變，會強制保持至少 `min_green_time` 個時間單位，此時 AI 的決策也會被忽略。
    - **規則3：簡化決策**: 如果路口只有一個方向有車，系統會直接採用最優解（讓有車的方向通行），無需諮詢 AI。
    - **DQN 決策**: **只有在上述所有規則都不被觸發的情況下**，控制器才會將當前路口的狀態提供給 DQN 模型，由模型來做出最終決策。

### 強化學習三要素 (State, Action, Reward)

- **狀態空間 (State) - AI 能「看」到什麼？**:
    - AI觀察的是一個由8個特徵組成的向量，所有特徵都被**正規化**到 [0, 1] 之間。
    - `[0]` 當前信號燈方向 (垂直/水平)
    - `[1]` 距離上次信號燈變換過了多久
    - `[2]` 水平方向排隊的機器人總數
    - `[3]` 垂直方向排隊的機器人總數
    - `[4]` 水平方向**高優先級**機器人的**比例**
    - `[5]` 垂直方向**高優先級**機器人的**比例**
    - `[6]` 水平方向的平均等待時間
    - `[7]` 垂直方向的平均等待時間

- **動作空間 (Action) - AI 能「做」什麼？**:
    - AI有3個選擇：
        - `0`: 保持當前方向不變。
        - `1`: 將信號燈變為垂直方向。
        - `2`: 將信號燈變為水平方向。

- **獎勵函數 (Reward) - AI 如何判斷「好壞」？**:
    - 這是設計最精巧的部分。獎勵是一個加權總和，旨在引導 AI 學會一個**全面、高效**的策略。
    - `+` **等待時間減少量**: 最大的正向獎勵來源。如果一個動作能有效降低路口整體的平均等待時間，就會獲得高獎勵。
    - `+` **機器人通行量**: 每成功通過一個機器人，給予少量正向獎勵。
    - `-` **切換信號燈懲罰**: 為了穩定交通流，每次切換信號燈都會有一個固定的負獎勵。
    - `-` **能源消耗懲罰**: 根據路口內所有機器人的總能耗，給予一個負獎勵。
    - `-` **啟停次數懲罰**: 根據路口內所有機器人的總啟停次數，給予一個負獎勵。

### 訓練機制

- 控制器採用了標準的 DQN 訓練流程，包含**經驗回放 (Experience Replay)** 和 **目標網路 (Target Network)** 這兩個關鍵技術，以保證訓練的穩定性。
- **訓練時機**: 每 64 個時間單位，從記憶體中抽取一批過去的經驗 (`(狀態, 動作, 獎勵, 下一狀態)`) 來進行訓練。
- **模型保存**: 每 5000 個時間單位，會自動保存一次模型。

## 2. 正確性驗證

- **與日誌一致性**: **完全一致**。程式碼的實現細節，包括8個狀態特徵、獎勵函數的組成、以及提到的 `AttributeError` 錯誤修復，都與 `v0.3.0` 的日誌描述完全相符。
- **邏輯缺陷評估**:
    1.  **狀態正規化問題**: 狀態特徵的正規化是靠除以一個**硬編碼**的最大值（例如，機器人數量除以10，等待時間除以50）。這是一個設計上的**重大缺陷**。如果路口的車輛數超過10台，AI所看到的狀態將一直被"截斷"在最大值1.0，它將無法區分「11台車」和「20台車」的區別，這極大地限制了它在高密度交通下的決策能力。
    2.  **資訊孤島問題**: 每個交叉口的 DQN 智能體都是**獨立的**，它只能看到自己路口的狀態，對相鄰路口的交通狀況一無所知。這正是導致「卍」字鎖死問題的**根本原因**：四個路口各自做出了對自己最優的決策，但組合起來卻導致了全局的災難。

## 3. 潛在改進點

- **【潛在改進點 1】**: **自適應狀態正規化 (Adaptive State Normalization)**
    - **問題**: 硬編碼的正規化參數限制了模型的泛化能力和在高流量場景下的表現。
    - **建議**: 放棄硬編碼，改為使用運行時的統計數據（如滑動平均值和標準差）來動態地正規化狀態，或採用 `log(1+x)` 這類更穩健的函數。
    - **後續行動**: 這是提升 AI 效能的一個重要技術細節，應記錄下來作為未來優化的任務。

- **【潛在改進點 2】**: **擴展狀態空間以包含鄰居資訊**
    - **問題**: 「資訊孤島」是導致全局鎖死的根源。
    - **建議**: 將狀態空間從8維擴展，加入相鄰路口的關鍵資訊（例如，鄰居的信號燈狀態、鄰居的排隊長度等）。這將讓 AI 具備「全局視野」的雛形，是解決「卍」字鎖死問題最直接、最有效的方法。
    - **後續行動**: 這是一個價值極高的改進點，與 `代辦事項2.md` 的目標完全一致，應作為核心任務來規劃。

- **【潛在改進點 3】**: **獎勵函數權重優化**
    - **問題**: 獎勵函數中各個部分的權重（如 `energy_penalty = -0.1 * ...`）是手動設定的，不一定是最佳組合。
    - **建議**: 對這些權重進行系統性的實驗和調整（超參數搜索），以找到最優的獎勵結構。這本身就可以是論文中一個有趣的研究點。
    - **後續行動**: 記錄下來，作為未來可以深入探索的實驗方向。 